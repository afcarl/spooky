{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding, Lambda\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def preprocess(text):\n",
    "    text = text.replace(\"' \", \" ' \")\n",
    "    signs = set(',.:;\"?')\n",
    "    prods = set(text) & signs\n",
    "    if not prods:\n",
    "        return text\n",
    "\n",
    "    for sign in prods:\n",
    "        text = text.replace(sign, ' {} '.format(sign) )\n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dims = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = {name : defaultdict(int) for name in set(df.author)}\n",
    "for (text, author) in zip(df.text, df.author):\n",
    "    text = text.replace(' ', '')\n",
    "    for c in text:\n",
    "        counter[author][c] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c HPL   MWS   EAP   \n",
      "A 1167 943 1258 \n",
      "e 88259 97515 114885 \n",
      "w 15554 16062 17507 \n",
      "\" 513 1469 2987 \n",
      "p 10965 12361 17422 \n",
      "G 318 246 313 \n",
      "R 237 385 258 \n",
      "t 62235 63142 82426 \n",
      "K 176 35 86 \n",
      "Ν 1 0 0 \n",
      ". 5908 5761 8406 \n",
      "ê 2 0 28 \n",
      "i 44250 46080 60952 \n",
      "B 533 395 835 \n",
      "Å 1 0 0 \n",
      "S 841 578 729 \n",
      "I 3480 4917 4846 \n",
      "Π 1 0 0 \n",
      "U 94 46 166 \n",
      "ä 6 0 1 \n",
      "T 1583 1230 2217 \n",
      "x 1061 1267 1951 \n",
      "W 732 681 739 \n",
      "ë 12 0 0 \n",
      "æ 10 0 36 \n",
      "ô 0 0 8 \n",
      "N 345 204 411 \n",
      "ç 0 0 1 \n",
      "X 5 4 17 \n",
      ": 47 339 176 \n",
      "J 210 66 164 \n",
      "P 320 365 442 \n",
      "r 40590 44042 51221 \n",
      "O 503 282 414 \n",
      "Ο 3 0 0 \n",
      "F 269 232 383 \n",
      "â 0 0 6 \n",
      "H 741 669 864 \n",
      "ï 7 0 0 \n",
      "D 334 227 491 \n",
      "o 50996 53386 67145 \n",
      "q 779 677 1030 \n",
      "à 0 0 10 \n",
      "g 14951 12601 16088 \n",
      "ñ 7 0 0 \n",
      "Q 10 7 21 \n",
      "m 17622 20471 22792 \n",
      "v 6529 7948 9624 \n",
      "L 249 307 458 \n",
      "Υ 1 0 0 \n",
      "u 19519 21025 26311 \n",
      "M 645 415 1065 \n",
      "y 12534 14877 17001 \n",
      "Z 51 2 23 \n",
      "? 169 419 510 \n",
      "c 18338 17911 24127 \n",
      "Æ 4 0 1 \n",
      "V 67 57 156 \n",
      "Y 111 234 282 \n",
      "ö 3 0 16 \n",
      "δ 2 0 0 \n",
      "è 0 0 15 \n",
      "α 2 0 0 \n",
      "é 15 0 47 \n",
      "' 1710 476 1334 \n",
      "; 1143 2662 1354 \n",
      "a 56815 55274 68525 \n",
      "h 42770 43738 51580 \n",
      "j 424 682 683 \n",
      "d 33366 35315 36862 \n",
      ", 8581 12045 17594 \n",
      "s 43915 45962 53841 \n",
      "l 30273 27819 35371 \n",
      "f 16272 18351 22354 \n",
      "ü 5 0 1 \n",
      "ἶ 2 0 0 \n",
      "E 281 445 435 \n",
      "n 50879 50291 62636 \n",
      "k 5204 3707 4277 \n",
      "î 0 0 1 \n",
      "C 439 308 395 \n",
      "b 10636 9611 13245 \n",
      "z 529 400 634 \n",
      "Σ 1 0 0 \n"
     ]
    }
   ],
   "source": [
    "chars = set()\n",
    "for v in counter.values():\n",
    "    chars |= v.keys()\n",
    "    \n",
    "names = [author for author in counter.keys()]\n",
    "special_latters = set()\n",
    "print('c ', end='')\n",
    "for n in names:\n",
    "    print(n, end='   ')\n",
    "print()\n",
    "for c in chars:    \n",
    "    print(c, end=' ')\n",
    "    for n in names:\n",
    "        print(counter[n][c], end=' ')\n",
    "    print()\n",
    "    special_latters.add(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴とか\n",
    "\n",
    "- MWSはasciiしか系しか使わない（イギリスの作家だから？、それ以外の2人は使い、アメリカ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "special_latters = special_latters - set(string.ascii_uppercase) - set(string.ascii_lowercase) - set(',.:;\"\\'?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à ñ Ν ê Υ Å Æ ö δ è Π α é ä ë æ ô ç ü ἶ Ο â î ï Σ'"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(special_latters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for (text, author) in zip(df.text, df.author):\n",
    "#     if len(set(text) & special_latters):\n",
    "#         print(set(text) & special_latters, author, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_docs(df, n_gram_max=1):\n",
    "    docs = []\n",
    "\n",
    "    for i, text in enumerate(df.text):    \n",
    "        def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "\n",
    "        doc = preprocess(text).split()\n",
    "        \n",
    "        prod = special_latters & set(text)\n",
    "        special_chars = ''\n",
    "        if prod:\n",
    "            for c in prod:\n",
    "                freq = text.count(c)\n",
    "                special_chars += (' {} '.format(c) * freq)            \n",
    "                \n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)) + special_chars)\n",
    "        \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = create_docs(df, n_gram_max=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=2, filters='', lower=False)\n",
    "tokenizer.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 1\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=num_words, filters='', lower=False)\n",
    "tokenizer.fit_on_texts(docs)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "maxlen = max([len(i) for i in  docs])\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28285"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array([a2c[a] for a in df.author])\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28285, 875)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = np.max(docs)+1\n",
    "input_dim, maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "model.add(GlobalAveragePooling1D())\n",
    "model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train validation case\n",
    "# epochs = 45\n",
    "# x_train, x_test, y_train, y_test = train_test_split(docs, y, test_size=0.15)\n",
    "\n",
    "# n_samples = x_train.shape[0]\n",
    "# class_weight = {c : (n_samples/(np.sum(y_train, axis=0)[c] * 3)) for c in range(3)}\n",
    "\n",
    "# hist = model.fit(x_train, y_train,\n",
    "#                  batch_size=16,\n",
    "#                  validation_data=(x_test, y_test),\n",
    "#                  epochs=epochs,\n",
    "#                  callbacks=[EarlyStopping(patience=4, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# history = hist.history\n",
    "# for i in range(epochs):\n",
    "#     print('{0:2d}'.format(i), \n",
    "#           '{0:.4f}'.format(history['loss'][i]), \n",
    "#           '{0:.4f}'.format(history['acc'][i]),\n",
    "#           '{0:.4f}'.format(history['val_loss'][i]),\n",
    "#           '{0:.4f}'.format(history['val_acc'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/38\n",
      "19579/19579 [==============================] - 18s - loss: 1.0846 - acc: 0.4035    \n",
      "Epoch 2/38\n",
      "19579/19579 [==============================] - 15s - loss: 1.0662 - acc: 0.4093    \n",
      "Epoch 3/38\n",
      "19579/19579 [==============================] - 14s - loss: 1.0157 - acc: 0.4810    \n",
      "Epoch 4/38\n",
      "19579/19579 [==============================] - 14s - loss: 0.9370 - acc: 0.6169    \n",
      "Epoch 5/38\n",
      "19579/19579 [==============================] - 14s - loss: 0.8509 - acc: 0.6957    \n",
      "Epoch 6/38\n",
      "19579/19579 [==============================] - 14s - loss: 0.7730 - acc: 0.7400    \n",
      "Epoch 7/38\n",
      "19579/19579 [==============================] - 14s - loss: 0.7072 - acc: 0.7623    \n",
      "Epoch 8/38\n",
      "19579/19579 [==============================] - 19s - loss: 0.6518 - acc: 0.7798    \n",
      "Epoch 9/38\n",
      "19579/19579 [==============================] - 16s - loss: 0.6046 - acc: 0.7956    \n",
      "Epoch 10/38\n",
      "19579/19579 [==============================] - 18s - loss: 0.5634 - acc: 0.8115    \n",
      "Epoch 11/38\n",
      "19579/19579 [==============================] - 18s - loss: 0.5275 - acc: 0.8207    \n",
      "Epoch 12/38\n",
      "19579/19579 [==============================] - 18s - loss: 0.4943 - acc: 0.8352    \n",
      "Epoch 13/38\n",
      "19579/19579 [==============================] - 17s - loss: 0.4650 - acc: 0.8445    \n",
      "Epoch 14/38\n",
      "19579/19579 [==============================] - 20s - loss: 0.4392 - acc: 0.8548    \n",
      "Epoch 15/38\n",
      "19579/19579 [==============================] - 16s - loss: 0.4135 - acc: 0.8640    \n",
      "Epoch 16/38\n",
      "19579/19579 [==============================] - 16s - loss: 0.3919 - acc: 0.8715    \n",
      "Epoch 17/38\n",
      "19579/19579 [==============================] - 18s - loss: 0.3709 - acc: 0.8783    \n",
      "Epoch 18/38\n",
      "19579/19579 [==============================] - 20s - loss: 0.3519 - acc: 0.8862    \n",
      "Epoch 19/38\n",
      "19579/19579 [==============================] - 19s - loss: 0.3344 - acc: 0.8925    \n",
      "Epoch 20/38\n",
      "19579/19579 [==============================] - 18s - loss: 0.3179 - acc: 0.8990    \n",
      "Epoch 21/38\n",
      "19579/19579 [==============================] - 15s - loss: 0.3027 - acc: 0.9029    \n",
      "Epoch 22/38\n",
      "19579/19579 [==============================] - 17s - loss: 0.2886 - acc: 0.9078    \n",
      "Epoch 23/38\n",
      "19579/19579 [==============================] - 17s - loss: 0.2758 - acc: 0.9153    \n",
      "Epoch 24/38\n",
      "19579/19579 [==============================] - 17s - loss: 0.2628 - acc: 0.9190    \n",
      "Epoch 25/38\n",
      "19579/19579 [==============================] - 17s - loss: 0.2510 - acc: 0.9233    \n",
      "Epoch 26/38\n",
      "19579/19579 [==============================] - 17s - loss: 0.2406 - acc: 0.9268    \n",
      "Epoch 27/38\n",
      "19579/19579 [==============================] - 15s - loss: 0.2302 - acc: 0.9306    \n",
      "Epoch 28/38\n",
      "19579/19579 [==============================] - 16s - loss: 0.2206 - acc: 0.9313    \n",
      "Epoch 29/38\n",
      "19579/19579 [==============================] - 17s - loss: 0.2125 - acc: 0.9347    \n",
      "Epoch 30/38\n",
      "19579/19579 [==============================] - ETA: 0s - loss: 0.2036 - acc: 0.937 - 18s - loss: 0.2034 - acc: 0.9378    \n",
      "Epoch 31/38\n",
      "19579/19579 [==============================] - 18s - loss: 0.1948 - acc: 0.9413    \n",
      "Epoch 32/38\n",
      "19579/19579 [==============================] - 16s - loss: 0.1873 - acc: 0.9437    \n",
      "Epoch 33/38\n",
      "19579/19579 [==============================] - 14s - loss: 0.1806 - acc: 0.9456    \n",
      "Epoch 34/38\n",
      "19579/19579 [==============================] - 14s - loss: 0.1736 - acc: 0.9482    \n",
      "Epoch 35/38\n",
      "19579/19579 [==============================] - 14s - loss: 0.1684 - acc: 0.9502    \n",
      "Epoch 36/38\n",
      "19579/19579 [==============================] - 14s - loss: 0.1604 - acc: 0.9511    \n",
      "Epoch 37/38\n",
      "19579/19579 [==============================] - 14s - loss: 0.1551 - acc: 0.9541    \n",
      "Epoch 38/38\n",
      "19579/19579 [==============================] - 15s - loss: 0.1495 - acc: 0.9553    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a1d416ba8>"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit(docs, y,\n",
    "          batch_size=16,\n",
    "          epochs=38)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7968/8392 [===========================>..] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "test_df = pd.read_csv('test.csv')\n",
    "docs = create_docs(test_df)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "y = model.predict_proba(docs)\n",
    "\n",
    "df = pd.read_csv('./sample_submission.csv')\n",
    "for a, i in a2c.items():\n",
    "    df[a] = y[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./fastText_submission_preprocess_min1-ngram1_epoch38.csv', 'w') as f:\n",
    "    f.write('\"id\",\"EAP\",\"HPL\",\"MWS\"\\n')\n",
    "    for (index, row), t in zip(df.iterrows(), test_df.text):\n",
    "        f.write(','.join(['\\\"' + row[\"id\"] + '\\\"'] +  list(map(str, [row['EAP'], row['HPL'], row['MWS']]))) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# min=1, ngram=1, preprocess upper special dim=20 test=0.15\n",
    "\n",
    "# eppoh 40 since previous result is not good...\n",
    "#  0 1.0851 0.4049 1.0840 0.3956\n",
    "#  1 1.0721 0.4067 1.0625 0.3997\n",
    "#  2 1.0372 0.4494 1.0153 0.4760\n",
    "#  3 0.9778 0.5570 0.9535 0.6919\n",
    "#  4 0.9042 0.6492 0.8816 0.6779\n",
    "#  5 0.8326 0.7074 0.8199 0.7283\n",
    "#  6 0.7666 0.7405 0.7659 0.7293\n",
    "#  7 0.7096 0.7651 0.7179 0.7320\n",
    "#  8 0.6606 0.7821 0.6797 0.7477\n",
    "#  9 0.6165 0.7938 0.6554 0.7508\n",
    "# 10 0.5786 0.8069 0.6121 0.7797\n",
    "# 11 0.5444 0.8185 0.5876 0.7865\n",
    "# 12 0.5126 0.8289 0.5651 0.7916\n",
    "# 13 0.4851 0.8408 0.5426 0.8066\n",
    "# 14 0.4590 0.8461 0.5288 0.7947\n",
    "# 15 0.4352 0.8566 0.5089 0.8035\n",
    "# 16 0.4118 0.8659 0.4905 0.8138\n",
    "# 17 0.3919 0.8709 0.4754 0.8189\n",
    "# 18 0.3738 0.8773 0.4657 0.8240\n",
    "# 19 0.3549 0.8860 0.4565 0.8338\n",
    "# 20 0.3378 0.8953 0.4465 0.8223\n",
    "# 21 0.3215 0.9009 0.4300 0.8352\n",
    "# 22 0.3075 0.9047 0.4211 0.8437\n",
    "# 23 0.2948 0.9082 0.4121 0.8403\n",
    "# 24 0.2816 0.9122 0.4114 0.8458\n",
    "# 25 0.2692 0.9167 0.3990 0.8444\n",
    "# 26 0.2576 0.9225 0.4038 0.8369\n",
    "# 27 0.2471 0.9254 0.3846 0.8485\n",
    "# 28 0.2377 0.9298 0.3855 0.8509\n",
    "# 29 0.2275 0.9323 0.4034 0.8366\n",
    "# 30 0.2174 0.9334 0.3786 0.8492\n",
    "# 31 0.2096 0.9385 0.3894 0.8413\n",
    "# 32 0.2019 0.9394 0.3703 0.8597\n",
    "# 33 0.1940 0.9430 0.3634 0.8594\n",
    "# 34 0.1862 0.9451 0.3590 0.8553\n",
    "# 35 0.1792 0.9468 0.3681 0.8550\n",
    "# 36 0.1730 0.9499 0.3610 0.8533\n",
    "# 37 0.1666 0.9509 0.3589 0.8662\n",
    "# 38 0.1605 0.9536 0.3477 0.8655\n",
    "# 39 0.1550 0.9554 0.3494 0.8621\n",
    "# 40 0.1497 0.9561 0.3462 0.8601 xxx\n",
    "# 41 0.1448 0.9572 0.3858 0.8471\n",
    "# 42 0.1390 0.9600 0.3600 0.8645\n",
    "# 43 0.1351 0.9620 0.3530 0.8679\n",
    "# 44 0.1298 0.9632 0.3835 0.8515"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
