{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding, Lambda, Input, Concatenate\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import word2vec\n",
    "from utils import preprocess\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dims = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'This process, however, afforded me no means of ascertaining the dimensions of my dungeon; as I might make its circuit, and return to the point whence I set out, without being aware of the fact; so perfectly uniform seemed the wall.'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.text[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = {name : defaultdict(int) for name in set(df.author)}\n",
    "for (text, author) in zip(df.text, df.author):\n",
    "    text = text.replace(' ', '')\n",
    "    for c in text:\n",
    "        counter[author][c] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c EAP   MWS   HPL   \n",
      ". 8406 5761 5908 \n",
      "Y 282 234 111 \n",
      "i 60952 46080 44250 \n",
      "B 835 395 533 \n",
      "y 17001 14877 12534 \n",
      "I 4846 4917 3480 \n",
      "w 17507 16062 15554 \n",
      "Æ 1 0 4 \n",
      "q 1030 677 779 \n",
      "æ 36 0 10 \n",
      "Ν 0 0 1 \n",
      "x 1951 1267 1061 \n",
      "X 17 4 5 \n",
      "ê 28 0 2 \n",
      "g 16088 12601 14951 \n",
      "' 1334 476 1710 \n",
      "ä 1 0 6 \n",
      "l 35371 27819 30273 \n",
      "F 383 232 269 \n",
      "N 411 204 345 \n",
      "Q 21 7 10 \n",
      "Å 0 0 1 \n",
      "o 67145 53386 50996 \n",
      "U 166 46 94 \n",
      "E 435 445 281 \n",
      "p 17422 12361 10965 \n",
      "a 68525 55274 56815 \n",
      "d 36862 35315 33366 \n",
      "ï 0 0 7 \n",
      ": 176 339 47 \n",
      "L 458 307 249 \n",
      "é 47 0 15 \n",
      "Π 0 0 1 \n",
      "H 864 669 741 \n",
      "ñ 0 0 7 \n",
      "ἶ 0 0 2 \n",
      "G 313 246 318 \n",
      "ç 1 0 0 \n",
      "m 22792 20471 17622 \n",
      "u 26311 21025 19519 \n",
      "R 258 385 237 \n",
      "â 6 0 0 \n",
      "A 1258 943 1167 \n",
      "ë 0 0 12 \n",
      "z 634 400 529 \n",
      "\" 2987 1469 513 \n",
      "n 62636 50291 50879 \n",
      "à 10 0 0 \n",
      "O 414 282 503 \n",
      "ô 8 0 0 \n",
      "ü 1 0 5 \n",
      "? 510 419 169 \n",
      "α 0 0 2 \n",
      "V 156 57 67 \n",
      "K 86 35 176 \n",
      "ö 16 0 3 \n",
      "b 13245 9611 10636 \n",
      "Ο 0 0 3 \n",
      "f 22354 18351 16272 \n",
      "P 442 365 320 \n",
      "c 24127 17911 18338 \n",
      "j 683 682 424 \n",
      "M 1065 415 645 \n",
      "; 1354 2662 1143 \n",
      "r 51221 44042 40590 \n",
      "S 729 578 841 \n",
      "î 1 0 0 \n",
      "è 15 0 0 \n",
      "Σ 0 0 1 \n",
      "k 4277 3707 5204 \n",
      "t 82426 63142 62235 \n",
      "h 51580 43738 42770 \n",
      "s 53841 45962 43915 \n",
      "e 114885 97515 88259 \n",
      "D 491 227 334 \n",
      "W 739 681 732 \n",
      "v 9624 7948 6529 \n",
      "δ 0 0 2 \n",
      ", 17594 12045 8581 \n",
      "J 164 66 210 \n",
      "Υ 0 0 1 \n",
      "Z 23 2 51 \n",
      "T 2217 1230 1583 \n",
      "C 395 308 439 \n"
     ]
    }
   ],
   "source": [
    "chars = set()\n",
    "for v in counter.values():\n",
    "    chars |= v.keys()\n",
    "    \n",
    "names = [author for author in counter.keys()]\n",
    "special_latters = set()\n",
    "print('c ', end='')\n",
    "for n in names:\n",
    "    print(n, end='   ')\n",
    "print()\n",
    "for c in chars:    \n",
    "    print(c, end=' ')\n",
    "    for n in names:\n",
    "        print(counter[n][c], end=' ')\n",
    "    print()\n",
    "    special_latters.add(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴とか\n",
    "\n",
    "- MWSはasciiしか系しか使わない（イギリスの作家だから？、それ以外の2人は使い、アメリカ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "special_latters = special_latters - set(string.ascii_uppercase) - set(string.ascii_lowercase) - set(',.:;\"\\'?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'à ô ü α Æ æ ö Ν ê Ο ä Å î è ï Σ é Π ñ ἶ ç δ â ë Υ'"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(special_latters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for (text, author) in zip(df.text, df.author):\n",
    "#     if len(set(text) & special_latters):\n",
    "#         print(set(text) & special_latters, author, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_docs(df, n_gram_max=1):\n",
    "    docs = []\n",
    "\n",
    "    for i, text in enumerate(df.text):    \n",
    "        def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "\n",
    "        doc = preprocess(text).split()\n",
    "        \n",
    "        prod = special_latters & set(text)\n",
    "        special_chars = ''\n",
    "        if prod:\n",
    "            for c in prod:\n",
    "                freq = text.count(c)\n",
    "                special_chars += (' {} '.format(c) * freq)            \n",
    "                \n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)) + special_chars)\n",
    "        \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = create_docs(df, n_gram_max=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=2, filters='', lower=False)\n",
    "tokenizer.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 1\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=num_words, filters='', lower=False)\n",
    "tokenizer.fit_on_texts(docs)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "maxlen = max([len(i) for i in  docs])\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "28285"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array([a2c[a] for a in df.author])\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28285, 875)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = np.max(docs)+1\n",
    "input_dim, maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vec = word2vec.KeyedVectors.load_word2vec_format('./fastText/model.vec') # ./fasttext skipgram -input ../data/fasttext-inputs.txt -output model -minCount 1  -neg 15 -ws 10 -epoch 7\n",
    "fasttext_dim = vec.vector_size\n",
    "fasttext_emb = np.zeros((len(df), fasttext_dim))\n",
    "\n",
    "for i, text in enumerate(df.text):\n",
    "    doc_vec = np.zeros(fasttext_dim)\n",
    "    words = preprocess(text).lower().split()\n",
    "    for w in words:\n",
    "        doc_vec += vec[w]\n",
    "    doc_vec /= len(words)\n",
    "    fasttext_emb[i] = doc_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "inputs = Input(shape=(None, ))\n",
    "emb = Embedding(input_dim=input_dim, output_dim=embedding_dims)(inputs)\n",
    "mean = GlobalAveragePooling1D()(emb)\n",
    "\n",
    "fasttext_input = Input(shape=(fasttext_dim, ))\n",
    "concat = Concatenate()([mean, fasttext_input])\n",
    "out = Dense(3, activation='softmax')(concat)\n",
    "\n",
    "model = Model(inputs=[inputs, fasttext_input], outputs=[out])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19579, 100), (19579, 875))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_emb.shape, docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# # train validation case\n",
    "# epochs = 45\n",
    "# x_train, x_test, x_fast_train, x_fast_test, y_train, y_test = train_test_split(docs, fasttext_emb, y, test_size=0.15)\n",
    "\n",
    "# # n_samples = x_train.shape[0]\n",
    "# # class_weight = {c : (n_samples/(np.sum(y_train, axis=0)[c] * 3)) for c in range(3)}\n",
    "\n",
    "# hist = model.fit([x_train, x_fast_train], [y_train],\n",
    "#                  batch_size=16,\n",
    "#                  validation_data=([x_test, x_fast_test], [y_test]),\n",
    "#                  epochs=epochs,\n",
    "#                  callbacks=[EarlyStopping(patience=4, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = hist.history\n",
    "# for i in range(len(history['acc'])):\n",
    "#     print('{0:2d}'.format(i), \n",
    "#           '{0:.4f}'.format(history['loss'][i]), \n",
    "#           '{0:.4f}'.format(history['acc'][i]),\n",
    "#           '{0:.4f}'.format(history['val_loss'][i]),\n",
    "#           '{0:.4f}'.format(history['val_acc'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/36\n",
      "19579/19579 [==============================] - 16s - loss: 0.9013 - acc: 0.6377    \n",
      "Epoch 2/36\n",
      "19579/19579 [==============================] - 15s - loss: 0.7123 - acc: 0.7470    \n",
      "Epoch 3/36\n",
      "19579/19579 [==============================] - 18s - loss: 0.6313 - acc: 0.7712    \n",
      "Epoch 4/36\n",
      "19579/19579 [==============================] - 23s - loss: 0.5840 - acc: 0.7842    \n",
      "Epoch 5/36\n",
      "19579/19579 [==============================] - 23s - loss: 0.5498 - acc: 0.7958    \n",
      "Epoch 6/36\n",
      "19579/19579 [==============================] - 31s - loss: 0.5222 - acc: 0.8045    \n",
      "Epoch 7/36\n",
      "19579/19579 [==============================] - 18s - loss: 0.4981 - acc: 0.8120    \n",
      "Epoch 8/36\n",
      "19579/19579 [==============================] - 17s - loss: 0.4759 - acc: 0.8201    \n",
      "Epoch 9/36\n",
      "19579/19579 [==============================] - 17s - loss: 0.4554 - acc: 0.8292    \n",
      "Epoch 10/36\n",
      "19579/19579 [==============================] - 17s - loss: 0.4357 - acc: 0.8377    \n",
      "Epoch 11/36\n",
      "19579/19579 [==============================] - 21s - loss: 0.4168 - acc: 0.8455    \n",
      "Epoch 12/36\n",
      "19579/19579 [==============================] - 21s - loss: 0.3989 - acc: 0.8541    \n",
      "Epoch 13/36\n",
      "19579/19579 [==============================] - 21s - loss: 0.3809 - acc: 0.8612    \n",
      "Epoch 14/36\n",
      "19579/19579 [==============================] - 21s - loss: 0.3648 - acc: 0.8686    \n",
      "Epoch 15/36\n",
      "19579/19579 [==============================] - 22s - loss: 0.3483 - acc: 0.8745    \n",
      "Epoch 16/36\n",
      "19579/19579 [==============================] - 24s - loss: 0.3338 - acc: 0.8806    \n",
      "Epoch 17/36\n",
      "19579/19579 [==============================] - 23s - loss: 0.3188 - acc: 0.8868    \n",
      "Epoch 18/36\n",
      "19579/19579 [==============================] - 22s - loss: 0.3053 - acc: 0.8913    \n",
      "Epoch 19/36\n",
      "19579/19579 [==============================] - 22s - loss: 0.2917 - acc: 0.8972    \n",
      "Epoch 20/36\n",
      "19579/19579 [==============================] - 22s - loss: 0.2791 - acc: 0.9037    \n",
      "Epoch 21/36\n",
      "19579/19579 [==============================] - 20s - loss: 0.2664 - acc: 0.9078    \n",
      "Epoch 22/36\n",
      "19579/19579 [==============================] - 21s - loss: 0.2549 - acc: 0.9140    \n",
      "Epoch 23/36\n",
      "19579/19579 [==============================] - 22s - loss: 0.2443 - acc: 0.9163    \n",
      "Epoch 24/36\n",
      "19579/19579 [==============================] - 22s - loss: 0.2337 - acc: 0.9213    \n",
      "Epoch 25/36\n",
      "19579/19579 [==============================] - 20s - loss: 0.2234 - acc: 0.9261    \n",
      "Epoch 26/36\n",
      "19579/19579 [==============================] - 20s - loss: 0.2149 - acc: 0.9280    \n",
      "Epoch 27/36\n",
      "19579/19579 [==============================] - 20s - loss: 0.2053 - acc: 0.9337    \n",
      "Epoch 28/36\n",
      "19579/19579 [==============================] - 22s - loss: 0.1977 - acc: 0.9351    \n",
      "Epoch 29/36\n",
      "19579/19579 [==============================] - 22s - loss: 0.1899 - acc: 0.9390    \n",
      "Epoch 30/36\n",
      "19579/19579 [==============================] - 23s - loss: 0.1818 - acc: 0.9419    \n",
      "Epoch 31/36\n",
      "19579/19579 [==============================] - 22s - loss: 0.1742 - acc: 0.9440    \n",
      "Epoch 32/36\n",
      "19579/19579 [==============================] - 21s - loss: 0.1675 - acc: 0.9466    \n",
      "Epoch 33/36\n",
      "19579/19579 [==============================] - 22s - loss: 0.1619 - acc: 0.9479    \n",
      "Epoch 34/36\n",
      "19579/19579 [==============================] - 21s - loss: 0.1549 - acc: 0.9510    \n",
      "Epoch 35/36\n",
      "19579/19579 [==============================] - 19s - loss: 0.1504 - acc: 0.9525    \n",
      "Epoch 36/36\n",
      "19579/19579 [==============================] - 20s - loss: 0.1430 - acc: 0.9549    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a24fa7518>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([docs, fasttext_emb], [y],\n",
    "          batch_size=16,\n",
    "          epochs=36)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./data/test.csv')\n",
    "docs = create_docs(test_df)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "\n",
    "# create feature for test\n",
    "\n",
    "fasttext_emb = np.zeros((len(test_df), fasttext_dim))\n",
    "for i, text in enumerate(test_df.text):\n",
    "    doc_vec = np.zeros(fasttext_dim)\n",
    "    words = preprocess(text).lower().split()\n",
    "    for w in words:\n",
    "        if w in vec.vocab:\n",
    "            doc_vec += vec[w]\n",
    "    doc_vec /= len(words)\n",
    "    fasttext_emb[i] = doc_vec\n",
    "    \n",
    "model_predict_prob = Model(inputs=[model.layers[0].input, model.layers[3].input],\n",
    "                                 outputs=[model.layers[-1].output])\n",
    "prob = model_predict_prob.predict([docs, fasttext_emb])\n",
    "\n",
    "df = pd.read_csv('./data/sample_submission.csv')\n",
    "for a, i in a2c.items():\n",
    "    df[a] = prob[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./results/fastText_submission_preprocess_min1-ngram1_epoch36-unsuper-concat.csv', 'w') as f:\n",
    "    f.write('\"id\",\"EAP\",\"HPL\",\"MWS\"\\n')\n",
    "    for (index, row), t in zip(df.iterrows(), test_df.text):\n",
    "        f.write(','.join(['\\\"' + row['id'] + '\\\"'] +  list(map(str, [row['EAP'], row['HPL'], row['MWS']]))) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# min=1, ngram=1, preprocess upper special dim=20 test=0.15\n",
    "\n",
    "# eppoh 40 since previous result is not good...\n",
    "#  0 0.9201 0.6252 0.7953 0.7385\n",
    "#  1 0.7406 0.7405 0.6770 0.7671\n",
    "#  2 0.6576 0.7613 0.6168 0.7794\n",
    "#  3 0.6085 0.7769 0.5794 0.7889\n",
    "#  4 0.5733 0.7877 0.5476 0.7954\n",
    "#  5 0.5467 0.7980 0.5271 0.7971\n",
    "#  6 0.5227 0.8035 0.5073 0.8073\n",
    "#  7 0.5020 0.8114 0.4912 0.8124\n",
    "#  8 0.4820 0.8197 0.4800 0.8165\n",
    "#  9 0.4626 0.8279 0.4723 0.8158\n",
    "# 10 0.4447 0.8354 0.4529 0.8260\n",
    "# 11 0.4277 0.8427 0.4431 0.8335\n",
    "# 12 0.4104 0.8491 0.4345 0.8335\n",
    "# 13 0.3947 0.8562 0.4235 0.8383\n",
    "# 14 0.3787 0.8622 0.4183 0.8366\n",
    "# 15 0.3638 0.8674 0.4107 0.8390\n",
    "# 16 0.3487 0.8757 0.4016 0.8434\n",
    "# 17 0.3350 0.8799 0.3920 0.8481\n",
    "# 18 0.3227 0.8864 0.3884 0.8458\n",
    "# 19 0.3097 0.8917 0.3863 0.8488\n",
    "# 20 0.2963 0.8974 0.3790 0.8498\n",
    "# 21 0.2834 0.9027 0.3705 0.8573\n",
    "# 22 0.2731 0.9069 0.3710 0.8539\n",
    "# 23 0.2621 0.9116 0.3586 0.8604\n",
    "# 24 0.2519 0.9162 0.3624 0.8570\n",
    "# 25 0.2419 0.9195 0.3511 0.8611\n",
    "# 26 0.2317 0.9245 0.3573 0.8604\n",
    "# 27 0.2228 0.9277 0.3453 0.8638\n",
    "# 28 0.2145 0.9294 0.3470 0.8655\n",
    "# 29 0.2058 0.9339 0.3626 0.8597\n",
    "# 30 0.1973 0.9362 0.3436 0.8665\n",
    "# 31 0.1906 0.9404 0.3570 0.8614\n",
    "# 32 0.1829 0.9415 0.3395 0.8699\n",
    "# 33 0.1763 0.9445 0.3381 0.8686\n",
    "# 34 0.1692 0.9463 0.3346 0.8716\n",
    "# 35 0.1630 0.9493 0.3388 0.8682\n",
    "# 36 0.1571 0.9512 0.3372 0.8716\n",
    "# 37 0.1518 0.9528 0.3459 0.8618\n",
    "# 38 0.1461 0.9549 0.3275 0.8733 xx\n",
    "# 39 0.1405 0.9582 0.3307 0.8733\n",
    "# 40 0.1355 0.9584 0.3319 0.8767\n",
    "# 41 0.1312 0.9590 0.3723 0.8509\n",
    "# 42 0.1262 0.9622 0.3545 0.8621\n",
    "# 43 0.1228 0.9617 0.3379 0.8682"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
