{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding, Lambda, Input, Concatenate, Add, Multiply\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping\n",
    "from keras.models import Sequential, Model\n",
    "\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import train_test_split\n",
    "from gensim.models import word2vec\n",
    "from utils import preprocess\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dims = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "counter = {name : defaultdict(int) for name in set(df.author)}\n",
    "for (text, author) in zip(df.text, df.author):\n",
    "    text = text.replace(' ', '')\n",
    "    for c in text:\n",
    "        counter[author][c] += 1\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c HPL   MWS   EAP   \n",
      "ç 0 0 1 \n",
      "ë 12 0 0 \n",
      "s 43915 45962 53841 \n",
      "G 318 246 313 \n",
      "Σ 1 0 0 \n",
      "ñ 7 0 0 \n",
      "â 0 0 6 \n",
      "H 741 669 864 \n",
      "î 0 0 1 \n",
      "δ 2 0 0 \n",
      "Å 1 0 0 \n",
      "F 269 232 383 \n",
      "α 2 0 0 \n",
      "v 6529 7948 9624 \n",
      "æ 10 0 36 \n",
      "b 10636 9611 13245 \n",
      "w 15554 16062 17507 \n",
      "ï 7 0 0 \n",
      "i 44250 46080 60952 \n",
      "r 40590 44042 51221 \n",
      "j 424 682 683 \n",
      "Ο 3 0 0 \n",
      "f 16272 18351 22354 \n",
      "U 94 46 166 \n",
      "A 1167 943 1258 \n",
      "d 33366 35315 36862 \n",
      "K 176 35 86 \n",
      "; 1143 2662 1354 \n",
      "e 88259 97515 114885 \n",
      "x 1061 1267 1951 \n",
      "Ν 1 0 0 \n",
      "ô 0 0 8 \n",
      "S 841 578 729 \n",
      "D 334 227 491 \n",
      ": 47 339 176 \n",
      "E 281 445 435 \n",
      ". 5908 5761 8406 \n",
      "l 30273 27819 35371 \n",
      "? 169 419 510 \n",
      "Z 51 2 23 \n",
      "X 5 4 17 \n",
      "a 56815 55274 68525 \n",
      "' 1710 476 1334 \n",
      "M 645 415 1065 \n",
      "B 533 395 835 \n",
      "é 15 0 47 \n",
      "Y 111 234 282 \n",
      "P 320 365 442 \n",
      "I 3480 4917 4846 \n",
      "Π 1 0 0 \n",
      "T 1583 1230 2217 \n",
      "\" 513 1469 2987 \n",
      "g 14951 12601 16088 \n",
      ", 8581 12045 17594 \n",
      "t 62235 63142 82426 \n",
      "ü 5 0 1 \n",
      "q 779 677 1030 \n",
      "Q 10 7 21 \n",
      "z 529 400 634 \n",
      "o 50996 53386 67145 \n",
      "N 345 204 411 \n",
      "V 67 57 156 \n",
      "L 249 307 458 \n",
      "n 50879 50291 62636 \n",
      "c 18338 17911 24127 \n",
      "Æ 4 0 1 \n",
      "m 17622 20471 22792 \n",
      "Υ 1 0 0 \n",
      "J 210 66 164 \n",
      "p 10965 12361 17422 \n",
      "h 42770 43738 51580 \n",
      "W 732 681 739 \n",
      "y 12534 14877 17001 \n",
      "R 237 385 258 \n",
      "ö 3 0 16 \n",
      "O 503 282 414 \n",
      "ἶ 2 0 0 \n",
      "à 0 0 10 \n",
      "è 0 0 15 \n",
      "C 439 308 395 \n",
      "ä 6 0 1 \n",
      "u 19519 21025 26311 \n",
      "k 5204 3707 4277 \n",
      "ê 2 0 28 \n"
     ]
    }
   ],
   "source": [
    "chars = set()\n",
    "for v in counter.values():\n",
    "    chars |= v.keys()\n",
    "    \n",
    "names = [author for author in counter.keys()]\n",
    "special_latters = set()\n",
    "print('c ', end='')\n",
    "for n in names:\n",
    "    print(n, end='   ')\n",
    "print()\n",
    "for c in chars:    \n",
    "    print(c, end=' ')\n",
    "    for n in names:\n",
    "        print(counter[n][c], end=' ')\n",
    "    print()\n",
    "    special_latters.add(c)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 特徴とか\n",
    "\n",
    "- MWSはasciiしか系しか使わない（イギリスの作家だから？、それ以外の2人は使い、アメリカ）"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "special_latters = special_latters - set(string.ascii_uppercase) - set(string.ascii_lowercase) - set(',.:;\"\\'?')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ç ë Σ ñ â é î Π δ Å α ü æ ï Ο Æ Υ Ν ô ö ἶ à è ä ê'"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "' '.join(special_latters)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for (text, author) in zip(df.text, df.author):\n",
    "#     if len(set(text) & special_latters):\n",
    "#         print(set(text) & special_latters, author, text)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_docs(df, n_gram_max=1):\n",
    "    docs = []\n",
    "\n",
    "    for i, text in enumerate(df.text):    \n",
    "        def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "\n",
    "        doc = preprocess(text).split()\n",
    "        \n",
    "        prod = special_latters & set(text)\n",
    "        special_chars = ''\n",
    "        if prod:\n",
    "            for c in prod:\n",
    "                freq = text.count(c)\n",
    "                special_chars += (' {} '.format(c) * freq)            \n",
    "                \n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)) + special_chars)\n",
    "        \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs = create_docs(df, n_gram_max=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=2, filters='', lower=False)\n",
    "tokenizer.fit_on_texts(docs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 2\n",
    "num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = Tokenizer(num_words=num_words, filters='', lower=False)\n",
    "tokenizer.fit_on_texts(docs)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "maxlen = max([len(i) for i in docs])\n",
    "maxlen = 256\n",
    "# 512 0.3326\n",
    "# min=1 256 0.3242 # min = 2 0.3236\n",
    "\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "embedding_dims = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "257110"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(tokenizer.word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "a2c = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array([a2c[a] for a in df.author])\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(76614, 256)"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_dim = np.max(docs)+1\n",
    "input_dim, maxlen"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ./fasttext skipgram -input ../data/fasttext-inputs.txt -output skip20 -minCount 1 -neg 15 -ws 20 -epoch 7 -dim 20\n",
    "vec = word2vec.KeyedVectors.load_word2vec_format('./fastText/skip20_min2_neg15_epoch_7_ws_20.vec')\n",
    "fasttext_dim = vec.vector_size\n",
    "fasttext_emb = np.zeros((len(df), fasttext_dim))\n",
    "\n",
    "for i, text in enumerate(df.text):\n",
    "    doc_vec = np.zeros(fasttext_dim)\n",
    "    words = preprocess(text).lower().split()\n",
    "    num_words = 0\n",
    "    for w in words:\n",
    "        if w in vec.vocab:\n",
    "            doc_vec += vec[w]\n",
    "            num_words += 1\n",
    "    doc_vec /= num_words\n",
    "    fasttext_emb[i] = doc_vec\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import Dropout\n",
    "inputs = Input(shape=(None, ))\n",
    "emb = Embedding(input_dim=input_dim, output_dim=embedding_dims)(inputs)\n",
    "mean = GlobalAveragePooling1D()(emb)\n",
    "\n",
    "fasttext_input = Input(shape=(fasttext_dim, ))\n",
    "concat = Concatenate()([mean, fasttext_input])\n",
    "out = Dense(3, activation='softmax')(concat)\n",
    "\n",
    "model = Model(inputs=[inputs, fasttext_input], outputs=[out])\n",
    "\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((19579, 20), (19579, 256))"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "fasttext_emb.shape, docs.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# # train validation case\n",
    "# epochs = 45\n",
    "# x_train, x_test, x_fast_train, x_fast_test, y_train, y_test = train_test_split(docs, fasttext_emb, y, test_size=0.15)\n",
    "\n",
    "# # n_samples = x_train.shape[0]\n",
    "# # class_weight = {c : (n_samples/(np.sum(y_train, axis=0)[c] * 3)) for c in range(3)}\n",
    "\n",
    "# hist = model.fit([x_train, x_fast_train], [y_train],\n",
    "#                  batch_size=16,\n",
    "#                  validation_data=([x_test, x_fast_test], [y_test]),\n",
    "#                  epochs=epochs,\n",
    "#                  callbacks=[EarlyStopping(patience=4, monitor='val_loss')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# history = hist.history\n",
    "# for i in range(len(history['acc'])):\n",
    "#     print('{0:2d}'.format(i), \n",
    "#           '{0:.4f}'.format(history['loss'][i]), \n",
    "#           '{0:.4f}'.format(history['acc'][i]),\n",
    "#           '{0:.4f}'.format(history['val_loss'][i]),\n",
    "#           '{0:.4f}'.format(history['val_acc'][i]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/14\n",
      "19579/19579 [==============================] - 34s - loss: 0.9376 - acc: 0.6038    \n",
      "Epoch 2/14\n",
      "19579/19579 [==============================] - 35s - loss: 0.6733 - acc: 0.7993    \n",
      "Epoch 3/14\n",
      "19579/19579 [==============================] - 34s - loss: 0.5036 - acc: 0.8451    \n",
      "Epoch 4/14\n",
      "19579/19579 [==============================] - 33s - loss: 0.3986 - acc: 0.8751    \n",
      "Epoch 5/14\n",
      "19579/19579 [==============================] - 36s - loss: 0.3231 - acc: 0.9006    \n",
      "Epoch 6/14\n",
      "19579/19579 [==============================] - 37s - loss: 0.2637 - acc: 0.9219    \n",
      "Epoch 7/14\n",
      "19579/19579 [==============================] - 33s - loss: 0.2162 - acc: 0.9372    \n",
      "Epoch 8/14\n",
      "19579/19579 [==============================] - 36s - loss: 0.1770 - acc: 0.9519    \n",
      "Epoch 9/14\n",
      "19579/19579 [==============================] - 36s - loss: 0.1457 - acc: 0.9623    \n",
      "Epoch 10/14\n",
      "19579/19579 [==============================] - 34s - loss: 0.1197 - acc: 0.9705    \n",
      "Epoch 11/14\n",
      "19579/19579 [==============================] - 34s - loss: 0.0980 - acc: 0.9768    \n",
      "Epoch 12/14\n",
      "19579/19579 [==============================] - 34s - loss: 0.0811 - acc: 0.9822    \n",
      "Epoch 13/14\n",
      "19579/19579 [==============================] - 34s - loss: 0.0668 - acc: 0.9850    \n",
      "Epoch 14/14\n",
      "19579/19579 [==============================] - 35s - loss: 0.0554 - acc: 0.9883    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x1a273a8e10>"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fit([docs, fasttext_emb], [y],\n",
    "          batch_size=16,\n",
    "          epochs=14,\n",
    "          shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "test_df = pd.read_csv('./data/test.csv')\n",
    "docs = create_docs(test_df)\n",
    "docs = tokenizer.texts_to_sequences(docs)\n",
    "docs = pad_sequences(sequences=docs, maxlen=maxlen)\n",
    "\n",
    "# create feature for test\n",
    "\n",
    "fasttext_emb = np.zeros((len(test_df), fasttext_dim))\n",
    "for i, text in enumerate(test_df.text):\n",
    "    doc_vec = np.zeros(fasttext_dim)\n",
    "    words = preprocess(text).lower().split()\n",
    "    num_words = 0\n",
    "    for w in words:\n",
    "        if w in vec.vocab:\n",
    "            doc_vec += vec[w]\n",
    "            num_words += 1\n",
    "    doc_vec /= num_words\n",
    "    fasttext_emb[i] = doc_vec\n",
    "    \n",
    "model_predict_prob = Model(inputs=[model.layers[0].input, model.layers[3].input],\n",
    "                                 outputs=[model.layers[-1].output])\n",
    "prob = model_predict_prob.predict([docs, fasttext_emb])\n",
    "\n",
    "df = pd.read_csv('./data/sample_submission.csv')\n",
    "for a, i in a2c.items():\n",
    "    df[a] = prob[:, i]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "with open('./results/fastText_submission_preprocess_min2-ngram2-maxlength_epoch14-unsuper-concat.csv', 'w') as f:\n",
    "    f.write('\"id\",\"EAP\",\"HPL\",\"MWS\"\\n')\n",
    "    for (index, row), t in zip(df.iterrows(), test_df.text):\n",
    "        f.write(','.join(['\\\"' + row['id'] + '\\\"'] +  list(map(str, [row['EAP'], row['HPL'], row['MWS']]))) + '\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# min=2, ngram=2, preprocess upper special dim=20 test=0.15\n",
    "\n",
    "# eppoh 40 since previous result is not good...\n",
    "\n",
    "#  0 0.9574 0.5803 0.8303 0.7732\n",
    "#  1 0.7220 0.7836 0.6339 0.8131\n",
    "#  2 0.5502 0.8333 0.5265 0.8260\n",
    "#  3 0.4399 0.8664 0.4612 0.8427\n",
    "#  4 0.3595 0.8909 0.4182 0.8522\n",
    "#  5 0.2979 0.9105 0.3882 0.8601\n",
    "#  6 0.2472 0.9305 0.3624 0.8679\n",
    "#  7 0.2064 0.9421 0.3440 0.8693\n",
    "#  8 0.1715 0.9552 0.3339 0.8710\n",
    "#  9 0.1423 0.9653 0.3256 0.8764\n",
    "# 10 0.1184 0.9722 0.3139 0.8795\n",
    "# 11 0.0986 0.9788 0.3125 0.8812\n",
    "# 12 0.0817 0.9823 0.3070 0.8822\n",
    "# 13 0.0688 0.9856 0.3080 0.8815\n",
    "# 14 0.0573 0.9888 0.3056 0.8819\n",
    "# 15 0.0481 0.9902 0.3080 0.8836\n",
    "# 16 0.0404 0.9922 0.3157 0.8808\n",
    "# 17 0.0337 0.9933 0.3222 0.8788\n",
    "# 18 0.0287 0.9947 0.3297 0.8764\n",
    "# 19 0.0240 0.9956 0.3256 0.8798"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
