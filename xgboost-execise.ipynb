{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 227,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.svm import LinearSVC, SVC\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from gensim.models import word2vec\n",
    "import xgboost as xgb\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# from keras.models import Sequential\n",
    "# from keras.utils.np_utils import to_categorical\n",
    "# from keras.layers import Dropout, Dense, Activation\n",
    "# from keras.layers import BatchNormalization\n",
    "\n",
    "from utils import preprocess\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 228,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_split = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 229,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train.csv')\n",
    "df_test = pd.read_csv('./data/test.csv')\n",
    "text = df.text.values\n",
    "text_test = df_test.text.values\n",
    "\n",
    "author2class = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "class2author = ['EAP', 'HPL', 'MWS']\n",
    "y = np.array([author2class[a] for a in df.author])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 230,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vec = word2vec.KeyedVectors.load_word2vec_format('./fastText/model.vec')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 231,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('loved', 0.8400787115097046),\n",
       " ('lovelier', 0.8375217914581299),\n",
       " ('lover', 0.8341007828712463),\n",
       " ('loves', 0.8205721378326416),\n",
       " ('clove', 0.8196173906326294),\n",
       " ('loveliest', 0.8140965700149536),\n",
       " (\"beloved's\", 0.7933589220046997),\n",
       " (\"lover's\", 0.7773091197013855),\n",
       " ('jove', 0.7757792472839355),\n",
       " ('glove', 0.7705390453338623)]"
      ]
     },
     "execution_count": 231,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.most_similar(positive=['love'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 232,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[('theft', 0.8367539048194885),\n",
       " (\"amalthea's\", 0.8247981071472168),\n",
       " ('therefrom', 0.8049554824829102),\n",
       " (\"'υπνος\", 0.7993500232696533),\n",
       " ('cn', 0.7949093580245972),\n",
       " ('xiv', 0.7938920259475708),\n",
       " ('oxydracae', 0.786963701248169),\n",
       " ('cimabué', 0.7844108939170837),\n",
       " ('ottawa', 0.7829217910766602),\n",
       " (\"azrael's\", 0.7828168272972107)]"
      ]
     },
     "execution_count": 232,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec.most_similar(positive=['the'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 233,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vector(vec):\n",
    "    n = vec.vector_size\n",
    "    x = np.zeros((len(df), n))\n",
    "    for i, doc in enumerate(text):\n",
    "        doc_vec = np.zeros(n)\n",
    "        words = preprocess(doc).lower().split()\n",
    "        num_words = 0\n",
    "        for w in words:\n",
    "            if w in vec.vocab:\n",
    "                doc_vec += vec[w]\n",
    "                num_words += 1\n",
    "        doc_vec /= num_words\n",
    "        x[i] = doc_vec\n",
    "\n",
    "    x_test = np.zeros((len(df_test), n))\n",
    "    for i, doc in enumerate(text_test):\n",
    "        doc_vec = np.zeros(n)\n",
    "        words = preprocess(doc).lower().split()\n",
    "        num_words = 0\n",
    "        for w in words:\n",
    "            if w in vec.vocab:\n",
    "                doc_vec += vec[w]\n",
    "                num_words += 1\n",
    "        doc_vec /= num_words\n",
    "        x_test[i] = doc_vec\n",
    "    return x, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 234,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic(x, x_test, seed=7):\n",
    "    num_split = 5\n",
    "    kf = KFold(n_splits=num_split, random_state=seed, shuffle=True)\n",
    "    loss = 0.\n",
    "\n",
    "    predict_prob_features = np.zeros((len(df), 3))\n",
    "    predict_prob_features_test = np.zeros((len(df_test), 3))\n",
    "\n",
    "    for train_index, val_index in kf.split(x):\n",
    "        x_train, x_val = x[train_index], x[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        model = LogisticRegression()\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict_proba(x_val)\n",
    "        predict_prob_features_test += model.predict_proba(x_test)\n",
    "        predict_prob_features[val_index] = y_pred\n",
    "        loss += log_loss(y_pred=y_pred, y_true=y_val)\n",
    "\n",
    "    print(loss/5)\n",
    "    return predict_prob_features, predict_prob_features_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 235,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#  ./fasttext skipgram -input ../data/fasttext-inputs.txt -output model -minCount 1  -neg 15 -ws 10 -epoch 7\n",
    "vec = word2vec.KeyedVectors.load_word2vec_format('./fastText/model.vec')\n",
    "x, x_test = create_vector(vec)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 236,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.521292042282\n"
     ]
    }
   ],
   "source": [
    "predict_prob_features, predict_prob_features_test = logistic(x, x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 237,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for a, c in author2class.items():\n",
    "    df['{}_fasttext_logi'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_fasttext_logi'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 238,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 239,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# vec = word2vec.KeyedVectors.load_word2vec_format('./fastText/skip20_min2_neg15_epoch_7_ws_20.vec') # 0.527408070746\n",
    "# x, x_test = create_vector(vec)\n",
    "# predict_prob_features, predict_prob_features_test = logistic(x, x_test, 7)\n",
    "# for a, c in author2class.items():\n",
    "#     df['{}_fasttext_low_dim_logi'.format(a)] = predict_prob_features[:, c]\n",
    "#     df_test['{}_fasttext_low_dim_logi'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 240,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.802494276952\n"
     ]
    }
   ],
   "source": [
    "vec = word2vec.KeyedVectors.load_word2vec_format('./fastText/cbow100_min1_neg15_epoch_7_ws_10.vec')\n",
    "x, x_test = create_vector(vec)\n",
    "predict_prob_features, predict_prob_features_test = logistic(x, x_test, 9)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# for a, c in author2class.items():\n",
    "#     df['{}_fasttext_cbow_logi'.format(a)] = predict_prob_features[:, c]\n",
    "#     df_test['{}_fasttext_cbow_logi'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 242,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vectorizer_feature(vectorizer, seed=8, num_split=5, alphas=[0.005, 0.007, 0.01, 0.03, 0.05]):\n",
    "    param_grid = dict(alpha=alphas)\n",
    "    print(param_grid, vectorizer)\n",
    "    \n",
    "    kf = KFold(n_splits=num_split, random_state=seed, shuffle=True)\n",
    "    sum_loss = 0.\n",
    "\n",
    "    predict_prob_features = np.zeros((len(df), 3))\n",
    "    predict_prob_features_test = np.zeros((len(df_test), 3))\n",
    "    ite = 0\n",
    "    for train_index, val_index in kf.split(text):\n",
    "        ite += 1\n",
    "        print('{}/{}: #Trains: {}, #Val: {}'.format(ite, num_split, len(train_index), len(val_index)), end=' ')\n",
    "        text_train, text_val = text[train_index], text[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        x_train = vectorizer.fit_transform(text_train)\n",
    "        x_val = vectorizer.transform(text_val)\n",
    "        \n",
    "        \n",
    "        if len(alphas) > 1:\n",
    "            model = MultinomialNB()\n",
    "            clf = GridSearchCV(estimator=model, param_grid=param_grid, cv=10, scoring='neg_log_loss', n_jobs=-1)\n",
    "            clf.fit(x_train, y_train)\n",
    "            model = clf.best_estimator_            \n",
    "        else:\n",
    "            alpha = 1.\n",
    "            if len(alphas) == 1:\n",
    "                alpha = alphas[0]\n",
    "            model = MultinomialNB(alpha)\n",
    "            model.fit(x_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict_proba(x_val)\n",
    "\n",
    "        \n",
    "        # save features\n",
    "        predict_prob_features[val_index] = y_pred\n",
    "        predict_prob_features_test += model.predict_proba(vectorizer.transform(text_test))\n",
    "        \n",
    "        best_param = model.alpha\n",
    "\n",
    "        loss = log_loss(y_pred=y_pred, y_true=y_val)\n",
    "        sum_loss += loss\n",
    "\n",
    "        \n",
    "        print('valLoss: {}, best_param α= {}'.format(loss, best_param))\n",
    "        \n",
    "    print(sum_loss/num_split)\n",
    "    return predict_prob_features, predict_prob_features_test\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 248,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='word')\n",
    "predict_prob_features, predict_prob_features_test = vectorizer_feature(vectorizer, 7, alphas=[0.007]) # from [0.005,0.007, 0.01]\n",
    "for a, c in author2class.items():\n",
    "    df['{}_word_tfidf_NB'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_word_tfidf_NB'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 167,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 5), analyzer='char')\n",
    "predict_prob_features, predict_prob_features_test = vectorizer_feature(vectorizer, 8, alphas=[0.013]) # 0.012, 0.013, 0.014\n",
    "for a, c in author2class.items():\n",
    "    df['{}_char_tfidf_NB'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_char_tfidf_NB'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 176,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': [1.2, 1.4, 1.6]} CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 4), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "1/5: #Trains: 15663, #Val: 3916 valLoss: 0.8398629328746968, best_param α= 1.2\n",
      "2/5: #Trains: 15663, #Val: 3916 valLoss: 0.9363253700528534, best_param α= 1.2\n",
      "3/5: #Trains: 15663, #Val: 3916 "
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m       Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-176-c46eb4301e48>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0mvectorizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mCountVectorizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mngram_range\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m4\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0manalyzer\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'word'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 2\u001b[0;31m \u001b[0mpredict_prob_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpredict_prob_features_test\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mvectorizer_feature\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m9\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0malphas\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1.2\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.4\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m1.6\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-161-4afb8aa9bd26>\u001b[0m in \u001b[0;36mvectorizer_feature\u001b[0;34m(vectorizer, seed, num_split, alphas)\u001b[0m\n\u001b[1;32m     20\u001b[0m         \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mMultinomialNB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     21\u001b[0m         \u001b[0mclf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mGridSearchCV\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mestimator\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mparam_grid\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mparam_grid\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mscoring\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m'neg_log_loss'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mn_jobs\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 22\u001b[0;31m         \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     24\u001b[0m         \u001b[0my_pred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mclf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbest_estimator_\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict_proba\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_val\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    636\u001b[0m                                   error_score=self.error_score)\n\u001b[1;32m    637\u001b[0m           for parameters, (train, test) in product(candidate_params,\n\u001b[0;32m--> 638\u001b[0;31m                                                    cv.split(X, y, groups)))\n\u001b[0m\u001b[1;32m    639\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    640\u001b[0m         \u001b[0;31m# if one choose to see train score, \"out\" will contain train score info\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, iterable)\u001b[0m\n\u001b[1;32m    787\u001b[0m                 \u001b[0;31m# consumption.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    788\u001b[0m                 \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_iterating\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 789\u001b[0;31m             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mretrieve\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    790\u001b[0m             \u001b[0;31m# Make sure that we get a last message telling us we are done\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    791\u001b[0m             \u001b[0melapsed_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_start_time\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/externals/joblib/parallel.py\u001b[0m in \u001b[0;36mretrieve\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    697\u001b[0m             \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    698\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mgetattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_backend\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'supports_timeout'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;32mFalse\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 699\u001b[0;31m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    700\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    701\u001b[0m                     \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_output\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mextend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mjob\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mget\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 602\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    603\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mready\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    604\u001b[0m             \u001b[0;32mraise\u001b[0m \u001b[0mTimeoutError\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.6/multiprocessing/pool.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    597\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    598\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 599\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_event\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    600\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    601\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mget\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtimeout\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    549\u001b[0m             \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_flag\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    550\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 551\u001b[0;31m                 \u001b[0msignaled\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_cond\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mwait\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtimeout\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    552\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0msignaled\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    553\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.6/threading.py\u001b[0m in \u001b[0;36mwait\u001b[0;34m(self, timeout)\u001b[0m\n\u001b[1;32m    293\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m    \u001b[0;31m# restore state no matter what (e.g., KeyboardInterrupt)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    294\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mtimeout\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 295\u001b[0;31m                 \u001b[0mwaiter\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0macquire\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    296\u001b[0m                 \u001b[0mgotit\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    297\u001b[0m             \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 3), analyzer='word')\n",
    "predict_prob_features, predict_prob_features_test = vectorizer_feature(vectorizer, 9, alphas=[1.2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 196,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 4), analyzer='char')\n",
    "predict_prob_features, predict_prob_features_test = vectorizer_feature(vectorizer, seed=7, alphas=[0.1, 0.2, 0.3, 0.4])\n",
    "for a, c in author2class.items():\n",
    "    df['{}_char_count_NB'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_char_count_NB'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': [0.5, 0.1, 1.0, 1.5, 2.0, 2.5, 3.0]} CountVectorizer(analyzer='char_wb', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 5), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "1/5: #Trains: 15663, #Val: 3916 valLoss: 2.911286873284481, best_param α= 2.5\n",
      "2/5: #Trains: 15663, #Val: 3916 valLoss: 2.693418535058206, best_param α= 1.5\n",
      "3/5: #Trains: 15663, #Val: 3916 valLoss: 2.600940347993735, best_param α= 2.0\n",
      "4/5: #Trains: 15663, #Val: 3916 valLoss: 3.018726215529485, best_param α= 2.0\n",
      "5/5: #Trains: 15664, #Val: 3915 valLoss: 2.699994701500262, best_param α= 2.0\n",
      "2.78487333467\n"
     ]
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 5), analyzer='char_wb')\n",
    "predict_prob_features, predict_prob_features_test = vectorizer_feature(vectorizer, seed=10, alphas=[1.5, 2., 2.5]) # 0.5, 0.1, 1., 1.5, 2., 2.5, 3.\n",
    "for a, c in author2class.items():\n",
    "    df['{}_char_wb_count_NB'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_char_wb_count_NB'.format(a)] = predict_prob_features_test[:, c]/num_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# counter = {name : defaultdict(int) for name in set(df.author)}\n",
    "# for (text, author) in zip(df.text, df.author):\n",
    "#     text = text.replace(' ', '')\n",
    "#     for c in text:\n",
    "#         counter[author][c] += 1\n",
    "    \n",
    "# chars = set()\n",
    "# for v in counter.values():\n",
    "#     chars |= v.keys()\n",
    "    \n",
    "# names = [author for author in counter.keys()]\n",
    "# special_latters = set()\n",
    "\n",
    "# for c in chars:    \n",
    "#     special_latters.add(c)\n",
    "# special_latters = special_latters - "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['num_chars'] = np.array([len(t) for t in df.text])\n",
    "df_test['num_chars'] = np.array([len(t) for t in df_test.text])\n",
    "\n",
    "df['num_words'] = np.array([len(t.split()) for t in df.text])\n",
    "df_test['num_words'] = np.array([len(t.split()) for t in df_test.text])\n",
    "\n",
    "df['average_num_chars'] = np.array([np.mean([len(word) for word in t.split()]) for t in df.text])\n",
    "df_test['average_num_chars'] = np.array([np.mean([len(word) for word in t.split()]) for t in df_test.text])\n",
    "\n",
    "special = ',' # ',.:;\"\\'?'\n",
    "for c in special:\n",
    "    df['num_'+c] = np.array([t.count(c) for t in df.text])\n",
    "    df_test['num_'+c] = np.array([t.count(c) for t in df_test.text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# num_special_chars = []\n",
    "# for t in df.text:\n",
    "#     prod = set(t) - normal_latters\n",
    "#     c = 0\n",
    "#     for p in prod:\n",
    "#         c += t.count(p)\n",
    "    \n",
    "#     num_special_chars.append(c)\n",
    "\n",
    "# df['num_special_chars'] = np.array(num_special_chars)\n",
    "\n",
    "# num_special_chars = []\n",
    "# for t in df_test.text:\n",
    "#     prod = set(t) - normal_latters\n",
    "#     c = 0\n",
    "#     for p in prod:\n",
    "#         c += t.count(p)\n",
    "    \n",
    "#     num_special_chars.append(c)\n",
    "\n",
    "# df_test['num_special_chars'] = np.array(num_special_chars)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.drop('id', axis=1, inplace=True)\n",
    "df.drop('text', axis=1, inplace=True)\n",
    "df.drop('author', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# kf = model_selection.KFold(n_splits=num_split, shuffle=True, random_state=2017)\n",
    "# xgb_model = xgb.XGBClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['EAP_fasttext_logi', 'HPL_fasttext_logi', 'MWS_fasttext_logi',\n",
       "       'EAP_char_tfidf_NB', 'HPL_char_tfidf_NB', 'MWS_char_tfidf_NB',\n",
       "       'EAP_word_tfidf_NB', 'HPL_word_tfidf_NB', 'MWS_word_tfidf_NB',\n",
       "       'EAP_word_count_NB', 'HPL_word_count_NB', 'MWS_word_count_NB',\n",
       "       'EAP_char_count_NB', 'HPL_char_count_NB', 'MWS_char_count_NB',\n",
       "       'num_chars', 'num_words', 'average_num_chars', 'num_,'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.keys()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# df.drop('EAP_fasttext_logi', axis=1, inplace=True)\n",
    "# df.drop('HPL_fasttext_logi', axis=1, inplace=True)\n",
    "# df.drop('MWS_fasttext_low_dim_logi', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style>\n",
       "    .dataframe thead tr:only-child th {\n",
       "        text-align: right;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: left;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>EAP_fasttext_logi</th>\n",
       "      <th>HPL_fasttext_logi</th>\n",
       "      <th>MWS_fasttext_logi</th>\n",
       "      <th>EAP_char_tfidf_NB</th>\n",
       "      <th>HPL_char_tfidf_NB</th>\n",
       "      <th>MWS_char_tfidf_NB</th>\n",
       "      <th>EAP_word_tfidf_NB</th>\n",
       "      <th>HPL_word_tfidf_NB</th>\n",
       "      <th>MWS_word_tfidf_NB</th>\n",
       "      <th>EAP_word_count_NB</th>\n",
       "      <th>HPL_word_count_NB</th>\n",
       "      <th>MWS_word_count_NB</th>\n",
       "      <th>EAP_char_count_NB</th>\n",
       "      <th>HPL_char_count_NB</th>\n",
       "      <th>MWS_char_count_NB</th>\n",
       "      <th>num_chars</th>\n",
       "      <th>num_words</th>\n",
       "      <th>average_num_chars</th>\n",
       "      <th>num_,</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>19574</th>\n",
       "      <td>0.502453</td>\n",
       "      <td>0.407642</td>\n",
       "      <td>0.089905</td>\n",
       "      <td>0.646308</td>\n",
       "      <td>0.178946</td>\n",
       "      <td>0.174746</td>\n",
       "      <td>0.545433</td>\n",
       "      <td>0.271914</td>\n",
       "      <td>0.182653</td>\n",
       "      <td>0.821004</td>\n",
       "      <td>1.788949e-01</td>\n",
       "      <td>1.014361e-04</td>\n",
       "      <td>4.385717e-04</td>\n",
       "      <td>1.418646e-01</td>\n",
       "      <td>8.576969e-01</td>\n",
       "      <td>108</td>\n",
       "      <td>20</td>\n",
       "      <td>4.450000</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19575</th>\n",
       "      <td>0.648131</td>\n",
       "      <td>0.008790</td>\n",
       "      <td>0.343079</td>\n",
       "      <td>0.668146</td>\n",
       "      <td>0.075501</td>\n",
       "      <td>0.256353</td>\n",
       "      <td>0.530387</td>\n",
       "      <td>0.170033</td>\n",
       "      <td>0.299580</td>\n",
       "      <td>0.960863</td>\n",
       "      <td>3.663572e-04</td>\n",
       "      <td>3.877103e-02</td>\n",
       "      <td>3.393561e-01</td>\n",
       "      <td>2.354951e-04</td>\n",
       "      <td>6.604084e-01</td>\n",
       "      <td>55</td>\n",
       "      <td>10</td>\n",
       "      <td>4.600000</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19576</th>\n",
       "      <td>0.974873</td>\n",
       "      <td>0.021578</td>\n",
       "      <td>0.003548</td>\n",
       "      <td>0.871611</td>\n",
       "      <td>0.061914</td>\n",
       "      <td>0.066475</td>\n",
       "      <td>0.854690</td>\n",
       "      <td>0.064722</td>\n",
       "      <td>0.080588</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>9.020228e-12</td>\n",
       "      <td>1.238931e-09</td>\n",
       "      <td>1.000000e+00</td>\n",
       "      <td>5.040300e-13</td>\n",
       "      <td>5.375959e-13</td>\n",
       "      <td>68</td>\n",
       "      <td>13</td>\n",
       "      <td>4.307692</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19577</th>\n",
       "      <td>0.693803</td>\n",
       "      <td>0.113793</td>\n",
       "      <td>0.192404</td>\n",
       "      <td>0.746419</td>\n",
       "      <td>0.167788</td>\n",
       "      <td>0.085793</td>\n",
       "      <td>0.568534</td>\n",
       "      <td>0.252775</td>\n",
       "      <td>0.178690</td>\n",
       "      <td>0.957735</td>\n",
       "      <td>4.218386e-02</td>\n",
       "      <td>8.136838e-05</td>\n",
       "      <td>9.999985e-01</td>\n",
       "      <td>1.540827e-06</td>\n",
       "      <td>8.148653e-11</td>\n",
       "      <td>74</td>\n",
       "      <td>15</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19578</th>\n",
       "      <td>0.205407</td>\n",
       "      <td>0.672515</td>\n",
       "      <td>0.122078</td>\n",
       "      <td>0.616580</td>\n",
       "      <td>0.212289</td>\n",
       "      <td>0.171131</td>\n",
       "      <td>0.576162</td>\n",
       "      <td>0.244461</td>\n",
       "      <td>0.179377</td>\n",
       "      <td>0.959739</td>\n",
       "      <td>4.016009e-02</td>\n",
       "      <td>1.007295e-04</td>\n",
       "      <td>1.042207e-08</td>\n",
       "      <td>9.999672e-01</td>\n",
       "      <td>3.275839e-05</td>\n",
       "      <td>109</td>\n",
       "      <td>22</td>\n",
       "      <td>4.000000</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       EAP_fasttext_logi  HPL_fasttext_logi  MWS_fasttext_logi  \\\n",
       "19574           0.502453           0.407642           0.089905   \n",
       "19575           0.648131           0.008790           0.343079   \n",
       "19576           0.974873           0.021578           0.003548   \n",
       "19577           0.693803           0.113793           0.192404   \n",
       "19578           0.205407           0.672515           0.122078   \n",
       "\n",
       "       EAP_char_tfidf_NB  HPL_char_tfidf_NB  MWS_char_tfidf_NB  \\\n",
       "19574           0.646308           0.178946           0.174746   \n",
       "19575           0.668146           0.075501           0.256353   \n",
       "19576           0.871611           0.061914           0.066475   \n",
       "19577           0.746419           0.167788           0.085793   \n",
       "19578           0.616580           0.212289           0.171131   \n",
       "\n",
       "       EAP_word_tfidf_NB  HPL_word_tfidf_NB  MWS_word_tfidf_NB  \\\n",
       "19574           0.545433           0.271914           0.182653   \n",
       "19575           0.530387           0.170033           0.299580   \n",
       "19576           0.854690           0.064722           0.080588   \n",
       "19577           0.568534           0.252775           0.178690   \n",
       "19578           0.576162           0.244461           0.179377   \n",
       "\n",
       "       EAP_word_count_NB  HPL_word_count_NB  MWS_word_count_NB  \\\n",
       "19574           0.821004       1.788949e-01       1.014361e-04   \n",
       "19575           0.960863       3.663572e-04       3.877103e-02   \n",
       "19576           1.000000       9.020228e-12       1.238931e-09   \n",
       "19577           0.957735       4.218386e-02       8.136838e-05   \n",
       "19578           0.959739       4.016009e-02       1.007295e-04   \n",
       "\n",
       "       EAP_char_count_NB  HPL_char_count_NB  MWS_char_count_NB  num_chars  \\\n",
       "19574       4.385717e-04       1.418646e-01       8.576969e-01        108   \n",
       "19575       3.393561e-01       2.354951e-04       6.604084e-01         55   \n",
       "19576       1.000000e+00       5.040300e-13       5.375959e-13         68   \n",
       "19577       9.999985e-01       1.540827e-06       8.148653e-11         74   \n",
       "19578       1.042207e-08       9.999672e-01       3.275839e-05        109   \n",
       "\n",
       "       num_words  average_num_chars  num_,  \n",
       "19574         20           4.450000      2  \n",
       "19575         10           4.600000      0  \n",
       "19576         13           4.307692      1  \n",
       "19577         15           4.000000      1  \n",
       "19578         22           4.000000      1  "
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_test.drop('id', axis=1, inplace=True)\n",
    "df_test.drop('text', axis=1, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "scale_targets = ['num_words', 'num_chars', 'average_num_chars'] + ['num_{}'.format(c) for c in special]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['num_words', 'num_chars', 'average_num_chars', 'num_,']"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "scale_targets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663 3916\n",
      "[0]\ttrain-mlogloss:1.01131\ttest-mlogloss:1.01085\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 170 rounds.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/nzw/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/sklearn/utils/validation.py:444: DataConversionWarning: Data with input dtype int64 was converted to float64 by MinMaxScaler.\n",
      "  warnings.warn(msg, DataConversionWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[50]\ttrain-mlogloss:0.35361\ttest-mlogloss:0.361164\n",
      "[100]\ttrain-mlogloss:0.325971\ttest-mlogloss:0.347428\n",
      "[150]\ttrain-mlogloss:0.308152\ttest-mlogloss:0.34272\n",
      "[200]\ttrain-mlogloss:0.294754\ttest-mlogloss:0.340339\n",
      "[250]\ttrain-mlogloss:0.282812\ttest-mlogloss:0.339512\n",
      "[300]\ttrain-mlogloss:0.272016\ttest-mlogloss:0.338953\n",
      "[350]\ttrain-mlogloss:0.26156\ttest-mlogloss:0.339619\n",
      "[400]\ttrain-mlogloss:0.25216\ttest-mlogloss:0.339701\n",
      "[450]\ttrain-mlogloss:0.243208\ttest-mlogloss:0.339946\n",
      "Stopping. Best iteration:\n",
      "[309]\ttrain-mlogloss:0.270044\ttest-mlogloss:0.338756\n",
      "\n",
      "15663 3916\n",
      "[0]\ttrain-mlogloss:1.01115\ttest-mlogloss:1.01171\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 170 rounds.\n",
      "[50]\ttrain-mlogloss:0.351386\ttest-mlogloss:0.369919\n",
      "[100]\ttrain-mlogloss:0.323953\ttest-mlogloss:0.355402\n",
      "[150]\ttrain-mlogloss:0.306556\ttest-mlogloss:0.350841\n",
      "[200]\ttrain-mlogloss:0.292636\ttest-mlogloss:0.348247\n",
      "[250]\ttrain-mlogloss:0.279991\ttest-mlogloss:0.346911\n",
      "[300]\ttrain-mlogloss:0.26946\ttest-mlogloss:0.34636\n",
      "[350]\ttrain-mlogloss:0.26004\ttest-mlogloss:0.346765\n",
      "[400]\ttrain-mlogloss:0.250651\ttest-mlogloss:0.347123\n",
      "[450]\ttrain-mlogloss:0.241193\ttest-mlogloss:0.347832\n",
      "Stopping. Best iteration:\n",
      "[308]\ttrain-mlogloss:0.267919\ttest-mlogloss:0.346294\n",
      "\n",
      "15663 3916\n",
      "[0]\ttrain-mlogloss:1.01051\ttest-mlogloss:1.01244\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 170 rounds.\n",
      "[50]\ttrain-mlogloss:0.350595\ttest-mlogloss:0.373916\n",
      "[100]\ttrain-mlogloss:0.321878\ttest-mlogloss:0.361389\n",
      "[150]\ttrain-mlogloss:0.304188\ttest-mlogloss:0.356942\n",
      "[200]\ttrain-mlogloss:0.290546\ttest-mlogloss:0.355659\n",
      "[250]\ttrain-mlogloss:0.278956\ttest-mlogloss:0.355354\n",
      "[300]\ttrain-mlogloss:0.268318\ttest-mlogloss:0.354998\n",
      "[350]\ttrain-mlogloss:0.258244\ttest-mlogloss:0.355402\n",
      "[400]\ttrain-mlogloss:0.248725\ttest-mlogloss:0.356561\n",
      "[450]\ttrain-mlogloss:0.239936\ttest-mlogloss:0.35725\n",
      "Stopping. Best iteration:\n",
      "[290]\ttrain-mlogloss:0.27051\ttest-mlogloss:0.354626\n",
      "\n",
      "15663 3916\n",
      "[0]\ttrain-mlogloss:1.01128\ttest-mlogloss:1.01079\n",
      "Multiple eval metrics have been passed: 'test-mlogloss' will be used for early stopping.\n",
      "\n",
      "Will train until test-mlogloss hasn't improved in 170 rounds.\n",
      "[50]\ttrain-mlogloss:0.353284\ttest-mlogloss:0.365832\n",
      "[100]\ttrain-mlogloss:0.326389\ttest-mlogloss:0.348426\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m----------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0mTraceback (most recent call last)",
      "\u001b[0;32m<ipython-input-89-3de6aff5d160>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     33\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[0mwatchlist\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxgtrain\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m'train'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mxgtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'test'\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 35\u001b[0;31m     \u001b[0mmodel\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mxgb\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtrain\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mplst\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mxgtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnum_rounds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mwatchlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mearly_stopping_rounds\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m170\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose_eval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m50\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     36\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     37\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/xgboost-0.6-py3.6.egg/xgboost/training.py\u001b[0m in \u001b[0;36mtrain\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, maximize, early_stopping_rounds, evals_result, verbose_eval, xgb_model, callbacks, learning_rates)\u001b[0m\n\u001b[1;32m    202\u001b[0m                            \u001b[0mevals\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mevals\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    203\u001b[0m                            \u001b[0mobj\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeval\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mfeval\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 204\u001b[0;31m                            xgb_model=xgb_model, callbacks=callbacks)\n\u001b[0m\u001b[1;32m    205\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    206\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/xgboost-0.6-py3.6.egg/xgboost/training.py\u001b[0m in \u001b[0;36m_train_internal\u001b[0;34m(params, dtrain, num_boost_round, evals, obj, feval, xgb_model, callbacks)\u001b[0m\n\u001b[1;32m     72\u001b[0m         \u001b[0;31m# Skip the first update if it is a recovery step.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     73\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mversion\u001b[0m \u001b[0;34m%\u001b[0m \u001b[0;36m2\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 74\u001b[0;31m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     75\u001b[0m             \u001b[0mbst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msave_rabit_checkpoint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0mversion\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/.pyenv/versions/miniconda3-latest/lib/python3.6/site-packages/xgboost-0.6-py3.6.egg/xgboost/core.py\u001b[0m in \u001b[0;36mupdate\u001b[0;34m(self, dtrain, iteration, fobj)\u001b[0m\n\u001b[1;32m    894\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mfobj\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    895\u001b[0m             _check_call(_LIB.XGBoosterUpdateOneIter(self.handle, ctypes.c_int(iteration),\n\u001b[0;32m--> 896\u001b[0;31m                                                     dtrain.handle))\n\u001b[0m\u001b[1;32m    897\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    898\u001b[0m             \u001b[0mpred\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdtrain\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "param = {}\n",
    "param['objective'] = 'multi:softprob'\n",
    "param['eta'] = 0.1 #01\n",
    "param['max_depth'] = 3\n",
    "param['silent'] = 1\n",
    "param['num_class'] = 3\n",
    "param['eval_metric'] = \"mlogloss\"\n",
    "param['min_child_weight'] = 1\n",
    "param['subsample'] = 0.8\n",
    "param['colsample_bytree'] = 0.3\n",
    "param['seed'] = 7\n",
    "num_rounds = 6000\n",
    "\n",
    "cv_scores = []\n",
    "kf = KFold(n_splits=5, shuffle=True, random_state=2017)\n",
    "results = np.zeros((len(df_test), 3))\n",
    "for train_ids, val_ids in kf.split(df):\n",
    "    x_train, x_val = df.loc[train_ids], df.loc[val_ids]\n",
    "    y_train, y_val = y[train_ids], y[val_ids]\n",
    "    print(len(x_train), len(x_val))\n",
    "    \n",
    "\n",
    "    for k in scale_targets:\n",
    "        scaler = MinMaxScaler()\n",
    "        x_train[k] = scaler.fit_transform(x_train[k].values.reshape(len(x_train[k]), 1)).reshape(-1)\n",
    "        x_val[k] = scaler.transform(x_val[k].values.reshape(len(x_val[k]), -1)).reshape(-1)\n",
    "        df_test[k] = scaler.transform(df_test[k].values.reshape(len(df_test[k]), -1)).reshape(-1)\n",
    "\n",
    "    \n",
    "    plst = list(param.items())\n",
    "    xgtrain = xgb.DMatrix(x_train, label=y_train)\n",
    "    xgtest = xgb.DMatrix(x_val, label=y_val)\n",
    "    \n",
    "    watchlist = [ (xgtrain,'train'), (xgtest, 'test') ]\n",
    "    model = xgb.train(plst, xgtrain, num_rounds, watchlist, early_stopping_rounds=170, verbose_eval=50)\n",
    "    \n",
    "\n",
    "    xgtest2 = xgb.DMatrix(df_test)\n",
    "    results += model.predict(xgtest2, ntree_limit=model.best_ntree_limit)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "xgb.plot_importance(model, max_num_features=50, height=0.8, ax=ax)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission_df = pd.read_csv('./data/sample_submission.csv')\n",
    "for a, i in author2class.items():\n",
    "    submission_df[a] = results[:, i]/5\n",
    "\n",
    "with open('./results/first_xgboost_cbow.csv', 'w') as f:\n",
    "    f.write('\"id\",\"EAP\",\"HPL\",\"MWS\"\\n')\n",
    "    for (index, row) in submission_df.iterrows():\n",
    "        f.write(','.join(['\\\"' + row['id'] + '\\\"'] +  list(map(str, [row['EAP'], row['HPL'], row['MWS']]))) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
