# 1. Create text data from train data to use fastText embeddings

1. clone fastText from github
2. run `sh create_unsupervised_fastText.sh`

# 2. Run `feature_create.ipynb`

Some naive bayes feature and logstic regression based on unsupervised fasttext embeddings.

# 3. Download glove and word2vec pre-traiend model.

1. `preprocessing-pre-trained-word-embeddings`
1. `pretrained-embeddings-logistic.ipynb`

# 4. Trainined supervised fastText (two models!)

# 5. Train LSTM

# 6. xgboost

run `xgboost.ipynb`
