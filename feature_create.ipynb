{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import pandas as pd\n",
    "from utils import preprocess\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding, Lambda\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from utils import preprocess\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_split = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train.csv')\n",
    "df_test = pd.read_csv('./data/test.csv')\n",
    "text = df.text.values\n",
    "text_test = df_test.text.values\n",
    "\n",
    "author2class = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "class2author = ['EAP', 'HPL', 'MWS']\n",
    "y = np.array([author2class[a] for a in df.author])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vector(vec):\n",
    "    n = vec.vector_size\n",
    "    x = np.zeros((len(df), n))\n",
    "    for i, doc in enumerate(text):\n",
    "        doc_vec = np.zeros(n)\n",
    "        words = preprocess(doc).lower().split()\n",
    "        num_words = 0\n",
    "        for w in words:\n",
    "            if w in vec.vocab:\n",
    "                doc_vec += vec[w]\n",
    "                num_words += 1\n",
    "        doc_vec /= num_words\n",
    "        x[i] = doc_vec\n",
    "\n",
    "    x_test = np.zeros((len(df_test), n))\n",
    "    for i, doc in enumerate(text_test):\n",
    "        doc_vec = np.zeros(n)\n",
    "        words = preprocess(doc).lower().split()\n",
    "        num_words = 0\n",
    "        for w in words:\n",
    "            if w in vec.vocab:\n",
    "                doc_vec += vec[w]\n",
    "                num_words += 1\n",
    "        doc_vec /= num_words\n",
    "        x_test[i] = doc_vec\n",
    "    return x, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic(x, x_test, seed=7):\n",
    "    num_split = 5\n",
    "    kf = KFold(n_splits=num_split, random_state=seed, shuffle=True)\n",
    "    loss = 0.\n",
    "\n",
    "    predict_prob_features = np.zeros((len(df), 3))\n",
    "    predict_prob_features_test = np.zeros((len(df_test), 3))\n",
    "\n",
    "    for train_index, val_index in kf.split(x):\n",
    "        x_train, x_val = x[train_index], x[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        model = LogisticRegression()\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict_proba(x_val)\n",
    "        predict_prob_features_test += model.predict_proba(x_test)\n",
    "        predict_prob_features[val_index] = y_pred\n",
    "        loss += log_loss(y_pred=y_pred, y_true=y_val)\n",
    "\n",
    "    print(loss/5)\n",
    "    return predict_prob_features, predict_prob_features_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.521292042282\n"
     ]
    }
   ],
   "source": [
    "#  ./fasttext skipgram -input ../data/fasttext-inputs.txt -output model -minCount 1  -neg 15 -ws 10 -epoch 7\n",
    "vec = word2vec.KeyedVectors.load_word2vec_format('./fastText/model.vec')\n",
    "\n",
    "x, x_test = create_vector(vec)\n",
    "\n",
    "predict_prob_features, predict_prob_features_test = logistic(x, x_test)\n",
    "\n",
    "for a, c in author2class.items():\n",
    "    df['{}_fasttext_logi'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_fasttext_logi'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.527408070746\n"
     ]
    }
   ],
   "source": [
    "vec = word2vec.KeyedVectors.load_word2vec_format('./fastText/skip20_min2_neg15_epoch_7_ws_20.vec') # 0.527408070746\n",
    "\n",
    "x, x_test = create_vector(vec)\n",
    "\n",
    "predict_prob_features, predict_prob_features_test = logistic(x, x_test, 7)\n",
    "\n",
    "for a, c in author2class.items():\n",
    "    df['{}_fasttext_low_dim_logi'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_fasttext_low_dim_logi'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.802494276952\n"
     ]
    }
   ],
   "source": [
    "vec = word2vec.KeyedVectors.load_word2vec_format('./fastText/cbow100_min1_neg15_epoch_7_ws_10.vec')\n",
    "\n",
    "x, x_test = create_vector(vec)\n",
    "\n",
    "predict_prob_features, predict_prob_features_test = logistic(x, x_test, 9)\n",
    "\n",
    "for a, c in author2class.items():\n",
    "    df['{}_fasttext_cbow_logi'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_fasttext_cbow_logi'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizer_feature(vectorizer, seed=8, num_split=5, alphas=[1.]):\n",
    "    param_grid = dict(alpha=alphas)\n",
    "    print(param_grid, vectorizer)\n",
    "    \n",
    "    kf = KFold(n_splits=num_split, random_state=seed, shuffle=True)\n",
    "    sum_loss = 0.\n",
    "\n",
    "    predict_prob_features = np.zeros((len(df), 3))\n",
    "    predict_prob_features_test = np.zeros((len(df_test), 3))\n",
    "    ite = 0\n",
    "    for train_index, val_index in kf.split(text):\n",
    "        ite += 1\n",
    "        print('{}/{}: #Trains: {}, #Val: {}'.format(ite, num_split, len(train_index), len(val_index)), end=' ')\n",
    "        text_train, text_val = text[train_index], text[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        x_train = vectorizer.fit_transform(text_train)\n",
    "        x_val = vectorizer.transform(text_val)\n",
    "        \n",
    "        \n",
    "        if len(alphas) > 1:\n",
    "            model = MultinomialNB()\n",
    "            clf = GridSearchCV(estimator=model, param_grid=param_grid, cv=10, scoring='neg_log_loss', n_jobs=-1)\n",
    "            clf.fit(x_train, y_train)\n",
    "            model = clf.best_estimator_            \n",
    "        else:\n",
    "            alpha = 1.\n",
    "            if len(alphas) == 1:\n",
    "                alpha = alphas[0]\n",
    "            model = MultinomialNB(alpha)\n",
    "            model.fit(x_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict_proba(x_val)\n",
    "\n",
    "        # save features\n",
    "        predict_prob_features[val_index] = y_pred\n",
    "        predict_prob_features_test += model.predict_proba(vectorizer.transform(text_test))\n",
    "        \n",
    "        best_param = model.alpha\n",
    "\n",
    "        loss = log_loss(y_pred=y_pred, y_true=y_val)\n",
    "        sum_loss += loss\n",
    "\n",
    "        \n",
    "        print('valLoss: {}, best_param α= {}'.format(loss, best_param))\n",
    "        \n",
    "    print(sum_loss/num_split)\n",
    "    return predict_prob_features, predict_prob_features_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': [0.007]} TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "1/5: #Trains: 15663, #Val: 3916 valLoss: 0.4191531979947033, best_param α= 0.007\n",
      "2/5: #Trains: 15663, #Val: 3916 valLoss: 0.3979100889218946, best_param α= 0.007\n",
      "3/5: #Trains: 15663, #Val: 3916 valLoss: 0.403906857121704, best_param α= 0.007\n",
      "4/5: #Trains: 15663, #Val: 3916 valLoss: 0.4146626933703398, best_param α= 0.007\n",
      "5/5: #Trains: 15664, #Val: 3915 valLoss: 0.4054966330640256, best_param α= 0.007\n",
      "0.408225894095\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='word')\n",
    "predict_prob_features, predict_prob_features_test = vectorizer_feature(vectorizer, 7, alphas=[0.007]) # from [0.005,0.007, 0.01]\n",
    "for a, c in author2class.items():\n",
    "    df['{}_word_tfidf_NB'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_word_tfidf_NB'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': [0.013]} TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 5), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "1/5: #Trains: 15663, #Val: 3916 valLoss: 0.3951854930297089, best_param α= 0.013\n",
      "2/5: #Trains: 15663, #Val: 3916 valLoss: 0.37123938784736266, best_param α= 0.013\n",
      "3/5: #Trains: 15663, #Val: 3916 valLoss: 0.4002779893173318, best_param α= 0.013\n",
      "4/5: #Trains: 15663, #Val: 3916 valLoss: 0.3916746370981128, best_param α= 0.013\n",
      "5/5: #Trains: 15664, #Val: 3915 valLoss: 0.3654883416610354, best_param α= 0.013\n",
      "0.384773169791\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 5), analyzer='char')\n",
    "predict_prob_features, predict_prob_features_test = vectorizer_feature(vectorizer, 8, alphas=[0.013]) # 0.012, 0.013, 0.014\n",
    "for a, c in author2class.items():\n",
    "    df['{}_char_tfidf_NB'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_char_tfidf_NB'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': [1.2]} CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "1/5: #Trains: 15663, #Val: 3916 valLoss: 0.7884866155256142, best_param α= 1.2\n",
      "2/5: #Trains: 15663, #Val: 3916 valLoss: 0.8623959123023731, best_param α= 1.2\n",
      "3/5: #Trains: 15663, #Val: 3916 valLoss: 0.8252652372000494, best_param α= 1.2\n",
      "4/5: #Trains: 15663, #Val: 3916 valLoss: 0.776055308824219, best_param α= 1.2\n",
      "5/5: #Trains: 15664, #Val: 3915 valLoss: 0.8172614666911712, best_param α= 1.2\n",
      "0.813892908109\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 3), analyzer='word')\n",
    "predict_prob_features, predict_prob_features_test = vectorizer_feature(vectorizer, 9, alphas=[1.2])\n",
    "for a, c in author2class.items():\n",
    "    df['{}_word_count_NB'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_word_count_NB'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': [0.15, 0.2, 0.3, 0.4, 0.5]} CountVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 4), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "1/5: #Trains: 15663, #Val: 3916 valLoss: 2.5132982409813023, best_param α= 0.2\n",
      "2/5: #Trains: 15663, #Val: 3916 valLoss: 2.4407679975214327, best_param α= 0.4\n",
      "3/5: #Trains: 15663, #Val: 3916 valLoss: 2.504654489912232, best_param α= 0.3\n",
      "4/5: #Trains: 15663, #Val: 3916 valLoss: 2.837462002734221, best_param α= 0.2\n",
      "5/5: #Trains: 15664, #Val: 3915 valLoss: 2.561142701900116, best_param α= 0.2\n",
      "2.57146508661\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 4), analyzer='char')\n",
    "predict_prob_features, predict_prob_features_test = vectorizer_feature(vectorizer, seed=7, alphas=[0.15, 0.2, 0.3, 0.4, 0.5])\n",
    "for a, c in author2class.items():\n",
    "    df['{}_char_count_NB'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_char_count_NB'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': [1.5, 2.0, 2.5]} CountVectorizer(analyzer='char_wb', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 5), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "1/5: #Trains: 15663, #Val: 3916 valLoss: 2.911286873284481, best_param α= 2.5\n",
      "2/5: #Trains: 15663, #Val: 3916 valLoss: 2.693418535058206, best_param α= 1.5\n",
      "3/5: #Trains: 15663, #Val: 3916 valLoss: 2.600940347993735, best_param α= 2.0\n",
      "4/5: #Trains: 15663, #Val: 3916 valLoss: 3.018726215529485, best_param α= 2.0\n",
      "5/5: #Trains: 15664, #Val: 3915 valLoss: 2.699994701500262, best_param α= 2.0\n",
      "2.78487333467\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 5), analyzer='char_wb')\n",
    "predict_prob_features, predict_prob_features_test = vectorizer_feature(vectorizer, seed=10, alphas=[1.5, 2., 2.5]) # 0.5, 0.1, 1., 1.5, 2., 2.5, 3.\n",
    "for a, c in author2class.items():\n",
    "    df['{}_char_wb_count_NB'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_char_wb_count_NB'.format(a)] = predict_prob_features_test[:, c]/num_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MetaFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normal_latters = set(string.ascii_uppercase) | set(string.ascii_lowercase) | set(',.:;\"\\'?! ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['num_words']      = np.array([len(t.split()) for t in df.text])\n",
    "df_test['num_words'] = np.array([len(t.split()) for t in df_test.text])\n",
    "\n",
    "df['num_chars']      = np.array([len(t) for t in df.text])\n",
    "df_test['num_chars'] = np.array([len(t) for t in df_test.text])\n",
    "\n",
    "df['average_num_chars']      = np.array([np.mean([len(word) for word in t.split()]) for t in df.text])\n",
    "df_test['average_num_chars'] = np.array([np.mean([len(word) for word in t.split()]) for t in df_test.text])\n",
    "\n",
    "df['num_uniq_words']      = np.array([len(set(t.split())) for t in df.text])\n",
    "df_test['num_uniq_words'] = np.array([len(set(t.split())) for t in df_test.text])\n",
    "\n",
    "df['num_uniq_chars']      = np.array([len(set(t)) for t in df.text])\n",
    "df_test['num_uniq_chars'] = np.array([len(set(t)) for t in df_test.text])\n",
    "\n",
    "df['rate_uniq_words']      = np.array([len(set(t.split()))/len(t.split()) for t in df.text])\n",
    "df_test['rate_uniq_words'] = np.array([len(set(t.split()))/len(t.split()) for t in df_test.text])\n",
    "\n",
    "df['rate_uniq_chars']       = np.array([len(set(t))/len(t) for t in df.text])\n",
    "df_test['rate_uniq_chars'] = np.array([len(set(t))/len(t) for t in df_test.text])\n",
    "\n",
    "\n",
    "special = ',!' # ',.:;\"\\!'?!'\n",
    "for c in special:\n",
    "    df['num_'+c] = np.array([t.count(c) for t in df.text])\n",
    "    df_test['num_'+c] = np.array([t.count(c) for t in df_test.text])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# FastText!!!!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "y = np.array([author2class[a] for a in df.author])\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embedding_dims = 20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_docs(df, n_gram_max=1):\n",
    "    docs = []\n",
    "    special_latters = set('à ñ Ν ê Υ Å Æ ö δ è Π α é ä ë æ ô ç ü ἶ Ο â î ï Σ'.split()) - set(string.ascii_uppercase) - set(string.ascii_lowercase) - set(',.:;\"\\'?')\n",
    "    for i, text in enumerate(df.text):    \n",
    "        def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "\n",
    "        doc = preprocess(text).split()\n",
    "        \n",
    "        prod = special_latters & set(text)\n",
    "        special_chars = ''\n",
    "        if prod:\n",
    "            for c in prod:\n",
    "                freq = text.count(c)\n",
    "                special_chars += (' {} '.format(c) * freq)            \n",
    "                \n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)) + special_chars)\n",
    "        \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(input_dim, embeddings_dims=20):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5: #Trains: 15663, #Val: 3916 #vocab: 28285  (15663, 256) (3916, 256) (8392, 256)\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/45\n",
      "15663/15663 [==============================] - 14s - loss: 1.0779 - acc: 0.4031 - val_loss: 1.0579 - val_acc: 0.4058\n",
      "Epoch 2/45\n",
      "15663/15663 [==============================] - 14s - loss: 1.0063 - acc: 0.4878 - val_loss: 0.9543 - val_acc: 0.5848\n",
      "Epoch 3/45\n",
      "15663/15663 [==============================] - 16s - loss: 0.8724 - acc: 0.6786 - val_loss: 0.8286 - val_acc: 0.7071\n",
      "Epoch 4/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.7439 - acc: 0.7523 - val_loss: 0.7323 - val_acc: 0.7288\n",
      "Epoch 5/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.6455 - acc: 0.7896 - val_loss: 0.6604 - val_acc: 0.7536\n",
      "Epoch 6/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.5691 - acc: 0.8126 - val_loss: 0.6082 - val_acc: 0.7643\n",
      "Epoch 7/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.5079 - acc: 0.8334 - val_loss: 0.5660 - val_acc: 0.7822\n",
      "Epoch 8/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.4555 - acc: 0.8532 - val_loss: 0.5361 - val_acc: 0.7949\n",
      "Epoch 9/45\n",
      "15663/15663 [==============================] - 14s - loss: 0.4120 - acc: 0.8661 - val_loss: 0.5054 - val_acc: 0.7993\n",
      "Epoch 10/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.3738 - acc: 0.8811 - val_loss: 0.4927 - val_acc: 0.8087\n",
      "Epoch 11/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.3400 - acc: 0.8944 - val_loss: 0.4627 - val_acc: 0.8169\n",
      "Epoch 12/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.3099 - acc: 0.9045 - val_loss: 0.4451 - val_acc: 0.8258\n",
      "Epoch 13/45\n",
      "15663/15663 [==============================] - 15s - loss: 0.2838 - acc: 0.9134 - val_loss: 0.4319 - val_acc: 0.8281\n",
      "Epoch 14/45\n",
      "15663/15663 [==============================] - 17s - loss: 0.2610 - acc: 0.9197 - val_loss: 0.4214 - val_acc: 0.8358\n",
      "Epoch 15/45\n",
      "15663/15663 [==============================] - 15s - loss: 0.2403 - acc: 0.9280 - val_loss: 0.4173 - val_acc: 0.8348\n",
      "Epoch 16/45\n",
      "15663/15663 [==============================] - 16s - loss: 0.2215 - acc: 0.9366 - val_loss: 0.4029 - val_acc: 0.8391\n",
      "Epoch 17/45\n",
      "15663/15663 [==============================] - 14s - loss: 0.2053 - acc: 0.9395 - val_loss: 0.3959 - val_acc: 0.8384\n",
      "Epoch 18/45\n",
      "15663/15663 [==============================] - 14s - loss: 0.1896 - acc: 0.9450 - val_loss: 0.3901 - val_acc: 0.8450\n",
      "Epoch 19/45\n",
      "15663/15663 [==============================] - 14s - loss: 0.1766 - acc: 0.9492 - val_loss: 0.3909 - val_acc: 0.8407\n",
      "Epoch 20/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.1637 - acc: 0.9521 - val_loss: 0.3845 - val_acc: 0.8447\n",
      "Epoch 21/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.1525 - acc: 0.9569 - val_loss: 0.3813 - val_acc: 0.8458\n",
      "Epoch 22/45\n",
      "15663/15663 [==============================] - 14s - loss: 0.1413 - acc: 0.9604 - val_loss: 0.3801 - val_acc: 0.8447\n",
      "Epoch 23/45\n",
      "15663/15663 [==============================] - 14s - loss: 0.1322 - acc: 0.9625 - val_loss: 0.3816 - val_acc: 0.8455\n",
      "Epoch 24/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.1236 - acc: 0.9669 - val_loss: 0.3809 - val_acc: 0.8463\n",
      "Epoch 25/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.1152 - acc: 0.9699 - val_loss: 0.3806 - val_acc: 0.8486\n",
      "Epoch 26/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.1079 - acc: 0.9712 - val_loss: 0.3869 - val_acc: 0.8432\n",
      "Epoch 27/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.1012 - acc: 0.9720 - val_loss: 0.4115 - val_acc: 0.8368\n",
      "7936/8392 [===========================>..] - ETA: 0svalLoss: 0.3801230070124737\n",
      "2/5: #Trains: 15663, #Val: 3916 #vocab: 28285  (15663, 256) (3916, 256) (8392, 256)\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/45\n",
      "15663/15663 [==============================] - 12s - loss: 1.0785 - acc: 0.4039 - val_loss: 1.0575 - val_acc: 0.4132\n",
      "Epoch 2/45\n",
      "15663/15663 [==============================] - 12s - loss: 1.0076 - acc: 0.4879 - val_loss: 0.9474 - val_acc: 0.5544\n",
      "Epoch 3/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.8698 - acc: 0.6706 - val_loss: 0.8180 - val_acc: 0.6765\n",
      "Epoch 4/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.7391 - acc: 0.7569 - val_loss: 0.7180 - val_acc: 0.7403\n",
      "Epoch 5/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.6397 - acc: 0.7924 - val_loss: 0.6443 - val_acc: 0.7689\n",
      "Epoch 6/45\n",
      "15663/15663 [==============================] - 12s - loss: 0.5628 - acc: 0.8179 - val_loss: 0.5904 - val_acc: 0.7863\n",
      "Epoch 7/45\n",
      "15663/15663 [==============================] - 12s - loss: 0.5003 - acc: 0.8387 - val_loss: 0.5510 - val_acc: 0.7939\n",
      "Epoch 8/45\n",
      "15663/15663 [==============================] - 12s - loss: 0.4485 - acc: 0.8544 - val_loss: 0.5245 - val_acc: 0.8018\n",
      "Epoch 9/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.4047 - acc: 0.8714 - val_loss: 0.4882 - val_acc: 0.8210\n",
      "Epoch 10/45\n",
      "15663/15663 [==============================] - 12s - loss: 0.3656 - acc: 0.8869 - val_loss: 0.4707 - val_acc: 0.8092\n",
      "Epoch 11/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.3321 - acc: 0.8980 - val_loss: 0.4469 - val_acc: 0.8274\n",
      "Epoch 12/45\n",
      "15663/15663 [==============================] - 12s - loss: 0.3027 - acc: 0.9077 - val_loss: 0.4308 - val_acc: 0.8366\n",
      "Epoch 13/45\n",
      "15663/15663 [==============================] - 12s - loss: 0.2754 - acc: 0.9166 - val_loss: 0.4174 - val_acc: 0.8409\n",
      "Epoch 14/45\n",
      "15663/15663 [==============================] - 12s - loss: 0.2528 - acc: 0.9245 - val_loss: 0.4055 - val_acc: 0.8353\n",
      "Epoch 15/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.2326 - acc: 0.9307 - val_loss: 0.3935 - val_acc: 0.8440\n",
      "Epoch 16/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.2137 - acc: 0.9363 - val_loss: 0.3878 - val_acc: 0.8460\n",
      "Epoch 17/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.1968 - acc: 0.9425 - val_loss: 0.3797 - val_acc: 0.8465\n",
      "Epoch 18/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.1829 - acc: 0.9473 - val_loss: 0.3773 - val_acc: 0.8460\n",
      "Epoch 19/45\n",
      "15663/15663 [==============================] - 12s - loss: 0.1688 - acc: 0.9510 - val_loss: 0.3805 - val_acc: 0.8447\n",
      "Epoch 20/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.1574 - acc: 0.9540 - val_loss: 0.3813 - val_acc: 0.8445\n",
      "Epoch 21/45\n",
      "15663/15663 [==============================] - 12s - loss: 0.1457 - acc: 0.9575 - val_loss: 0.3685 - val_acc: 0.8527\n",
      "Epoch 22/45\n",
      "15663/15663 [==============================] - 12s - loss: 0.1354 - acc: 0.9621 - val_loss: 0.3661 - val_acc: 0.8578\n",
      "Epoch 23/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.1264 - acc: 0.9637 - val_loss: 0.3680 - val_acc: 0.8547\n",
      "Epoch 24/45\n",
      "15663/15663 [==============================] - 12s - loss: 0.1181 - acc: 0.9665 - val_loss: 0.3716 - val_acc: 0.8509\n",
      "Epoch 25/45\n",
      "15663/15663 [==============================] - 12s - loss: 0.1100 - acc: 0.9695 - val_loss: 0.3750 - val_acc: 0.8547\n",
      "Epoch 26/45\n",
      "15663/15663 [==============================] - 12s - loss: 0.1030 - acc: 0.9710 - val_loss: 0.3693 - val_acc: 0.8542\n",
      "Epoch 27/45\n",
      "15663/15663 [==============================] - 12s - loss: 0.0963 - acc: 0.9744 - val_loss: 0.3714 - val_acc: 0.8560\n",
      "7936/8392 [===========================>..] - ETA: 0svalLoss: 0.37312484408772695\n",
      "3/5: #Trains: 15663, #Val: 3916 #vocab: 28285  (15663, 256) (3916, 256) (8392, 256)\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/45\n",
      "15663/15663 [==============================] - 12s - loss: 1.0760 - acc: 0.4033 - val_loss: 1.0503 - val_acc: 0.4298\n",
      "Epoch 2/45\n",
      "15663/15663 [==============================] - 12s - loss: 0.9952 - acc: 0.5146 - val_loss: 0.9301 - val_acc: 0.6044\n",
      "Epoch 3/45\n",
      "15663/15663 [==============================] - 12s - loss: 0.8502 - acc: 0.6878 - val_loss: 0.8012 - val_acc: 0.6719\n",
      "Epoch 4/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.7202 - acc: 0.7619 - val_loss: 0.7064 - val_acc: 0.7280\n",
      "Epoch 5/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.6213 - acc: 0.7984 - val_loss: 0.6438 - val_acc: 0.7750\n",
      "Epoch 6/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 12s - loss: 0.5464 - acc: 0.8234 - val_loss: 0.5892 - val_acc: 0.7748\n",
      "Epoch 7/45\n",
      "15663/15663 [==============================] - 12s - loss: 0.4849 - acc: 0.8433 - val_loss: 0.5515 - val_acc: 0.7850\n",
      "Epoch 8/45\n",
      "15663/15663 [==============================] - 12s - loss: 0.4342 - acc: 0.8617 - val_loss: 0.5186 - val_acc: 0.8006\n",
      "Epoch 9/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.3900 - acc: 0.8767 - val_loss: 0.4861 - val_acc: 0.8161\n",
      "Epoch 10/45\n",
      "15663/15663 [==============================] - 12s - loss: 0.3515 - acc: 0.8910 - val_loss: 0.4637 - val_acc: 0.8271\n",
      "Epoch 11/45\n",
      "15663/15663 [==============================] - 12s - loss: 0.3189 - acc: 0.9014 - val_loss: 0.4430 - val_acc: 0.8312\n",
      "Epoch 12/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.2899 - acc: 0.9109 - val_loss: 0.4283 - val_acc: 0.8320\n",
      "Epoch 13/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.2643 - acc: 0.9215 - val_loss: 0.4162 - val_acc: 0.8404\n",
      "Epoch 14/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.2412 - acc: 0.9282 - val_loss: 0.4041 - val_acc: 0.8412\n",
      "Epoch 15/45\n",
      "15663/15663 [==============================] - 14s - loss: 0.2210 - acc: 0.9349 - val_loss: 0.4041 - val_acc: 0.8363\n",
      "Epoch 16/45\n",
      "15663/15663 [==============================] - 14s - loss: 0.2021 - acc: 0.9413 - val_loss: 0.3891 - val_acc: 0.8470\n",
      "Epoch 17/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.1856 - acc: 0.9478 - val_loss: 0.3834 - val_acc: 0.8483\n",
      "Epoch 18/45\n",
      "15663/15663 [==============================] - 14s - loss: 0.1721 - acc: 0.9500 - val_loss: 0.3820 - val_acc: 0.8473\n",
      "Epoch 19/45\n",
      "15663/15663 [==============================] - 16s - loss: 0.1588 - acc: 0.9536 - val_loss: 0.3782 - val_acc: 0.8501\n",
      "Epoch 20/45\n",
      "15663/15663 [==============================] - 15s - loss: 0.1473 - acc: 0.9577 - val_loss: 0.3858 - val_acc: 0.8399\n",
      "Epoch 21/45\n",
      "15663/15663 [==============================] - 15s - loss: 0.1355 - acc: 0.9612 - val_loss: 0.3717 - val_acc: 0.8493\n",
      "Epoch 22/45\n",
      "15663/15663 [==============================] - 15s - loss: 0.1255 - acc: 0.9640 - val_loss: 0.3739 - val_acc: 0.8534\n",
      "Epoch 23/45\n",
      "15663/15663 [==============================] - 15s - loss: 0.1175 - acc: 0.9676 - val_loss: 0.3813 - val_acc: 0.8496\n",
      "Epoch 24/45\n",
      "15663/15663 [==============================] - 15s - loss: 0.1089 - acc: 0.9697 - val_loss: 0.3782 - val_acc: 0.8516\n",
      "Epoch 25/45\n",
      "15663/15663 [==============================] - 15s - loss: 0.1004 - acc: 0.9727 - val_loss: 0.3763 - val_acc: 0.8519\n",
      "Epoch 26/45\n",
      "15663/15663 [==============================] - 15s - loss: 0.0941 - acc: 0.9749 - val_loss: 0.3980 - val_acc: 0.8455\n",
      "7744/8392 [==========================>...] - ETA: 0svalLoss: 0.37266479304770606\n",
      "4/5: #Trains: 15663, #Val: 3916 #vocab: 28285  (15663, 256) (3916, 256) (8392, 256)\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/45\n",
      "15663/15663 [==============================] - 14s - loss: 1.0762 - acc: 0.4065 - val_loss: 1.0605 - val_acc: 0.4116\n",
      "Epoch 2/45\n",
      "15663/15663 [==============================] - 15s - loss: 1.0058 - acc: 0.4979 - val_loss: 0.9630 - val_acc: 0.6241\n",
      "Epoch 3/45\n",
      "15663/15663 [==============================] - 15s - loss: 0.8743 - acc: 0.6612 - val_loss: 0.8449 - val_acc: 0.6282\n",
      "Epoch 4/45\n",
      "15663/15663 [==============================] - 14s - loss: 0.7467 - acc: 0.7451 - val_loss: 0.7433 - val_acc: 0.7390\n",
      "Epoch 5/45\n",
      "15663/15663 [==============================] - 15s - loss: 0.6465 - acc: 0.7880 - val_loss: 0.6718 - val_acc: 0.7531\n",
      "Epoch 6/45\n",
      "15663/15663 [==============================] - 16s - loss: 0.5691 - acc: 0.8150 - val_loss: 0.6158 - val_acc: 0.7766\n",
      "Epoch 7/45\n",
      "15663/15663 [==============================] - 16s - loss: 0.5071 - acc: 0.8346 - val_loss: 0.5697 - val_acc: 0.7972\n",
      "Epoch 8/45\n",
      "15663/15663 [==============================] - 15s - loss: 0.4551 - acc: 0.8519 - val_loss: 0.5360 - val_acc: 0.8054\n",
      "Epoch 9/45\n",
      "15663/15663 [==============================] - 12s - loss: 0.4114 - acc: 0.8665 - val_loss: 0.5023 - val_acc: 0.8159\n",
      "Epoch 10/45\n",
      "15663/15663 [==============================] - 13s - loss: 0.3728 - acc: 0.8822 - val_loss: 0.4827 - val_acc: 0.8136\n",
      "Epoch 11/45\n",
      "15663/15663 [==============================] - 12s - loss: 0.3387 - acc: 0.8956 - val_loss: 0.4670 - val_acc: 0.8243\n",
      "Epoch 12/45\n",
      "15663/15663 [==============================] - 12s - loss: 0.3089 - acc: 0.9060 - val_loss: 0.4363 - val_acc: 0.8355\n",
      "Epoch 13/45\n",
      "15663/15663 [==============================] - 15s - loss: 0.2825 - acc: 0.9155 - val_loss: 0.4252 - val_acc: 0.8315\n",
      "Epoch 14/45\n",
      "15663/15663 [==============================] - 15s - loss: 0.2586 - acc: 0.9216 - val_loss: 0.4098 - val_acc: 0.8414\n",
      "Epoch 15/45\n",
      "15663/15663 [==============================] - 15s - loss: 0.2377 - acc: 0.9300 - val_loss: 0.3999 - val_acc: 0.8427\n",
      "Epoch 16/45\n",
      "15663/15663 [==============================] - 14s - loss: 0.2197 - acc: 0.9360 - val_loss: 0.4099 - val_acc: 0.8358\n",
      "Epoch 17/45\n",
      "15663/15663 [==============================] - 15s - loss: 0.2025 - acc: 0.9411 - val_loss: 0.3841 - val_acc: 0.8475\n",
      "Epoch 18/45\n",
      "15663/15663 [==============================] - 14s - loss: 0.1867 - acc: 0.9471 - val_loss: 0.3801 - val_acc: 0.8496\n",
      "Epoch 19/45\n",
      "15663/15663 [==============================] - 15s - loss: 0.1729 - acc: 0.9502 - val_loss: 0.3726 - val_acc: 0.8532\n",
      "Epoch 20/45\n",
      "15663/15663 [==============================] - 17s - loss: 0.1604 - acc: 0.9518 - val_loss: 0.3798 - val_acc: 0.8501\n",
      "Epoch 21/45\n",
      "15663/15663 [==============================] - 16s - loss: 0.1489 - acc: 0.9589 - val_loss: 0.3707 - val_acc: 0.8524\n",
      "Epoch 22/45\n",
      "15663/15663 [==============================] - 16s - loss: 0.1389 - acc: 0.9604 - val_loss: 0.3663 - val_acc: 0.8550\n",
      "Epoch 23/45\n",
      "15663/15663 [==============================] - 16s - loss: 0.1296 - acc: 0.9658 - val_loss: 0.3668 - val_acc: 0.8529\n",
      "Epoch 24/45\n",
      "15663/15663 [==============================] - 16s - loss: 0.1208 - acc: 0.9678 - val_loss: 0.3670 - val_acc: 0.8532\n",
      "Epoch 25/45\n",
      "15663/15663 [==============================] - 16s - loss: 0.1129 - acc: 0.9711 - val_loss: 0.3695 - val_acc: 0.8542\n",
      "Epoch 26/45\n",
      "15663/15663 [==============================] - 16s - loss: 0.1057 - acc: 0.9725 - val_loss: 0.3701 - val_acc: 0.8542\n",
      "Epoch 27/45\n",
      "15663/15663 [==============================] - 16s - loss: 0.0990 - acc: 0.9740 - val_loss: 0.3728 - val_acc: 0.8555\n",
      "7648/8392 [==========================>...] - ETA: 0svalLoss: 0.3710684978740008\n",
      "5/5: #Trains: 15664, #Val: 3915 #vocab: 28285  (15664, 256) (3915, 256) (8392, 256)\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/45\n",
      "15664/15664 [==============================] - 16s - loss: 1.0776 - acc: 0.4058 - val_loss: 1.0589 - val_acc: 0.4197\n",
      "Epoch 2/45\n",
      "15664/15664 [==============================] - 16s - loss: 1.0089 - acc: 0.4920 - val_loss: 0.9673 - val_acc: 0.5946\n",
      "Epoch 3/45\n",
      "15664/15664 [==============================] - 16s - loss: 0.8782 - acc: 0.6563 - val_loss: 0.8340 - val_acc: 0.6496\n",
      "Epoch 4/45\n",
      "15664/15664 [==============================] - 16s - loss: 0.7495 - acc: 0.7467 - val_loss: 0.7317 - val_acc: 0.7448\n",
      "Epoch 5/45\n",
      "15664/15664 [==============================] - 16s - loss: 0.6477 - acc: 0.7869 - val_loss: 0.6554 - val_acc: 0.7806\n",
      "Epoch 6/45\n",
      "15664/15664 [==============================] - 17s - loss: 0.5684 - acc: 0.8158 - val_loss: 0.6005 - val_acc: 0.7921\n",
      "Epoch 7/45\n",
      "15664/15664 [==============================] - 16s - loss: 0.5049 - acc: 0.8354 - val_loss: 0.5559 - val_acc: 0.7969\n",
      "Epoch 8/45\n",
      "15664/15664 [==============================] - 17s - loss: 0.4512 - acc: 0.8569 - val_loss: 0.5225 - val_acc: 0.8089\n",
      "Epoch 9/45\n",
      "15664/15664 [==============================] - 16s - loss: 0.4065 - acc: 0.8707 - val_loss: 0.5032 - val_acc: 0.8036\n",
      "Epoch 10/45\n",
      "15664/15664 [==============================] - 17s - loss: 0.3672 - acc: 0.8837 - val_loss: 0.4710 - val_acc: 0.8258\n",
      "Epoch 11/45\n",
      "15664/15664 [==============================] - 17s - loss: 0.3321 - acc: 0.8964 - val_loss: 0.4551 - val_acc: 0.8255\n",
      "Epoch 12/45\n",
      "15664/15664 [==============================] - 16s - loss: 0.3031 - acc: 0.9084 - val_loss: 0.4397 - val_acc: 0.8340\n",
      "Epoch 13/45\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15664/15664 [==============================] - 16s - loss: 0.2761 - acc: 0.9172 - val_loss: 0.4296 - val_acc: 0.8347\n",
      "Epoch 14/45\n",
      "15664/15664 [==============================] - 16s - loss: 0.2528 - acc: 0.9268 - val_loss: 0.4119 - val_acc: 0.8401\n",
      "Epoch 15/45\n",
      "15664/15664 [==============================] - 17s - loss: 0.2328 - acc: 0.9310 - val_loss: 0.4095 - val_acc: 0.8411\n",
      "Epoch 16/45\n",
      "15664/15664 [==============================] - 16s - loss: 0.2135 - acc: 0.9376 - val_loss: 0.3960 - val_acc: 0.8437\n",
      "Epoch 17/45\n",
      "15664/15664 [==============================] - 17s - loss: 0.1970 - acc: 0.9443 - val_loss: 0.3946 - val_acc: 0.8398\n",
      "Epoch 18/45\n",
      "15664/15664 [==============================] - 17s - loss: 0.1813 - acc: 0.9498 - val_loss: 0.3959 - val_acc: 0.8319\n",
      "Epoch 19/45\n",
      "15664/15664 [==============================] - 16s - loss: 0.1678 - acc: 0.9533 - val_loss: 0.3865 - val_acc: 0.8452\n",
      "Epoch 20/45\n",
      "15664/15664 [==============================] - 16s - loss: 0.1559 - acc: 0.9565 - val_loss: 0.3870 - val_acc: 0.8434\n",
      "Epoch 21/45\n",
      "15664/15664 [==============================] - 16s - loss: 0.1445 - acc: 0.9577 - val_loss: 0.3942 - val_acc: 0.8429\n",
      "Epoch 22/45\n",
      "15664/15664 [==============================] - 17s - loss: 0.1332 - acc: 0.9639 - val_loss: 0.3790 - val_acc: 0.8526\n",
      "Epoch 23/45\n",
      "15664/15664 [==============================] - 16s - loss: 0.1253 - acc: 0.9653 - val_loss: 0.3870 - val_acc: 0.8480\n",
      "Epoch 24/45\n",
      "15664/15664 [==============================] - 15s - loss: 0.1162 - acc: 0.9687 - val_loss: 0.3899 - val_acc: 0.8480\n",
      "Epoch 25/45\n",
      "15664/15664 [==============================] - 16s - loss: 0.1082 - acc: 0.9711 - val_loss: 0.3820 - val_acc: 0.8544\n",
      "Epoch 26/45\n",
      "15664/15664 [==============================] - 16s - loss: 0.1012 - acc: 0.9736 - val_loss: 0.3927 - val_acc: 0.8531\n",
      "Epoch 27/45\n",
      "15664/15664 [==============================] - 16s - loss: 0.0949 - acc: 0.9741 - val_loss: 0.3873 - val_acc: 0.8521\n",
      "8392/8392 [==============================] - 0s     \n",
      "valLoss: 0.3726591018155969\n"
     ]
    }
   ],
   "source": [
    "raw_docs = create_docs(df, n_gram_max=1)\n",
    "raw_docs_test = create_docs(df_test, n_gram_max=1)\n",
    "\n",
    "seed = 7\n",
    "num_split = 5\n",
    "epochs = 45\n",
    "\n",
    "# for next training\n",
    "predict_prob_features = np.zeros((len(df), 3))\n",
    "predict_prob_features_test = np.zeros((len(df_test), 3))\n",
    "\n",
    "ite = 0\n",
    "sum_loss = 0.\n",
    "min_count = 1\n",
    "\n",
    "kf = KFold(n_splits=num_split, random_state=seed, shuffle=True)\n",
    "for train_index, val_index in kf.split(text):\n",
    "    ite += 1\n",
    "    print('{}/{}: #Trains: {}, #Val: {}'.format(ite, num_split, len(train_index), len(val_index)), end=' ')\n",
    "    \n",
    "    docs_train = [raw_docs[i] for i in train_index]\n",
    "    docs_val = [raw_docs[i] for i in val_index]\n",
    "\n",
    "    # get vocab\n",
    "    tokenizer = Tokenizer(filters='', lower=False)\n",
    "    #     tokenizer.fit_on_texts(docs_train)\n",
    "    tokenizer.fit_on_texts(docs_train + docs_val)\n",
    "\n",
    "    num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "\n",
    "    tokenizer = Tokenizer(num_words=num_words, filters='', lower=False)\n",
    "    #     tokenizer.fit_on_texts(docs_train)\n",
    "    tokenizer.fit_on_texts(docs_train + docs_val)    \n",
    "    \n",
    "    docs_train = tokenizer.texts_to_sequences(docs_train)\n",
    "    docs_val = tokenizer.texts_to_sequences(docs_val)\n",
    "    docs_test = tokenizer.texts_to_sequences(raw_docs_test)\n",
    "\n",
    "    maxlen = 256# max([len(i) for i in  docs_train] + [len(i) for i in  docs_val])\n",
    "    x_train = pad_sequences(sequences=docs_train, maxlen=maxlen)\n",
    "    x_val = pad_sequences(sequences=docs_val, maxlen=maxlen)\n",
    "    x_test = pad_sequences(sequences=docs_test, maxlen=maxlen)\n",
    "\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    input_dim = max(np.max(x_train), np.max(x_val)) + 1\n",
    "#     input_dim = np.max(np.max(x_train) + 1    \n",
    "    print('#vocab: {} '.format(num_words), end=' ')\n",
    "    print(x_train.shape, x_val.shape, x_test.shape)\n",
    "    \n",
    "\n",
    "    model = create_model(input_dim)\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath='./fasttext_weights/weights.hdf5', verbose=0, save_best_only=True)\n",
    "\n",
    "    hist = model.fit(x_train, y_train,\n",
    "                     batch_size=16,\n",
    "                     validation_data=(x_val, y_val),\n",
    "                     epochs=epochs,\n",
    "                     callbacks=[EarlyStopping(patience=4, monitor='val_loss'), \n",
    "                                checkpointer])\n",
    "\n",
    "    # load best weights\n",
    "    model.load_weights('./fasttext_weights/weights.hdf5')\n",
    "    y_pred = model.predict_proba(x_val)\n",
    "    sum_loss += log_loss(y_pred=y_pred, y_true=np.nonzero(y_val)[1])\n",
    "\n",
    "    # save features\n",
    "    predict_prob_features[val_index] = y_pred\n",
    "    predict_prob_features_test += model.predict_proba(x_test)\n",
    "\n",
    "    print('valLoss: {}'.format(sum_loss/ite))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "for a, c in author2class.items():\n",
    "    df['{}_fasttext_ngram1'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_fasttext_ngram1'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/train_feature.csv')\n",
    "df_test.to_csv('./data/test_feature.csv')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
