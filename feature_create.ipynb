{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "from utils import preprocess\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_split = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./data/train.csv')\n",
    "df_test = pd.read_csv('./data/test.csv')\n",
    "text = df.text.values\n",
    "text_test = df_test.text.values\n",
    "\n",
    "author2class = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "class2author = ['EAP', 'HPL', 'MWS']\n",
    "y = np.array([author2class[a] for a in df.author])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vector(vec):\n",
    "    n = vec.vector_size\n",
    "    x = np.zeros((len(df), n))\n",
    "    for i, doc in enumerate(text):\n",
    "        doc_vec = np.zeros(n)\n",
    "        words = preprocess(doc).lower().split()\n",
    "        num_words = 0\n",
    "        for w in words:\n",
    "            if w in vec.vocab:\n",
    "                doc_vec += vec[w]\n",
    "                num_words += 1\n",
    "        doc_vec /= num_words\n",
    "        x[i] = doc_vec\n",
    "\n",
    "    x_test = np.zeros((len(df_test), n))\n",
    "    for i, doc in enumerate(text_test):\n",
    "        doc_vec = np.zeros(n)\n",
    "        words = preprocess(doc).lower().split()\n",
    "        num_words = 0\n",
    "        for w in words:\n",
    "            if w in vec.vocab:\n",
    "                doc_vec += vec[w]\n",
    "                num_words += 1\n",
    "        doc_vec /= num_words\n",
    "        x_test[i] = doc_vec\n",
    "    return x, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic(x, x_test, seed=7):\n",
    "    num_split = 5\n",
    "    kf = KFold(n_splits=num_split, random_state=seed, shuffle=True)\n",
    "    loss = 0.\n",
    "\n",
    "    predict_prob_features = np.zeros((len(df), 3))\n",
    "    predict_prob_features_test = np.zeros((len(df_test), 3))\n",
    "\n",
    "    for train_index, val_index in kf.split(x):\n",
    "        x_train, x_val = x[train_index], x[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        model = LogisticRegression()\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict_proba(x_val)\n",
    "        predict_prob_features_test += model.predict_proba(x_test)\n",
    "        predict_prob_features[val_index] = y_pred\n",
    "        loss += log_loss(y_pred=y_pred, y_true=y_val)\n",
    "\n",
    "    print(loss/5)\n",
    "    return predict_prob_features, predict_prob_features_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.521292042282\n"
     ]
    }
   ],
   "source": [
    "#  ./fasttext skipgram -input ../data/fasttext-inputs.txt -output model -minCount 1  -neg 15 -ws 10 -epoch 7\n",
    "vec = word2vec.KeyedVectors.load_word2vec_format('./fastText/model.vec')\n",
    "\n",
    "x, x_test = create_vector(vec)\n",
    "\n",
    "predict_prob_features, predict_prob_features_test = logistic(x, x_test)\n",
    "\n",
    "for a, c in author2class.items():\n",
    "    df['{}_fasttext_logi'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_fasttext_logi'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.527408070746\n"
     ]
    }
   ],
   "source": [
    "vec = word2vec.KeyedVectors.load_word2vec_format('./fastText/skip20_min2_neg15_epoch_7_ws_20.vec') # 0.527408070746\n",
    "\n",
    "x, x_test = create_vector(vec)\n",
    "\n",
    "predict_prob_features, predict_prob_features_test = logistic(x, x_test, 7)\n",
    "\n",
    "for a, c in author2class.items():\n",
    "    df['{}_fasttext_low_dim_logi'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_fasttext_low_dim_logi'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.802494276952\n"
     ]
    }
   ],
   "source": [
    "vec = word2vec.KeyedVectors.load_word2vec_format('./fastText/cbow100_min1_neg15_epoch_7_ws_10.vec')\n",
    "\n",
    "x, x_test = create_vector(vec)\n",
    "\n",
    "predict_prob_features, predict_prob_features_test = logistic(x, x_test, 9)\n",
    "\n",
    "for a, c in author2class.items():\n",
    "    df['{}_fasttext_cbow_logi'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_fasttext_cbow_logi'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def vectorizer_feature(vectorizer, seed=8, num_split=5, alphas=[1.]):\n",
    "    param_grid = dict(alpha=alphas)\n",
    "    print(param_grid, vectorizer)\n",
    "    \n",
    "    kf = KFold(n_splits=num_split, random_state=seed, shuffle=True)\n",
    "    sum_loss = 0.\n",
    "\n",
    "    predict_prob_features = np.zeros((len(df), 3))\n",
    "    predict_prob_features_test = np.zeros((len(df_test), 3))\n",
    "    ite = 0\n",
    "    for train_index, val_index in kf.split(text):\n",
    "        ite += 1\n",
    "        print('{}/{}: #Trains: {}, #Val: {}'.format(ite, num_split, len(train_index), len(val_index)), end=' ')\n",
    "        text_train, text_val = text[train_index], text[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "        x_train = vectorizer.fit_transform(text_train)\n",
    "        x_val = vectorizer.transform(text_val)\n",
    "        \n",
    "        \n",
    "        if len(alphas) > 1:\n",
    "            model = MultinomialNB()\n",
    "            clf = GridSearchCV(estimator=model, param_grid=param_grid, cv=10, scoring='neg_log_loss', n_jobs=-1)\n",
    "            clf.fit(x_train, y_train)\n",
    "            model = clf.best_estimator_            \n",
    "        else:\n",
    "            alpha = 1.\n",
    "            if len(alphas) == 1:\n",
    "                alpha = alphas[0]\n",
    "            model = MultinomialNB(alpha)\n",
    "            model.fit(x_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict_proba(x_val)\n",
    "\n",
    "        \n",
    "        # save features\n",
    "        predict_prob_features[val_index] = y_pred\n",
    "        predict_prob_features_test += model.predict_proba(vectorizer.transform(text_test))\n",
    "        \n",
    "        best_param = model.alpha\n",
    "\n",
    "        loss = log_loss(y_pred=y_pred, y_true=y_val)\n",
    "        sum_loss += loss\n",
    "\n",
    "        \n",
    "        print('valLoss: {}, best_param α= {}'.format(loss, best_param))\n",
    "        \n",
    "    print(sum_loss/num_split)\n",
    "    return predict_prob_features, predict_prob_features_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': [0.007]} TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "1/5: #Trains: 15663, #Val: 3916 valLoss: 0.4191531979947033, best_param α= 0.007\n",
      "2/5: #Trains: 15663, #Val: 3916 valLoss: 0.3979100889218946, best_param α= 0.007\n",
      "3/5: #Trains: 15663, #Val: 3916 valLoss: 0.403906857121704, best_param α= 0.007\n",
      "4/5: #Trains: 15663, #Val: 3916 valLoss: 0.4146626933703398, best_param α= 0.007\n",
      "5/5: #Trains: 15664, #Val: 3915 valLoss: 0.4054966330640256, best_param α= 0.007\n",
      "0.408225894095\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='word')\n",
    "predict_prob_features, predict_prob_features_test = vectorizer_feature(vectorizer, 7, alphas=[0.007]) # from [0.005,0.007, 0.01]\n",
    "for a, c in author2class.items():\n",
    "    df['{}_word_tfidf_NB'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_word_tfidf_NB'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': [0.013]} TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 5), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "1/5: #Trains: 15663, #Val: 3916 valLoss: 0.3951854930297089, best_param α= 0.013\n",
      "2/5: #Trains: 15663, #Val: 3916 valLoss: 0.37123938784736266, best_param α= 0.013\n",
      "3/5: #Trains: 15663, #Val: 3916 valLoss: 0.4002779893173318, best_param α= 0.013\n",
      "4/5: #Trains: 15663, #Val: 3916 valLoss: 0.3916746370981128, best_param α= 0.013\n",
      "5/5: #Trains: 15664, #Val: 3915 valLoss: 0.3654883416610354, best_param α= 0.013\n",
      "0.384773169791\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 5), analyzer='char')\n",
    "predict_prob_features, predict_prob_features_test = vectorizer_feature(vectorizer, 8, alphas=[0.013]) # 0.012, 0.013, 0.014\n",
    "for a, c in author2class.items():\n",
    "    df['{}_char_tfidf_NB'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_char_tfidf_NB'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': [1.2]} CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "1/5: #Trains: 15663, #Val: 3916 valLoss: 0.7884866155256142, best_param α= 1.2\n",
      "2/5: #Trains: 15663, #Val: 3916 valLoss: 0.8623959123023731, best_param α= 1.2\n",
      "3/5: #Trains: 15663, #Val: 3916 valLoss: 0.8252652372000494, best_param α= 1.2\n",
      "4/5: #Trains: 15663, #Val: 3916 valLoss: 0.776055308824219, best_param α= 1.2\n",
      "5/5: #Trains: 15664, #Val: 3915 valLoss: 0.8172614666911712, best_param α= 1.2\n",
      "0.813892908109\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 3), analyzer='word')\n",
    "predict_prob_features, predict_prob_features_test = vectorizer_feature(vectorizer, 9, alphas=[1.2])\n",
    "for a, c in author2class.items():\n",
    "    df['{}_word_count_NB'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_word_count_NB'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': [0.1, 0.2, 0.3, 0.4]} CountVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 4), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "1/5: #Trains: 15663, #Val: 3916 valLoss: 2.5132982409813023, best_param α= 0.2\n",
      "2/5: #Trains: 15663, #Val: 3916 valLoss: 2.4407679975214327, best_param α= 0.4\n",
      "3/5: #Trains: 15663, #Val: 3916 valLoss: 2.504654489912232, best_param α= 0.3\n",
      "4/5: #Trains: 15663, #Val: 3916 valLoss: 2.837462002734221, best_param α= 0.2\n",
      "5/5: #Trains: 15664, #Val: 3915 valLoss: 2.561142701900116, best_param α= 0.2\n",
      "2.57146508661\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 4), analyzer='char')\n",
    "predict_prob_features, predict_prob_features_test = vectorizer_feature(vectorizer, seed=7, alphas=[0.1, 0.2, 0.3, 0.4])\n",
    "for a, c in author2class.items():\n",
    "    df['{}_char_count_NB'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_char_count_NB'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': [1.5, 2.0, 2.5]} CountVectorizer(analyzer='char_wb', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 5), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n",
      "        tokenizer=None, vocabulary=None)\n",
      "1/5: #Trains: 15663, #Val: 3916 valLoss: 2.911286873284481, best_param α= 2.5\n",
      "2/5: #Trains: 15663, #Val: 3916 valLoss: 2.693418535058206, best_param α= 1.5\n",
      "3/5: #Trains: 15663, #Val: 3916 valLoss: 2.600940347993735, best_param α= 2.0\n",
      "4/5: #Trains: 15663, #Val: 3916 valLoss: 3.018726215529485, best_param α= 2.0\n",
      "5/5: #Trains: 15664, #Val: 3915 valLoss: 2.699994701500262, best_param α= 2.0\n",
      "2.78487333467\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 5), analyzer='char_wb')\n",
    "predict_prob_features, predict_prob_features_test = vectorizer_feature(vectorizer, seed=10, alphas=[1.5, 2., 2.5]) # 0.5, 0.1, 1., 1.5, 2., 2.5, 3.\n",
    "for a, c in author2class.items():\n",
    "    df['{}_char_wb_count_NB'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_char_wb_count_NB'.format(a)] = predict_prob_features_test[:, c]/num_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MetaFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normal_latters = set(string.ascii_uppercase) | set(string.ascii_lowercase) | set(',.:;\"\\'? ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['num_words']      = np.array([len(t.split()) for t in df.text])\n",
    "df_test['num_words'] = np.array([len(t.split()) for t in df_test.text])\n",
    "\n",
    "df['num_chars']      = np.array([len(t) for t in df.text])\n",
    "df_test['num_chars'] = np.array([len(t) for t in df_test.text])\n",
    "\n",
    "df['average_num_chars']      = np.array([np.mean([len(word) for word in t.split()]) for t in df.text])\n",
    "df_test['average_num_chars'] = np.array([np.mean([len(word) for word in t.split()]) for t in df_test.text])\n",
    "\n",
    "df['num_uniq_words']      = np.array([len(set(t.split())) for t in df.text])\n",
    "df_test['num_uniq_words'] = np.array([len(set(t.split())) for t in df_test.text])\n",
    "\n",
    "df['num_uniq_chars']      = np.array([len(set(t)) for t in df.text])\n",
    "df_test['num_uniq_chars'] = np.array([len(set(t)) for t in df_test.text])\n",
    "\n",
    "df['rate_uniq_words']      = np.array([len(set(t.split()))/len(t.split()) for t in df.text])\n",
    "df_test['rate_uniq_words'] = np.array([len(set(t.split()))/len(t.split()) for t in df_test.text])\n",
    "\n",
    "df['rate_uniq_chars']       = np.array([len(set(t))/len(t) for t in df.text])\n",
    "df_test['rate_uniq_chars'] = np.array([len(set(t))/len(t) for t in df_test.text])\n",
    "\n",
    "\n",
    "special = ',' # ',.:;\"\\'?'\n",
    "for c in special:\n",
    "    df['num_'+c] = np.array([t.count(c) for t in df.text])\n",
    "    df_test['num_'+c] = np.array([t.count(c) for t in df_test.text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv('./data/train_feature.csv')\n",
    "df_test.to_csv('./data/test_feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
