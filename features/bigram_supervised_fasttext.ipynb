{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import preprocess\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding, Lambda\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_docs(df, n_gram_max=1):\n",
    "    docs = []\n",
    "\n",
    "    for i, text in enumerate(df.text):    \n",
    "        def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return q + ngrams\n",
    "\n",
    "        doc = preprocess(text).split()\n",
    "                        \n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "        \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./../data/train_feature.csv')\n",
    "df_test = pd.read_csv('./../data/test_feature.csv')\n",
    "text = df.text.values\n",
    "text_test = df_test.text.values\n",
    "\n",
    "author2class = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "class2author = ['EAP', 'HPL', 'MWS']\n",
    "y = np.array([author2class[a] for a in df.author])\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(input_dim, embeddings_dims=10):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5: #Trains: 15663, #Val: 3916 #vocab: 76629  (15663, 256) (3916, 256) (8392, 256)\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/50\n",
      "15663/15663 [==============================] - 16s - loss: 1.0761 - acc: 0.4045 - val_loss: 1.0558 - val_acc: 0.4076\n",
      "Epoch 2/50\n",
      "15663/15663 [==============================] - 17s - loss: 1.0001 - acc: 0.5006 - val_loss: 0.9562 - val_acc: 0.5409\n",
      "Epoch 3/50\n",
      "15663/15663 [==============================] - 17s - loss: 0.8566 - acc: 0.7141 - val_loss: 0.8264 - val_acc: 0.6941\n",
      "Epoch 4/50\n",
      "15663/15663 [==============================] - 17s - loss: 0.7121 - acc: 0.7987 - val_loss: 0.7217 - val_acc: 0.7449\n",
      "Epoch 5/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.5964 - acc: 0.8406 - val_loss: 0.6467 - val_acc: 0.7579\n",
      "Epoch 6/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.5053 - acc: 0.8691 - val_loss: 0.5855 - val_acc: 0.7863\n",
      "Epoch 7/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.4311 - acc: 0.8889 - val_loss: 0.5391 - val_acc: 0.8121\n",
      "Epoch 8/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.3698 - acc: 0.9104 - val_loss: 0.4986 - val_acc: 0.8179\n",
      "Epoch 9/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.3180 - acc: 0.9240 - val_loss: 0.4668 - val_acc: 0.8251\n",
      "Epoch 10/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.2738 - acc: 0.9380 - val_loss: 0.4414 - val_acc: 0.8312\n",
      "Epoch 11/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.2357 - acc: 0.9494 - val_loss: 0.4187 - val_acc: 0.8458\n",
      "Epoch 12/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.2038 - acc: 0.9582 - val_loss: 0.4035 - val_acc: 0.8442\n",
      "Epoch 13/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.1761 - acc: 0.9642 - val_loss: 0.3858 - val_acc: 0.8575\n",
      "Epoch 14/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.1524 - acc: 0.9705 - val_loss: 0.3744 - val_acc: 0.8578\n",
      "Epoch 15/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.1320 - acc: 0.9754 - val_loss: 0.3649 - val_acc: 0.8611\n",
      "Epoch 16/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.1142 - acc: 0.9793 - val_loss: 0.3574 - val_acc: 0.8654\n",
      "Epoch 17/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.0991 - acc: 0.9824 - val_loss: 0.3498 - val_acc: 0.8636\n",
      "Epoch 18/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.0859 - acc: 0.9856 - val_loss: 0.3452 - val_acc: 0.8695\n",
      "Epoch 19/50\n",
      "15663/15663 [==============================] - 19s - loss: 0.0745 - acc: 0.9874 - val_loss: 0.3440 - val_acc: 0.8677\n",
      "Epoch 20/50\n",
      "15663/15663 [==============================] - 21s - loss: 0.0643 - acc: 0.9901 - val_loss: 0.3409 - val_acc: 0.8703\n",
      "Epoch 21/50\n",
      "15663/15663 [==============================] - 20s - loss: 0.0564 - acc: 0.9912 - val_loss: 0.3391 - val_acc: 0.8708\n",
      "Epoch 22/50\n",
      "15663/15663 [==============================] - 19s - loss: 0.0489 - acc: 0.9928 - val_loss: 0.3356 - val_acc: 0.8700\n",
      "Epoch 23/50\n",
      "15663/15663 [==============================] - 21s - loss: 0.0429 - acc: 0.9941 - val_loss: 0.3359 - val_acc: 0.8708\n",
      "Epoch 24/50\n",
      "15663/15663 [==============================] - 19s - loss: 0.0370 - acc: 0.9950 - val_loss: 0.3461 - val_acc: 0.8664\n",
      "Epoch 25/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.0324 - acc: 0.9960 - val_loss: 0.3460 - val_acc: 0.8664\n",
      "Epoch 26/50\n",
      "15663/15663 [==============================] - 20s - loss: 0.0285 - acc: 0.9962 - val_loss: 0.3430 - val_acc: 0.8693\n",
      "Epoch 27/50\n",
      "15663/15663 [==============================] - 20s - loss: 0.0249 - acc: 0.9971 - val_loss: 0.3480 - val_acc: 0.8700\n",
      "7936/8392 [===========================>..] - ETA: 0svalLoss: 0.3355972463432824\n",
      "2/5: #Trains: 15663, #Val: 3916 #vocab: 76629  (15663, 256) (3916, 256) (8392, 256)\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/50\n",
      "15663/15663 [==============================] - 18s - loss: 1.0781 - acc: 0.4023 - val_loss: 1.0562 - val_acc: 0.4160\n",
      "Epoch 2/50\n",
      "15663/15663 [==============================] - 21s - loss: 1.0043 - acc: 0.4922 - val_loss: 0.9510 - val_acc: 0.557506\n",
      "Epoch 3/50\n",
      "15663/15663 [==============================] - 20s - loss: 0.8616 - acc: 0.6872 - val_loss: 0.8211 - val_acc: 0.6785\n",
      "Epoch 4/50\n",
      "15663/15663 [==============================] - 22s - loss: 0.7168 - acc: 0.7919 - val_loss: 0.7126 - val_acc: 0.7643\n",
      "Epoch 5/50\n",
      "15663/15663 [==============================] - 20s - loss: 0.5995 - acc: 0.8391 - val_loss: 0.6313 - val_acc: 0.7883\n",
      "Epoch 6/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.5062 - acc: 0.8689 - val_loss: 0.5695 - val_acc: 0.8126\n",
      "Epoch 7/50\n",
      "15663/15663 [==============================] - 15s - loss: 0.4306 - acc: 0.8938 - val_loss: 0.5234 - val_acc: 0.8200\n",
      "Epoch 8/50\n",
      "15663/15663 [==============================] - 16s - loss: 0.3682 - acc: 0.9102 - val_loss: 0.4870 - val_acc: 0.8281\n",
      "Epoch 9/50\n",
      "15663/15663 [==============================] - 16s - loss: 0.3160 - acc: 0.9261 - val_loss: 0.4519 - val_acc: 0.8394\n",
      "Epoch 10/50\n",
      "15663/15663 [==============================] - 16s - loss: 0.2713 - acc: 0.9385 - val_loss: 0.4332 - val_acc: 0.8389\n",
      "Epoch 11/50\n",
      "15663/15663 [==============================] - 15s - loss: 0.2334 - acc: 0.9486 - val_loss: 0.4045 - val_acc: 0.8514\n",
      "Epoch 12/50\n",
      "15663/15663 [==============================] - 15s - loss: 0.2010 - acc: 0.9568 - val_loss: 0.3900 - val_acc: 0.8537\n",
      "Epoch 13/50\n",
      "15663/15663 [==============================] - 15s - loss: 0.1729 - acc: 0.9653 - val_loss: 0.3755 - val_acc: 0.8583\n",
      "Epoch 14/50\n",
      "15663/15663 [==============================] - 15s - loss: 0.1491 - acc: 0.9699 - val_loss: 0.3636 - val_acc: 0.8603\n",
      "Epoch 15/50\n",
      "15663/15663 [==============================] - 17s - loss: 0.1286 - acc: 0.9760 - val_loss: 0.3517 - val_acc: 0.8629\n",
      "Epoch 16/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.1109 - acc: 0.9797 - val_loss: 0.3484 - val_acc: 0.8626\n",
      "Epoch 17/50\n",
      "15663/15663 [==============================] - 17s - loss: 0.0957 - acc: 0.9830 - val_loss: 0.3397 - val_acc: 0.8670\n",
      "Epoch 18/50\n",
      "15663/15663 [==============================] - 17s - loss: 0.0827 - acc: 0.9853 - val_loss: 0.3381 - val_acc: 0.8670\n",
      "Epoch 19/50\n",
      "15663/15663 [==============================] - 17s - loss: 0.0712 - acc: 0.9880 - val_loss: 0.3328 - val_acc: 0.8695\n",
      "Epoch 20/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.0619 - acc: 0.9900 - val_loss: 0.3344 - val_acc: 0.8690\n",
      "Epoch 21/50\n",
      "15663/15663 [==============================] - 17s - loss: 0.0535 - acc: 0.9918 - val_loss: 0.3307 - val_acc: 0.8705\n",
      "Epoch 22/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.0462 - acc: 0.9937 - val_loss: 0.3329 - val_acc: 0.8716\n",
      "Epoch 23/50\n",
      "15663/15663 [==============================] - 16s - loss: 0.0402 - acc: 0.9946 - val_loss: 0.3325 - val_acc: 0.8718\n",
      "Epoch 24/50\n",
      "15663/15663 [==============================] - 16s - loss: 0.0349 - acc: 0.9953 - val_loss: 0.3344 - val_acc: 0.8736\n",
      "Epoch 25/50\n",
      "15663/15663 [==============================] - 16s - loss: 0.0301 - acc: 0.9960 - val_loss: 0.3362 - val_acc: 0.8736\n",
      "Epoch 26/50\n",
      "15663/15663 [==============================] - 17s - loss: 0.0264 - acc: 0.9964 - val_loss: 0.3453 - val_acc: 0.8726\n",
      "7488/8392 [=========================>....] - ETA: 0svalLoss: 0.33312968735433046\n",
      "3/5: #Trains: 15663, #Val: 3916 #vocab: 76629  (15663, 256) (3916, 256) (8392, 256)\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/50\n",
      "15663/15663 [==============================] - 17s - loss: 1.0771 - acc: 0.4021 - val_loss: 1.0539 - val_acc: 0.4142\n",
      "Epoch 2/50\n",
      "15663/15663 [==============================] - 18s - loss: 1.0033 - acc: 0.4858 - val_loss: 0.9489 - val_acc: 0.5590\n",
      "Epoch 3/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.8590 - acc: 0.7115 - val_loss: 0.8168 - val_acc: 0.7153\n",
      "Epoch 4/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.7117 - acc: 0.8004 - val_loss: 0.7098 - val_acc: 0.7643\n",
      "Epoch 5/50\n",
      "15663/15663 [==============================] - 17s - loss: 0.5933 - acc: 0.8426 - val_loss: 0.6307 - val_acc: 0.7965\n",
      "Epoch 6/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.5001 - acc: 0.8703 - val_loss: 0.5704 - val_acc: 0.8001\n",
      "Epoch 7/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 16s - loss: 0.4250 - acc: 0.8909 - val_loss: 0.5261 - val_acc: 0.8095\n",
      "Epoch 8/50\n",
      "15663/15663 [==============================] - 16s - loss: 0.3626 - acc: 0.9110 - val_loss: 0.4834 - val_acc: 0.8299\n",
      "Epoch 9/50\n",
      "15663/15663 [==============================] - 16s - loss: 0.3101 - acc: 0.9259 - val_loss: 0.4535 - val_acc: 0.8394\n",
      "Epoch 10/50\n",
      "15663/15663 [==============================] - 17s - loss: 0.2654 - acc: 0.9396 - val_loss: 0.4291 - val_acc: 0.8460\n",
      "Epoch 11/50\n",
      "15663/15663 [==============================] - 16s - loss: 0.2276 - acc: 0.9499 - val_loss: 0.4065 - val_acc: 0.8519\n",
      "Epoch 12/50\n",
      "15663/15663 [==============================] - 16s - loss: 0.1955 - acc: 0.9586 - val_loss: 0.3993 - val_acc: 0.8473\n",
      "Epoch 13/50\n",
      "15663/15663 [==============================] - 17s - loss: 0.1679 - acc: 0.9655 - val_loss: 0.3762 - val_acc: 0.8539\n",
      "Epoch 14/50\n",
      "15663/15663 [==============================] - 17s - loss: 0.1448 - acc: 0.9710 - val_loss: 0.3664 - val_acc: 0.8585\n",
      "Epoch 15/50\n",
      "15663/15663 [==============================] - 16s - loss: 0.1246 - acc: 0.9757 - val_loss: 0.3570 - val_acc: 0.8641\n",
      "Epoch 16/50\n",
      "15663/15663 [==============================] - 16s - loss: 0.1073 - acc: 0.9810 - val_loss: 0.3483 - val_acc: 0.8634\n",
      "Epoch 17/50\n",
      "15663/15663 [==============================] - 17s - loss: 0.0921 - acc: 0.9838 - val_loss: 0.3415 - val_acc: 0.8680\n",
      "Epoch 18/50\n",
      "15663/15663 [==============================] - 16s - loss: 0.0794 - acc: 0.9870 - val_loss: 0.3390 - val_acc: 0.8682\n",
      "Epoch 19/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.0685 - acc: 0.9890 - val_loss: 0.3364 - val_acc: 0.8710\n",
      "Epoch 20/50\n",
      "15663/15663 [==============================] - 17s - loss: 0.0593 - acc: 0.9913 - val_loss: 0.3346 - val_acc: 0.8713\n",
      "Epoch 21/50\n",
      "15663/15663 [==============================] - 16s - loss: 0.0510 - acc: 0.9923 - val_loss: 0.3359 - val_acc: 0.8690\n",
      "Epoch 22/50\n",
      "15663/15663 [==============================] - 15s - loss: 0.0441 - acc: 0.9932 - val_loss: 0.3390 - val_acc: 0.8682\n",
      "Epoch 23/50\n",
      "15663/15663 [==============================] - 16s - loss: 0.0383 - acc: 0.9946 - val_loss: 0.3367 - val_acc: 0.8721\n",
      "Epoch 24/50\n",
      "15663/15663 [==============================] - 17s - loss: 0.0332 - acc: 0.9951 - val_loss: 0.3393 - val_acc: 0.8695\n",
      "Epoch 25/50\n",
      "15663/15663 [==============================] - 18s - loss: 0.0287 - acc: 0.9961 - val_loss: 0.3368 - val_acc: 0.8690\n",
      "8256/8392 [============================>.] - ETA: 0svalLoss: 0.3336169154800217\n",
      "4/5: #Trains: 15663, #Val: 3916 #vocab: 76629  (15663, 256) (3916, 256) (8392, 256)\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/50\n",
      "15663/15663 [==============================] - 18s - loss: 1.0777 - acc: 0.4028 - val_loss: 1.0632 - val_acc: 0.4060\n",
      "Epoch 2/50\n",
      "15663/15663 [==============================] - 18s - loss: 1.0083 - acc: 0.4933 - val_loss: 0.9743 - val_acc: 0.5054\n",
      "Epoch 3/50\n",
      "15663/15663 [==============================] - 19s - loss: 0.8802 - acc: 0.6605 - val_loss: 0.8664 - val_acc: 0.6517\n",
      "Epoch 4/50\n",
      "15663/15663 [==============================] - 19s - loss: 0.7532 - acc: 0.7693 - val_loss: 0.7714 - val_acc: 0.7352\n",
      "Epoch 5/50\n",
      "15663/15663 [==============================] - 19s - loss: 0.6428 - acc: 0.8199 - val_loss: 0.6929 - val_acc: 0.7646\n",
      "Epoch 6/50\n",
      "15663/15663 [==============================] - 19s - loss: 0.5488 - acc: 0.8525 - val_loss: 0.6323 - val_acc: 0.8021\n",
      "Epoch 7/50\n",
      "15663/15663 [==============================] - 20s - loss: 0.4711 - acc: 0.8802 - val_loss: 0.5767 - val_acc: 0.7980\n",
      "Epoch 8/50\n",
      "15663/15663 [==============================] - 21s - loss: 0.4036 - acc: 0.9008 - val_loss: 0.5338 - val_acc: 0.8230\n",
      "Epoch 9/50\n",
      "15663/15663 [==============================] - 19s - loss: 0.3475 - acc: 0.9173 - val_loss: 0.4965 - val_acc: 0.8223\n",
      "Epoch 10/50\n",
      "15663/15663 [==============================] - 19s - loss: 0.2989 - acc: 0.9325 - val_loss: 0.4662 - val_acc: 0.8348\n",
      "Epoch 11/50\n",
      "15663/15663 [==============================] - 19s - loss: 0.2583 - acc: 0.9441 - val_loss: 0.4413 - val_acc: 0.8417\n",
      "Epoch 12/50\n",
      "15663/15663 [==============================] - 19s - loss: 0.2223 - acc: 0.9549 - val_loss: 0.4208 - val_acc: 0.8427\n",
      "Epoch 13/50\n",
      "15663/15663 [==============================] - 19s - loss: 0.1922 - acc: 0.9612 - val_loss: 0.4030 - val_acc: 0.8509\n",
      "Epoch 14/50\n",
      "15663/15663 [==============================] - 19s - loss: 0.1658 - acc: 0.9681 - val_loss: 0.3913 - val_acc: 0.8524\n",
      "Epoch 15/50\n",
      "15663/15663 [==============================] - 20s - loss: 0.1432 - acc: 0.9729 - val_loss: 0.3840 - val_acc: 0.8488\n",
      "Epoch 16/50\n",
      "15663/15663 [==============================] - 20s - loss: 0.1240 - acc: 0.9779 - val_loss: 0.3691 - val_acc: 0.8621\n",
      "Epoch 17/50\n",
      "15663/15663 [==============================] - 20s - loss: 0.1070 - acc: 0.9820 - val_loss: 0.3651 - val_acc: 0.8629\n",
      "Epoch 18/50\n",
      "15663/15663 [==============================] - 21s - loss: 0.0932 - acc: 0.9842 - val_loss: 0.3575 - val_acc: 0.8634\n",
      "Epoch 19/50\n",
      "15663/15663 [==============================] - 21s - loss: 0.0804 - acc: 0.9872 - val_loss: 0.3530 - val_acc: 0.8667\n",
      "Epoch 20/50\n",
      "15663/15663 [==============================] - 20s - loss: 0.0698 - acc: 0.9892 - val_loss: 0.3522 - val_acc: 0.8687\n",
      "Epoch 21/50\n",
      "15663/15663 [==============================] - 20s - loss: 0.0606 - acc: 0.9912 - val_loss: 0.3509 - val_acc: 0.8662\n",
      "Epoch 22/50\n",
      "15663/15663 [==============================] - 19s - loss: 0.0529 - acc: 0.9925 - val_loss: 0.3527 - val_acc: 0.8700\n",
      "Epoch 23/50\n",
      "15663/15663 [==============================] - 19s - loss: 0.0458 - acc: 0.9941 - val_loss: 0.3545 - val_acc: 0.8649\n",
      "Epoch 24/50\n",
      "15663/15663 [==============================] - 19s - loss: 0.0399 - acc: 0.9950 - val_loss: 0.3531 - val_acc: 0.8677\n",
      "Epoch 25/50\n",
      "15663/15663 [==============================] - 20s - loss: 0.0349 - acc: 0.9955 - val_loss: 0.3609 - val_acc: 0.8654\n",
      "Epoch 26/50\n",
      "15663/15663 [==============================] - 17s - loss: 0.0306 - acc: 0.9964 - val_loss: 0.3613 - val_acc: 0.8672\n",
      "7136/8392 [========================>.....] - ETA: 0svalLoss: 0.3379491771605581\n",
      "5/5: #Trains: 15664, #Val: 3915 #vocab: 76629  (15664, 256) (3915, 256) (8392, 256)\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/50\n",
      "15664/15664 [==============================] - 17s - loss: 1.0766 - acc: 0.4037 - val_loss: 1.0569 - val_acc: 0.4051\n",
      "Epoch 2/50\n",
      "15664/15664 [==============================] - 18s - loss: 1.0018 - acc: 0.4844 - val_loss: 0.9591 - val_acc: 0.5520\n",
      "Epoch 3/50\n",
      "15664/15664 [==============================] - 18s - loss: 0.8598 - acc: 0.7079 - val_loss: 0.8251 - val_acc: 0.7185\n",
      "Epoch 4/50\n",
      "15664/15664 [==============================] - 20s - loss: 0.7142 - acc: 0.7965 - val_loss: 0.7164 - val_acc: 0.7768\n",
      "Epoch 5/50\n",
      "15664/15664 [==============================] - 20s - loss: 0.5969 - acc: 0.8413 - val_loss: 0.6393 - val_acc: 0.7936\n",
      "Epoch 6/50\n",
      "15664/15664 [==============================] - 19s - loss: 0.5046 - acc: 0.8664 - val_loss: 0.5768 - val_acc: 0.8107\n",
      "Epoch 7/50\n",
      "15664/15664 [==============================] - 17s - loss: 0.4300 - acc: 0.8896 - val_loss: 0.5310 - val_acc: 0.8232\n",
      "Epoch 8/50\n",
      "15664/15664 [==============================] - 18s - loss: 0.3680 - acc: 0.9085 - val_loss: 0.4924 - val_acc: 0.8289\n",
      "Epoch 9/50\n",
      "15664/15664 [==============================] - 19s - loss: 0.3163 - acc: 0.9237 - val_loss: 0.4620 - val_acc: 0.8363\n",
      "Epoch 10/50\n",
      "15664/15664 [==============================] - 19s - loss: 0.2716 - acc: 0.9372 - val_loss: 0.4366 - val_acc: 0.8401\n",
      "Epoch 11/50\n",
      "15664/15664 [==============================] - 16s - loss: 0.2343 - acc: 0.9490 - val_loss: 0.4161 - val_acc: 0.8447\n",
      "Epoch 12/50\n",
      "15664/15664 [==============================] - 16s - loss: 0.2019 - acc: 0.9569 - val_loss: 0.4023 - val_acc: 0.8490\n",
      "Epoch 13/50\n",
      "15664/15664 [==============================] - 15s - loss: 0.1746 - acc: 0.9654 - val_loss: 0.3869 - val_acc: 0.8503\n",
      "Epoch 14/50\n",
      "15664/15664 [==============================] - 15s - loss: 0.1505 - acc: 0.9702 - val_loss: 0.3758 - val_acc: 0.8547\n",
      "Epoch 15/50\n",
      "15664/15664 [==============================] - 15s - loss: 0.1299 - acc: 0.9773 - val_loss: 0.3690 - val_acc: 0.8542\n",
      "Epoch 16/50\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15664/15664 [==============================] - 16s - loss: 0.1128 - acc: 0.9801 - val_loss: 0.3590 - val_acc: 0.8600\n",
      "Epoch 17/50\n",
      "15664/15664 [==============================] - 15s - loss: 0.0976 - acc: 0.9831 - val_loss: 0.3538 - val_acc: 0.8603\n",
      "Epoch 18/50\n",
      "15664/15664 [==============================] - 16s - loss: 0.0842 - acc: 0.9859 - val_loss: 0.3537 - val_acc: 0.8593\n",
      "Epoch 19/50\n",
      "15664/15664 [==============================] - 15s - loss: 0.0728 - acc: 0.9883 - val_loss: 0.3606 - val_acc: 0.8580\n",
      "Epoch 20/50\n",
      "15664/15664 [==============================] - 14s - loss: 0.0633 - acc: 0.9898 - val_loss: 0.3462 - val_acc: 0.8636\n",
      "Epoch 21/50\n",
      "15664/15664 [==============================] - 18s - loss: 0.0550 - acc: 0.9912 - val_loss: 0.3483 - val_acc: 0.8656\n",
      "Epoch 22/50\n",
      "15664/15664 [==============================] - 16s - loss: 0.0480 - acc: 0.9927 - val_loss: 0.3461 - val_acc: 0.8692\n",
      "Epoch 23/50\n",
      "15664/15664 [==============================] - 17s - loss: 0.0416 - acc: 0.9936 - val_loss: 0.3477 - val_acc: 0.8690\n",
      "Epoch 24/50\n",
      "15664/15664 [==============================] - 16s - loss: 0.0364 - acc: 0.9950 - val_loss: 0.3492 - val_acc: 0.8695\n",
      "Epoch 25/50\n",
      "15664/15664 [==============================] - 15s - loss: 0.0315 - acc: 0.9954 - val_loss: 0.3545 - val_acc: 0.8679\n",
      "Epoch 26/50\n",
      "15664/15664 [==============================] - 16s - loss: 0.0277 - acc: 0.9964 - val_loss: 0.3575 - val_acc: 0.8687\n",
      "Epoch 27/50\n",
      "15664/15664 [==============================] - 16s - loss: 0.0243 - acc: 0.9971 - val_loss: 0.3630 - val_acc: 0.8690\n",
      "7008/8392 [========================>.....] - ETA: 0svalLoss: 0.3395819911986644\n"
     ]
    }
   ],
   "source": [
    "n_gram_max = 2\n",
    "embedding_dims = 10\n",
    "\n",
    "raw_docs = create_docs(df, n_gram_max=n_gram_max)\n",
    "raw_docs_test = create_docs(df_test, n_gram_max=n_gram_max)\n",
    "\n",
    "seed = 7\n",
    "num_split = 5\n",
    "epochs = 50\n",
    "\n",
    "# for next training\n",
    "predict_prob_features = np.zeros((len(df), 3))\n",
    "predict_prob_features_test = np.zeros((len(df_test), 3))\n",
    "\n",
    "ite = 0\n",
    "sum_loss = 0.\n",
    "min_count = 2\n",
    "\n",
    "kf = KFold(n_splits=num_split, random_state=seed, shuffle=True)\n",
    "for train_index, val_index in kf.split(text):\n",
    "    ite += 1\n",
    "    print('{}/{}: #Trains: {}, #Val: {}'.format(ite, num_split, len(train_index), len(val_index)), end=' ')\n",
    "    \n",
    "    docs_train = [raw_docs[i] for i in train_index]\n",
    "    docs_val = [raw_docs[i] for i in val_index]\n",
    "\n",
    "    # get vocab\n",
    "    tokenizer = Tokenizer(filters='', lower=False)\n",
    "    #     tokenizer.fit_on_texts(docs_train)\n",
    "    tokenizer.fit_on_texts(docs_train + docs_val)\n",
    "\n",
    "    num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=num_words, filters='', lower=False)\n",
    "    tokenizer.fit_on_texts(docs_train + docs_val)    \n",
    "\n",
    "    docs_train = tokenizer.texts_to_sequences(docs_train)\n",
    "    docs_val   = tokenizer.texts_to_sequences(docs_val)\n",
    "    docs_test  = tokenizer.texts_to_sequences(raw_docs_test)\n",
    "\n",
    "    maxlen = 256\n",
    "    x_train = pad_sequences(sequences=docs_train, maxlen=maxlen)\n",
    "    x_val  = pad_sequences(sequences=docs_val, maxlen=maxlen)\n",
    "    x_test = pad_sequences(sequences=docs_test, maxlen=maxlen)\n",
    "\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    input_dim = max(np.max(x_train), np.max(x_val)) + 1\n",
    "    print('#vocab: {} '.format(num_words), end=' ')\n",
    "    print(x_train.shape, x_val.shape, x_test.shape)\n",
    "\n",
    "    model = create_model(input_dim)\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath='./../fasttext_weights/weights.hdf5', verbose=0, save_best_only=True)\n",
    "\n",
    "    hist = model.fit(x_train, y_train,\n",
    "                     batch_size=16,\n",
    "                     validation_data=(x_val, y_val),\n",
    "                     epochs=epochs,\n",
    "                     callbacks=[EarlyStopping(patience=4, monitor='val_loss'), \n",
    "                                checkpointer])\n",
    "\n",
    "    # load best weights\n",
    "    model.load_weights('./../fasttext_weights/weights.hdf5')\n",
    "    y_pred = model.predict_proba(x_val)\n",
    "    sum_loss += log_loss(y_pred=y_pred, y_true=np.nonzero(y_val)[1])\n",
    "    \n",
    "    # save features\n",
    "    predict_prob_features[val_index] = y_pred\n",
    "    predict_prob_features_test += model.predict_proba(x_test)\n",
    "    print('valLoss: {}'.format(sum_loss/ite))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for a, c in author2class.items():\n",
    "    df['{}_fasttext_bigram'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_fasttext_bigram'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('./../data/train_feature.csv')\n",
    "df_test.to_csv('./../data/test_feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
