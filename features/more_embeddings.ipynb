{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import preprocess\n",
    "\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "import sklearn\n",
    "from sklearn.linear_model import LogisticRegressionCV, LogisticRegression\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_split = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../data/'\n",
    "df = pd.read_csv(data_path + 'train_feature.csv')\n",
    "df_test = pd.read_csv(data_path + 'test_feature.csv')\n",
    "text = df.text.values\n",
    "text_test = df_test.text.values\n",
    "\n",
    "author2class = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "class2author = ['EAP', 'HPL', 'MWS']\n",
    "y = np.array([author2class[a] for a in df.author])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_vector(vec):\n",
    "    n = vec.vector_size\n",
    "    x = np.zeros((len(df), n))\n",
    "    for i, doc in enumerate(text):\n",
    "        doc_vec = np.zeros(n)\n",
    "        words = preprocess(doc).lower().split()\n",
    "        num_words = 0\n",
    "        for w in words:\n",
    "            if w in vec.vocab:\n",
    "                doc_vec += vec[w]\n",
    "                num_words += 1\n",
    "        doc_vec /= num_words\n",
    "        x[i] = doc_vec\n",
    "\n",
    "    x_test = np.zeros((len(df_test), n))\n",
    "    for i, doc in enumerate(text_test):\n",
    "        doc_vec = np.zeros(n)\n",
    "        words = preprocess(doc).lower().split()\n",
    "        num_words = 0\n",
    "        for w in words:\n",
    "            if w in vec.vocab:\n",
    "                doc_vec += vec[w]\n",
    "                num_words += 1\n",
    "        doc_vec /= num_words\n",
    "        x_test[i] = doc_vec\n",
    "    return x, x_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def logistic(x, x_test, seed=7):\n",
    "    num_split = 5\n",
    "    kf = KFold(n_splits=num_split, random_state=seed, shuffle=True)\n",
    "    loss = 0.\n",
    "\n",
    "    predict_prob_features = np.zeros((len(df), 3))\n",
    "    predict_prob_features_test = np.zeros((len(df_test), 3))\n",
    "\n",
    "    for train_index, val_index in kf.split(x):\n",
    "        x_train, x_val = x[train_index], x[val_index]\n",
    "        y_train, y_val = y[train_index], y[val_index]\n",
    "        model = LogisticRegression()\n",
    "        model.fit(x_train, y_train)\n",
    "        y_pred = model.predict_proba(x_val)\n",
    "        predict_prob_features_test += model.predict_proba(x_test)\n",
    "        predict_prob_features[val_index] = y_pred\n",
    "        c_loss = log_loss(y_pred=y_pred, y_true=y_val)\n",
    "        print(c_loss)\n",
    "        loss += c_loss\n",
    "\n",
    "    print(loss/num_split)\n",
    "    return predict_prob_features, predict_prob_features_test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Unsupervised FastText"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.779193831664\n",
      "0.800083893574\n",
      "0.775175414186\n",
      "0.766897550796\n",
      "0.763102977046\n",
      "0.776890733453\n"
     ]
    }
   ],
   "source": [
    "vec = word2vec.KeyedVectors.load_word2vec_format('./../fastText/cbow20_min2_neg15_ws20_epoch7.vec')\n",
    "\n",
    "x, x_test = create_vector(vec)\n",
    "\n",
    "predict_prob_features, predict_prob_features_test = logistic(x, x_test, 15)\n",
    "\n",
    "for a, c in author2class.items():\n",
    "    df['{}_fasttext_cbow_wide_logi'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_fasttext_cbow_wide_logi'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('./../data/train_feature.csv')\n",
    "df_test.to_csv('./../data/test_feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
