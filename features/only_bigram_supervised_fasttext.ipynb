{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import preprocess\n",
    "\n",
    "import numpy as np\n",
    "import h5py\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, GlobalAveragePooling1D, Embedding, Lambda\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.utils import to_categorical\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_docs(df, n_gram_max=1):\n",
    "    docs = []\n",
    "\n",
    "    for i, text in enumerate(df.text):    \n",
    "        def add_ngram(q, n_gram_max):\n",
    "            ngrams = []\n",
    "            for n in range(2, n_gram_max+1):\n",
    "                for w_index in range(len(q)-n+1):\n",
    "                    ngrams.append('--'.join(q[w_index:w_index+n]))\n",
    "            return ngrams + q\n",
    "\n",
    "        doc = preprocess(text).split()\n",
    "                        \n",
    "        docs.append(' '.join(add_ngram(doc, n_gram_max)))\n",
    "        \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./../data/train_feature.csv')\n",
    "df_test = pd.read_csv('./../data/test_feature.csv')\n",
    "text = df.text.values\n",
    "text_test = df_test.text.values\n",
    "\n",
    "author2class = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "class2author = ['EAP', 'HPL', 'MWS']\n",
    "y = np.array([author2class[a] for a in df.author])\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(input_dim, embeddings_dims=10):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims))\n",
    "    model.add(GlobalAveragePooling1D())\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1/5: #Trains: 15663, #Val: 3916 #vocab: 75122  (15663, 1024) (3916, 1024) (8392, 1024)\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/60\n",
      "15663/15663 [==============================] - 20s - loss: 1.0861 - acc: 0.4037 - val_loss: 1.0826 - val_acc: 0.4032\n",
      "Epoch 2/60\n",
      "15663/15663 [==============================] - 19s - loss: 1.0759 - acc: 0.4037 - val_loss: 1.0722 - val_acc: 0.4040\n",
      "Epoch 3/60\n",
      "15663/15663 [==============================] - 20s - loss: 1.0533 - acc: 0.4121 - val_loss: 1.0421 - val_acc: 0.4096\n",
      "Epoch 4/60\n",
      "15663/15663 [==============================] - 20s - loss: 1.0114 - acc: 0.4753 - val_loss: 1.0002 - val_acc: 0.4517\n",
      "Epoch 5/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.9538 - acc: 0.5961 - val_loss: 0.9507 - val_acc: 0.5008\n",
      "Epoch 6/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.8879 - acc: 0.6930 - val_loss: 0.8910 - val_acc: 0.6241\n",
      "Epoch 7/60\n",
      "15663/15663 [==============================] - 22s - loss: 0.8200 - acc: 0.7499 - val_loss: 0.8361 - val_acc: 0.6902\n",
      "Epoch 8/60\n",
      "15663/15663 [==============================] - 17s - loss: 0.7556 - acc: 0.7862 - val_loss: 0.7852 - val_acc: 0.7240\n",
      "Epoch 9/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.6956 - acc: 0.8093 - val_loss: 0.7402 - val_acc: 0.7421\n",
      "Epoch 10/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.6415 - acc: 0.8244 - val_loss: 0.7025 - val_acc: 0.7339\n",
      "Epoch 11/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.5918 - acc: 0.8422 - val_loss: 0.6645 - val_acc: 0.7600\n",
      "Epoch 12/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.5482 - acc: 0.8534 - val_loss: 0.6360 - val_acc: 0.7720\n",
      "Epoch 13/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.5084 - acc: 0.8639 - val_loss: 0.6038 - val_acc: 0.7909\n",
      "Epoch 14/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.4726 - acc: 0.8735 - val_loss: 0.5815 - val_acc: 0.7952\n",
      "Epoch 15/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.4394 - acc: 0.8855 - val_loss: 0.5604 - val_acc: 0.8059\n",
      "Epoch 16/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.4092 - acc: 0.8935 - val_loss: 0.5373 - val_acc: 0.8149\n",
      "Epoch 17/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.3818 - acc: 0.9022 - val_loss: 0.5205 - val_acc: 0.8205\n",
      "Epoch 18/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.3566 - acc: 0.9094 - val_loss: 0.5025 - val_acc: 0.8189\n",
      "Epoch 19/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.3320 - acc: 0.9178 - val_loss: 0.4892 - val_acc: 0.8126\n",
      "Epoch 20/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.3097 - acc: 0.9249 - val_loss: 0.4720 - val_acc: 0.8299\n",
      "Epoch 21/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.2902 - acc: 0.9301 - val_loss: 0.4626 - val_acc: 0.8340\n",
      "Epoch 22/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.2713 - acc: 0.9353 - val_loss: 0.4481 - val_acc: 0.8322\n",
      "Epoch 23/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.2541 - acc: 0.9406 - val_loss: 0.4383 - val_acc: 0.8322\n",
      "Epoch 24/60\n",
      "15663/15663 [==============================] - 21s - loss: 0.2372 - acc: 0.9454 - val_loss: 0.4398 - val_acc: 0.8368\n",
      "Epoch 25/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.2220 - acc: 0.9508 - val_loss: 0.4181 - val_acc: 0.8424\n",
      "Epoch 26/60\n",
      "15663/15663 [==============================] - 17s - loss: 0.2081 - acc: 0.9549 - val_loss: 0.4084 - val_acc: 0.8453\n",
      "Epoch 27/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.1951 - acc: 0.9577 - val_loss: 0.4001 - val_acc: 0.8498\n",
      "Epoch 28/60\n",
      "15663/15663 [==============================] - 17s - loss: 0.1823 - acc: 0.9625 - val_loss: 0.4004 - val_acc: 0.8516\n",
      "Epoch 29/60\n",
      "15663/15663 [==============================] - 16s - loss: 0.1712 - acc: 0.9647 - val_loss: 0.3883 - val_acc: 0.8514\n",
      "Epoch 30/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.1605 - acc: 0.9683 - val_loss: 0.3829 - val_acc: 0.8550\n",
      "Epoch 31/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.1500 - acc: 0.9707 - val_loss: 0.3758 - val_acc: 0.8575\n",
      "Epoch 32/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.1410 - acc: 0.9727 - val_loss: 0.3696 - val_acc: 0.8596\n",
      "Epoch 33/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.1323 - acc: 0.9747 - val_loss: 0.3664 - val_acc: 0.8618\n",
      "Epoch 34/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.1238 - acc: 0.9762 - val_loss: 0.3628 - val_acc: 0.8596\n",
      "Epoch 35/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.1161 - acc: 0.9784 - val_loss: 0.3658 - val_acc: 0.8590\n",
      "Epoch 36/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.1092 - acc: 0.9805 - val_loss: 0.3561 - val_acc: 0.8624\n",
      "Epoch 37/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.1029 - acc: 0.9807 - val_loss: 0.3565 - val_acc: 0.8596\n",
      "Epoch 38/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.0964 - acc: 0.9831 - val_loss: 0.3501 - val_acc: 0.8687\n",
      "Epoch 39/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.0904 - acc: 0.9846 - val_loss: 0.3506 - val_acc: 0.8629\n",
      "Epoch 40/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.0848 - acc: 0.9851 - val_loss: 0.3446 - val_acc: 0.8685\n",
      "Epoch 41/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.0798 - acc: 0.9866 - val_loss: 0.3459 - val_acc: 0.8677\n",
      "Epoch 42/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.0752 - acc: 0.9874 - val_loss: 0.3409 - val_acc: 0.8698\n",
      "Epoch 43/60\n",
      "15663/15663 [==============================] - 21s - loss: 0.0706 - acc: 0.9883 - val_loss: 0.3428 - val_acc: 0.8698\n",
      "Epoch 44/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.0659 - acc: 0.9896 - val_loss: 0.3397 - val_acc: 0.8693\n",
      "Epoch 45/60\n",
      "15663/15663 [==============================] - 22s - loss: 0.0621 - acc: 0.9910 - val_loss: 0.3385 - val_acc: 0.8705\n",
      "Epoch 46/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.0584 - acc: 0.9909 - val_loss: 0.3426 - val_acc: 0.8703\n",
      "Epoch 47/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.0551 - acc: 0.9918 - val_loss: 0.3369 - val_acc: 0.8710\n",
      "Epoch 48/60\n",
      "15663/15663 [==============================] - 21s - loss: 0.0518 - acc: 0.9922 - val_loss: 0.3402 - val_acc: 0.8726\n",
      "Epoch 49/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.0487 - acc: 0.9934 - val_loss: 0.3390 - val_acc: 0.8677\n",
      "Epoch 50/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.0458 - acc: 0.9933 - val_loss: 0.3483 - val_acc: 0.8639\n",
      "Epoch 51/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.0438 - acc: 0.9930 - val_loss: 0.3418 - val_acc: 0.8677\n",
      "Epoch 52/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.0408 - acc: 0.9946 - val_loss: 0.3390 - val_acc: 0.8690\n",
      "8064/8392 [===========================>..] - ETA: 0svalLoss: 0.33693876189715016\n",
      "2/5: #Trains: 15663, #Val: 3916 #vocab: 75122  (15663, 1024) (3916, 1024) (8392, 1024)\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/60\n",
      "15663/15663 [==============================] - 17s - loss: 1.0866 - acc: 0.4027 - val_loss: 1.0828 - val_acc: 0.4037\n",
      "Epoch 2/60\n",
      "15663/15663 [==============================] - 20s - loss: 1.0765 - acc: 0.4038 - val_loss: 1.0710 - val_acc: 0.4040\n",
      "Epoch 3/60\n",
      "15663/15663 [==============================] - 20s - loss: 1.0532 - acc: 0.4080 - val_loss: 1.0403 - val_acc: 0.4124\n",
      "Epoch 4/60\n",
      "15663/15663 [==============================] - 19s - loss: 1.0096 - acc: 0.4703 - val_loss: 0.9928 - val_acc: 0.5013\n",
      "Epoch 5/60\n",
      "15663/15663 [==============================] - 21s - loss: 0.9492 - acc: 0.6039 - val_loss: 0.9369 - val_acc: 0.5212\n",
      "Epoch 6/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.8794 - acc: 0.7057 - val_loss: 0.8734 - val_acc: 0.6688\n",
      "Epoch 7/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.8075 - acc: 0.7669 - val_loss: 0.8149 - val_acc: 0.7380\n",
      "Epoch 8/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.7397 - acc: 0.7924 - val_loss: 0.7598 - val_acc: 0.7561\n",
      "Epoch 9/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 21s - loss: 0.6784 - acc: 0.8110 - val_loss: 0.7147 - val_acc: 0.7651\n",
      "Epoch 10/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.6231 - acc: 0.8291 - val_loss: 0.6704 - val_acc: 0.7845\n",
      "Epoch 11/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.5734 - acc: 0.8437 - val_loss: 0.6359 - val_acc: 0.7817\n",
      "Epoch 12/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.5286 - acc: 0.8579 - val_loss: 0.6034 - val_acc: 0.7998\n",
      "Epoch 13/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.4894 - acc: 0.8667 - val_loss: 0.5803 - val_acc: 0.7960\n",
      "Epoch 14/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.4520 - acc: 0.8801 - val_loss: 0.5527 - val_acc: 0.7980\n",
      "Epoch 15/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.4192 - acc: 0.8881 - val_loss: 0.5325 - val_acc: 0.8113\n",
      "Epoch 16/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.3892 - acc: 0.8966 - val_loss: 0.5055 - val_acc: 0.8266\n",
      "Epoch 17/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.3610 - acc: 0.9095 - val_loss: 0.4886 - val_acc: 0.8233\n",
      "Epoch 18/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.3352 - acc: 0.9148 - val_loss: 0.4723 - val_acc: 0.8332\n",
      "Epoch 19/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.3115 - acc: 0.9219 - val_loss: 0.4606 - val_acc: 0.8264\n",
      "Epoch 20/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.2885 - acc: 0.9302 - val_loss: 0.4436 - val_acc: 0.8399\n",
      "Epoch 21/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.2690 - acc: 0.9331 - val_loss: 0.4306 - val_acc: 0.8414\n",
      "Epoch 22/60\n",
      "15663/15663 [==============================] - 21s - loss: 0.2500 - acc: 0.9411 - val_loss: 0.4244 - val_acc: 0.8407\n",
      "Epoch 23/60\n",
      "15663/15663 [==============================] - 22s - loss: 0.2334 - acc: 0.9455 - val_loss: 0.4184 - val_acc: 0.8417\n",
      "Epoch 24/60\n",
      "15663/15663 [==============================] - 21s - loss: 0.2160 - acc: 0.9491 - val_loss: 0.3996 - val_acc: 0.8473\n",
      "Epoch 25/60\n",
      "15663/15663 [==============================] - 22s - loss: 0.2013 - acc: 0.9547 - val_loss: 0.3914 - val_acc: 0.8498\n",
      "Epoch 26/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.1875 - acc: 0.9582 - val_loss: 0.3922 - val_acc: 0.8514\n",
      "Epoch 27/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.1742 - acc: 0.9630 - val_loss: 0.3812 - val_acc: 0.8527\n",
      "Epoch 28/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.1626 - acc: 0.9648 - val_loss: 0.3761 - val_acc: 0.8516\n",
      "Epoch 29/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.1509 - acc: 0.9685 - val_loss: 0.3645 - val_acc: 0.8562\n",
      "Epoch 30/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.1408 - acc: 0.9698 - val_loss: 0.3653 - val_acc: 0.8537\n",
      "Epoch 31/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.1314 - acc: 0.9727 - val_loss: 0.3562 - val_acc: 0.8593\n",
      "Epoch 32/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.1222 - acc: 0.9750 - val_loss: 0.3511 - val_acc: 0.8616\n",
      "Epoch 33/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.1138 - acc: 0.9778 - val_loss: 0.3669 - val_acc: 0.8565\n",
      "Epoch 34/60\n",
      "15663/15663 [==============================] - 17s - loss: 0.1064 - acc: 0.9787 - val_loss: 0.3526 - val_acc: 0.8618\n",
      "Epoch 35/60\n",
      "15663/15663 [==============================] - 17s - loss: 0.0998 - acc: 0.9812 - val_loss: 0.3422 - val_acc: 0.8644\n",
      "Epoch 36/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.0927 - acc: 0.9815 - val_loss: 0.3438 - val_acc: 0.8639\n",
      "Epoch 37/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.0859 - acc: 0.9842 - val_loss: 0.3382 - val_acc: 0.8644\n",
      "Epoch 38/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.0806 - acc: 0.9849 - val_loss: 0.3357 - val_acc: 0.8634\n",
      "Epoch 39/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.0752 - acc: 0.9866 - val_loss: 0.3418 - val_acc: 0.8608\n",
      "Epoch 40/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.0701 - acc: 0.9868 - val_loss: 0.3352 - val_acc: 0.8644\n",
      "Epoch 41/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.0658 - acc: 0.9881 - val_loss: 0.3488 - val_acc: 0.8601\n",
      "Epoch 42/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.0612 - acc: 0.9891 - val_loss: 0.3326 - val_acc: 0.8667\n",
      "Epoch 43/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.0575 - acc: 0.9900 - val_loss: 0.3335 - val_acc: 0.8654\n",
      "Epoch 44/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.0536 - acc: 0.9906 - val_loss: 0.3328 - val_acc: 0.8685\n",
      "Epoch 45/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.0496 - acc: 0.9917 - val_loss: 0.3488 - val_acc: 0.8639\n",
      "Epoch 46/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.0467 - acc: 0.9924 - val_loss: 0.3370 - val_acc: 0.8664\n",
      "Epoch 47/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.0437 - acc: 0.9932 - val_loss: 0.3320 - val_acc: 0.8682\n",
      "Epoch 48/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.0413 - acc: 0.9930 - val_loss: 0.3464 - val_acc: 0.8662\n",
      "Epoch 49/60\n",
      "15663/15663 [==============================] - 21s - loss: 0.0383 - acc: 0.9943 - val_loss: 0.3393 - val_acc: 0.8680\n",
      "Epoch 50/60\n",
      "15663/15663 [==============================] - 21s - loss: 0.0359 - acc: 0.9940 - val_loss: 0.3382 - val_acc: 0.8682\n",
      "Epoch 51/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.0337 - acc: 0.9951 - val_loss: 0.3356 - val_acc: 0.8710\n",
      "Epoch 52/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.0312 - acc: 0.9957 - val_loss: 0.3384 - val_acc: 0.8703\n",
      "8032/8392 [===========================>..] - ETA: 0svalLoss: 0.3344818563961688\n",
      "3/5: #Trains: 15663, #Val: 3916 #vocab: 75122  (15663, 1024) (3916, 1024) (8392, 1024)\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/60\n",
      "15663/15663 [==============================] - 22s - loss: 1.0870 - acc: 0.4008 - val_loss: 1.0802 - val_acc: 0.4096\n",
      "Epoch 2/60\n",
      "15663/15663 [==============================] - 22s - loss: 1.0775 - acc: 0.4022 - val_loss: 1.0686 - val_acc: 0.4096\n",
      "Epoch 3/60\n",
      "15663/15663 [==============================] - 21s - loss: 1.0551 - acc: 0.4064 - val_loss: 1.0389 - val_acc: 0.4137\n",
      "Epoch 4/60\n",
      "15663/15663 [==============================] - 21s - loss: 1.0136 - acc: 0.4647 - val_loss: 0.9952 - val_acc: 0.5388\n",
      "Epoch 5/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.9561 - acc: 0.5963 - val_loss: 0.9422 - val_acc: 0.7028\n",
      "Epoch 6/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.8898 - acc: 0.7004 - val_loss: 0.8835 - val_acc: 0.6170\n",
      "Epoch 7/60\n",
      "15663/15663 [==============================] - 17s - loss: 0.8213 - acc: 0.7549 - val_loss: 0.8253 - val_acc: 0.7206\n",
      "Epoch 8/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.7551 - acc: 0.7900 - val_loss: 0.7729 - val_acc: 0.7283\n",
      "Epoch 9/60\n",
      "15663/15663 [==============================] - 21s - loss: 0.6942 - acc: 0.8102 - val_loss: 0.7279 - val_acc: 0.7750\n",
      "Epoch 10/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.6389 - acc: 0.8248 - val_loss: 0.6851 - val_acc: 0.7863\n",
      "Epoch 11/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.5889 - acc: 0.8384 - val_loss: 0.6505 - val_acc: 0.7921\n",
      "Epoch 12/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.5442 - acc: 0.8520 - val_loss: 0.6155 - val_acc: 0.8013\n",
      "Epoch 13/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.5044 - acc: 0.8638 - val_loss: 0.5860 - val_acc: 0.8049\n",
      "Epoch 14/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.4677 - acc: 0.8723 - val_loss: 0.5620 - val_acc: 0.8105\n",
      "Epoch 15/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.4344 - acc: 0.8820 - val_loss: 0.5386 - val_acc: 0.8049\n",
      "Epoch 16/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.4036 - acc: 0.8925 - val_loss: 0.5207 - val_acc: 0.8179\n",
      "Epoch 17/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.3757 - acc: 0.9021 - val_loss: 0.5011 - val_acc: 0.8281\n",
      "Epoch 18/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.3493 - acc: 0.9110 - val_loss: 0.4872 - val_acc: 0.8126\n",
      "Epoch 19/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 19s - loss: 0.3253 - acc: 0.9185 - val_loss: 0.4656 - val_acc: 0.8386\n",
      "Epoch 20/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.3028 - acc: 0.9252 - val_loss: 0.4532 - val_acc: 0.8424\n",
      "Epoch 21/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.2824 - acc: 0.9309 - val_loss: 0.4399 - val_acc: 0.8473\n",
      "Epoch 22/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.2633 - acc: 0.9375 - val_loss: 0.4354 - val_acc: 0.8378\n",
      "Epoch 23/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.2455 - acc: 0.9431 - val_loss: 0.4173 - val_acc: 0.8524\n",
      "Epoch 24/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.2291 - acc: 0.9484 - val_loss: 0.4097 - val_acc: 0.8483\n",
      "Epoch 25/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.2139 - acc: 0.9500 - val_loss: 0.3995 - val_acc: 0.8560\n",
      "Epoch 26/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.1994 - acc: 0.9563 - val_loss: 0.3924 - val_acc: 0.8593\n",
      "Epoch 27/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.1860 - acc: 0.9603 - val_loss: 0.3834 - val_acc: 0.8588\n",
      "Epoch 28/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.1739 - acc: 0.9635 - val_loss: 0.4034 - val_acc: 0.8401\n",
      "Epoch 29/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.1627 - acc: 0.9652 - val_loss: 0.3709 - val_acc: 0.8641\n",
      "Epoch 30/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.1515 - acc: 0.9692 - val_loss: 0.3714 - val_acc: 0.8598\n",
      "Epoch 31/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.1417 - acc: 0.9713 - val_loss: 0.3654 - val_acc: 0.8585\n",
      "Epoch 32/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.1326 - acc: 0.9744 - val_loss: 0.3573 - val_acc: 0.8682\n",
      "Epoch 33/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.1236 - acc: 0.9756 - val_loss: 0.3548 - val_acc: 0.8659\n",
      "Epoch 34/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.1161 - acc: 0.9774 - val_loss: 0.3507 - val_acc: 0.8685\n",
      "Epoch 35/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.1082 - acc: 0.9791 - val_loss: 0.3472 - val_acc: 0.8657\n",
      "Epoch 36/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.1016 - acc: 0.9811 - val_loss: 0.3496 - val_acc: 0.8629\n",
      "Epoch 37/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.0950 - acc: 0.9824 - val_loss: 0.3426 - val_acc: 0.8675\n",
      "Epoch 38/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.0886 - acc: 0.9853 - val_loss: 0.3376 - val_acc: 0.8685\n",
      "Epoch 39/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.0831 - acc: 0.9856 - val_loss: 0.3439 - val_acc: 0.8675\n",
      "Epoch 40/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.0777 - acc: 0.9869 - val_loss: 0.3390 - val_acc: 0.8667\n",
      "Epoch 41/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.0728 - acc: 0.9875 - val_loss: 0.3324 - val_acc: 0.8705\n",
      "Epoch 42/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.0681 - acc: 0.9882 - val_loss: 0.3342 - val_acc: 0.8682\n",
      "Epoch 43/60\n",
      "15663/15663 [==============================] - 21s - loss: 0.0635 - acc: 0.9894 - val_loss: 0.3351 - val_acc: 0.8652\n",
      "Epoch 44/60\n",
      "15663/15663 [==============================] - 22s - loss: 0.0593 - acc: 0.9911 - val_loss: 0.3320 - val_acc: 0.8667\n",
      "Epoch 45/60\n",
      "15663/15663 [==============================] - 21s - loss: 0.0558 - acc: 0.9914 - val_loss: 0.3353 - val_acc: 0.8657\n",
      "Epoch 46/60\n",
      "15663/15663 [==============================] - 22s - loss: 0.0522 - acc: 0.9915 - val_loss: 0.3374 - val_acc: 0.8672\n",
      "Epoch 47/60\n",
      "15663/15663 [==============================] - 21s - loss: 0.0491 - acc: 0.9927 - val_loss: 0.3327 - val_acc: 0.8708\n",
      "Epoch 48/60\n",
      "15663/15663 [==============================] - 21s - loss: 0.0461 - acc: 0.9930 - val_loss: 0.3338 - val_acc: 0.8682\n",
      "Epoch 49/60\n",
      "15663/15663 [==============================] - 21s - loss: 0.0431 - acc: 0.9932 - val_loss: 0.3442 - val_acc: 0.8672\n",
      "8392/8392 [==============================] - 0s     \n",
      "valLoss: 0.33364821142328127\n",
      "4/5: #Trains: 15663, #Val: 3916 #vocab: 75122  (15663, 1024) (3916, 1024) (8392, 1024)\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/60\n",
      "15663/15663 [==============================] - 21s - loss: 1.0864 - acc: 0.4015 - val_loss: 1.0847 - val_acc: 0.3994\n",
      "Epoch 2/60\n",
      "15663/15663 [==============================] - 21s - loss: 1.0736 - acc: 0.4059 - val_loss: 1.0719 - val_acc: 0.4203\n",
      "Epoch 3/60\n",
      "15663/15663 [==============================] - 21s - loss: 1.0443 - acc: 0.4497 - val_loss: 1.0374 - val_acc: 0.4673\n",
      "Epoch 4/60\n",
      "15663/15663 [==============================] - 21s - loss: 0.9940 - acc: 0.5246 - val_loss: 0.9928 - val_acc: 0.5363\n",
      "Epoch 5/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.9297 - acc: 0.5844 - val_loss: 0.9343 - val_acc: 0.5513\n",
      "Epoch 6/60\n",
      "15663/15663 [==============================] - 22s - loss: 0.8616 - acc: 0.6598 - val_loss: 0.8810 - val_acc: 0.6016\n",
      "Epoch 7/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.7937 - acc: 0.7376 - val_loss: 0.8284 - val_acc: 0.6517\n",
      "Epoch 8/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.7302 - acc: 0.7869 - val_loss: 0.7778 - val_acc: 0.7028\n",
      "Epoch 9/60\n",
      "15663/15663 [==============================] - 23s - loss: 0.6706 - acc: 0.8134 - val_loss: 0.7336 - val_acc: 0.7308\n",
      "Epoch 10/60\n",
      "15663/15663 [==============================] - 22s - loss: 0.6165 - acc: 0.8322 - val_loss: 0.7013 - val_acc: 0.7617\n",
      "Epoch 11/60\n",
      "15663/15663 [==============================] - 21s - loss: 0.5675 - acc: 0.8488 - val_loss: 0.6538 - val_acc: 0.7827\n",
      "Epoch 12/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.5230 - acc: 0.8613 - val_loss: 0.6271 - val_acc: 0.7663\n",
      "Epoch 13/60\n",
      "15663/15663 [==============================] - 23s - loss: 0.4840 - acc: 0.8721 - val_loss: 0.5985 - val_acc: 0.7760\n",
      "Epoch 14/60\n",
      "15663/15663 [==============================] - 22s - loss: 0.4477 - acc: 0.8828 - val_loss: 0.5687 - val_acc: 0.8126\n",
      "Epoch 15/60\n",
      "15663/15663 [==============================] - 21s - loss: 0.4144 - acc: 0.8926 - val_loss: 0.5441 - val_acc: 0.8108\n",
      "Epoch 16/60\n",
      "15663/15663 [==============================] - 22s - loss: 0.3841 - acc: 0.9025 - val_loss: 0.5279 - val_acc: 0.8031\n",
      "Epoch 17/60\n",
      "15663/15663 [==============================] - 21s - loss: 0.3568 - acc: 0.9095 - val_loss: 0.5089 - val_acc: 0.8169\n",
      "Epoch 18/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.3314 - acc: 0.9160 - val_loss: 0.4900 - val_acc: 0.8292\n",
      "Epoch 19/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.3080 - acc: 0.9252 - val_loss: 0.4797 - val_acc: 0.8200\n",
      "Epoch 20/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.2859 - acc: 0.9310 - val_loss: 0.4594 - val_acc: 0.8320\n",
      "Epoch 21/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.2658 - acc: 0.9362 - val_loss: 0.4471 - val_acc: 0.8363\n",
      "Epoch 22/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.2474 - acc: 0.9429 - val_loss: 0.4539 - val_acc: 0.8212\n",
      "Epoch 23/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.2297 - acc: 0.9459 - val_loss: 0.4280 - val_acc: 0.8422\n",
      "Epoch 24/60\n",
      "15663/15663 [==============================] - 17s - loss: 0.2140 - acc: 0.9509 - val_loss: 0.4126 - val_acc: 0.8478\n",
      "Epoch 25/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.1989 - acc: 0.9563 - val_loss: 0.4045 - val_acc: 0.8514\n",
      "Epoch 26/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.1851 - acc: 0.9602 - val_loss: 0.3966 - val_acc: 0.8521\n",
      "Epoch 27/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.1724 - acc: 0.9629 - val_loss: 0.3884 - val_acc: 0.8534\n",
      "Epoch 28/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.1613 - acc: 0.9662 - val_loss: 0.3835 - val_acc: 0.8544\n",
      "Epoch 29/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.1501 - acc: 0.9687 - val_loss: 0.3765 - val_acc: 0.8557\n",
      "Epoch 30/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.1391 - acc: 0.9722 - val_loss: 0.3745 - val_acc: 0.8570\n",
      "Epoch 31/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15663/15663 [==============================] - 19s - loss: 0.1291 - acc: 0.9740 - val_loss: 0.3746 - val_acc: 0.8560\n",
      "Epoch 32/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.1211 - acc: 0.9753 - val_loss: 0.3615 - val_acc: 0.8611\n",
      "Epoch 33/60\n",
      "15663/15663 [==============================] - 21s - loss: 0.1127 - acc: 0.9778 - val_loss: 0.3623 - val_acc: 0.8618\n",
      "Epoch 34/60\n",
      "15663/15663 [==============================] - 21s - loss: 0.1054 - acc: 0.9791 - val_loss: 0.3736 - val_acc: 0.8481\n",
      "Epoch 35/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.0980 - acc: 0.9809 - val_loss: 0.3533 - val_acc: 0.8647\n",
      "Epoch 36/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.0914 - acc: 0.9840 - val_loss: 0.3493 - val_acc: 0.8657\n",
      "Epoch 37/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.0861 - acc: 0.9850 - val_loss: 0.3492 - val_acc: 0.8654\n",
      "Epoch 38/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.0792 - acc: 0.9862 - val_loss: 0.3458 - val_acc: 0.8690\n",
      "Epoch 39/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.0740 - acc: 0.9878 - val_loss: 0.3533 - val_acc: 0.8613\n",
      "Epoch 40/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.0689 - acc: 0.9890 - val_loss: 0.3631 - val_acc: 0.8573\n",
      "Epoch 41/60\n",
      "15663/15663 [==============================] - 21s - loss: 0.0652 - acc: 0.9884 - val_loss: 0.3503 - val_acc: 0.8652\n",
      "Epoch 42/60\n",
      "15663/15663 [==============================] - 20s - loss: 0.0608 - acc: 0.9904 - val_loss: 0.3455 - val_acc: 0.8682\n",
      "Epoch 43/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.0563 - acc: 0.9920 - val_loss: 0.3463 - val_acc: 0.8680\n",
      "Epoch 44/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.0535 - acc: 0.9917 - val_loss: 0.3435 - val_acc: 0.8700\n",
      "Epoch 45/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.0498 - acc: 0.9920 - val_loss: 0.3446 - val_acc: 0.8695\n",
      "Epoch 46/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.0462 - acc: 0.9934 - val_loss: 0.3488 - val_acc: 0.8682\n",
      "Epoch 47/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.0436 - acc: 0.9928 - val_loss: 0.3504 - val_acc: 0.8659\n",
      "Epoch 48/60\n",
      "15663/15663 [==============================] - 18s - loss: 0.0411 - acc: 0.9937 - val_loss: 0.3503 - val_acc: 0.8708\n",
      "Epoch 49/60\n",
      "15663/15663 [==============================] - 19s - loss: 0.0385 - acc: 0.9945 - val_loss: 0.3461 - val_acc: 0.8685\n",
      "7936/8392 [===========================>..] - ETA: 0svalLoss: 0.3360987143232912\n",
      "5/5: #Trains: 15664, #Val: 3915 #vocab: 75122  (15664, 1024) (3915, 1024) (8392, 1024)\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/60\n",
      "15664/15664 [==============================] - 16s - loss: 1.0860 - acc: 0.4039 - val_loss: 1.0842 - val_acc: 0.4020\n",
      "Epoch 2/60\n",
      "15664/15664 [==============================] - 17s - loss: 1.0768 - acc: 0.4040 - val_loss: 1.0719 - val_acc: 0.4020\n",
      "Epoch 3/60\n",
      "15664/15664 [==============================] - 17s - loss: 1.0559 - acc: 0.4100 - val_loss: 1.0458 - val_acc: 0.4061\n",
      "Epoch 4/60\n",
      "15664/15664 [==============================] - 17s - loss: 1.0177 - acc: 0.4581 - val_loss: 1.0066 - val_acc: 0.4639\n",
      "Epoch 5/60\n",
      "15664/15664 [==============================] - 17s - loss: 0.9660 - acc: 0.5702 - val_loss: 0.9579 - val_acc: 0.6416\n",
      "Epoch 6/60\n",
      "15664/15664 [==============================] - 17s - loss: 0.9046 - acc: 0.6738 - val_loss: 0.9057 - val_acc: 0.7344\n",
      "Epoch 7/60\n",
      "15664/15664 [==============================] - 20s - loss: 0.8388 - acc: 0.7420 - val_loss: 0.8511 - val_acc: 0.6983\n",
      "Epoch 8/60\n",
      "15664/15664 [==============================] - 19s - loss: 0.7736 - acc: 0.7737 - val_loss: 0.7969 - val_acc: 0.7635\n",
      "Epoch 9/60\n",
      "15664/15664 [==============================] - 17s - loss: 0.7125 - acc: 0.7981 - val_loss: 0.7465 - val_acc: 0.7663\n",
      "Epoch 10/60\n",
      "15664/15664 [==============================] - 19s - loss: 0.6557 - acc: 0.8174 - val_loss: 0.7037 - val_acc: 0.7632\n",
      "Epoch 11/60\n",
      "15664/15664 [==============================] - 21s - loss: 0.6050 - acc: 0.8335 - val_loss: 0.6662 - val_acc: 0.7908\n",
      "Epoch 12/60\n",
      "15664/15664 [==============================] - 22s - loss: 0.5591 - acc: 0.8492 - val_loss: 0.6503 - val_acc: 0.7354\n",
      "Epoch 13/60\n",
      "15664/15664 [==============================] - 21s - loss: 0.5183 - acc: 0.8579 - val_loss: 0.6058 - val_acc: 0.7987\n",
      "Epoch 14/60\n",
      "15664/15664 [==============================] - 19s - loss: 0.4803 - acc: 0.8693 - val_loss: 0.5764 - val_acc: 0.8069\n",
      "Epoch 15/60\n",
      "15664/15664 [==============================] - 18s - loss: 0.4453 - acc: 0.8802 - val_loss: 0.5537 - val_acc: 0.8064\n",
      "Epoch 16/60\n",
      "15664/15664 [==============================] - 20s - loss: 0.4141 - acc: 0.8889 - val_loss: 0.5349 - val_acc: 0.8207\n",
      "Epoch 17/60\n",
      "15664/15664 [==============================] - 18s - loss: 0.3845 - acc: 0.8984 - val_loss: 0.5149 - val_acc: 0.8197\n",
      "Epoch 18/60\n",
      "15664/15664 [==============================] - 18s - loss: 0.3575 - acc: 0.9082 - val_loss: 0.5044 - val_acc: 0.8120\n",
      "Epoch 19/60\n",
      "15664/15664 [==============================] - 20s - loss: 0.3330 - acc: 0.9142 - val_loss: 0.4878 - val_acc: 0.8286\n",
      "Epoch 20/60\n",
      "15664/15664 [==============================] - 20s - loss: 0.3100 - acc: 0.9217 - val_loss: 0.4728 - val_acc: 0.8314\n",
      "Epoch 21/60\n",
      "15664/15664 [==============================] - 20s - loss: 0.2887 - acc: 0.9274 - val_loss: 0.4546 - val_acc: 0.8373\n",
      "Epoch 22/60\n",
      "15664/15664 [==============================] - 20s - loss: 0.2685 - acc: 0.9345 - val_loss: 0.4402 - val_acc: 0.8409\n",
      "Epoch 23/60\n",
      "15664/15664 [==============================] - 20s - loss: 0.2505 - acc: 0.9402 - val_loss: 0.4291 - val_acc: 0.8429\n",
      "Epoch 24/60\n",
      "15664/15664 [==============================] - 20s - loss: 0.2331 - acc: 0.9458 - val_loss: 0.4270 - val_acc: 0.8432\n",
      "Epoch 25/60\n",
      "15664/15664 [==============================] - 20s - loss: 0.2184 - acc: 0.9497 - val_loss: 0.4109 - val_acc: 0.8455\n",
      "Epoch 26/60\n",
      "15664/15664 [==============================] - 20s - loss: 0.2033 - acc: 0.9540 - val_loss: 0.4096 - val_acc: 0.8398\n",
      "Epoch 27/60\n",
      "15664/15664 [==============================] - 20s - loss: 0.1892 - acc: 0.9591 - val_loss: 0.3941 - val_acc: 0.8534\n",
      "Epoch 28/60\n",
      "15664/15664 [==============================] - 20s - loss: 0.1770 - acc: 0.9617 - val_loss: 0.3910 - val_acc: 0.8475\n",
      "Epoch 29/60\n",
      "15664/15664 [==============================] - 20s - loss: 0.1652 - acc: 0.9636 - val_loss: 0.3836 - val_acc: 0.8536\n",
      "Epoch 30/60\n",
      "15664/15664 [==============================] - 21s - loss: 0.1541 - acc: 0.9680 - val_loss: 0.3791 - val_acc: 0.8547\n",
      "Epoch 31/60\n",
      "15664/15664 [==============================] - 21s - loss: 0.1438 - acc: 0.9704 - val_loss: 0.3727 - val_acc: 0.8585\n",
      "Epoch 32/60\n",
      "15664/15664 [==============================] - 21s - loss: 0.1347 - acc: 0.9731 - val_loss: 0.3724 - val_acc: 0.8585\n",
      "Epoch 33/60\n",
      "15664/15664 [==============================] - 21s - loss: 0.1258 - acc: 0.9754 - val_loss: 0.3751 - val_acc: 0.8534\n",
      "Epoch 34/60\n",
      "15664/15664 [==============================] - 16s - loss: 0.1175 - acc: 0.9770 - val_loss: 0.3685 - val_acc: 0.8603\n",
      "Epoch 35/60\n",
      "15664/15664 [==============================] - 18s - loss: 0.1092 - acc: 0.9793 - val_loss: 0.3689 - val_acc: 0.8564\n",
      "Epoch 36/60\n",
      "15664/15664 [==============================] - 19s - loss: 0.1023 - acc: 0.9805 - val_loss: 0.3557 - val_acc: 0.8621\n",
      "Epoch 37/60\n",
      "15664/15664 [==============================] - 19s - loss: 0.0959 - acc: 0.9824 - val_loss: 0.3505 - val_acc: 0.8623\n",
      "Epoch 38/60\n",
      "15664/15664 [==============================] - 17s - loss: 0.0894 - acc: 0.9840 - val_loss: 0.3555 - val_acc: 0.8608\n",
      "Epoch 39/60\n",
      "15664/15664 [==============================] - 16s - loss: 0.0838 - acc: 0.9844 - val_loss: 0.3468 - val_acc: 0.8664\n",
      "Epoch 40/60\n",
      "15664/15664 [==============================] - 19s - loss: 0.0788 - acc: 0.9858 - val_loss: 0.3555 - val_acc: 0.8626\n",
      "Epoch 41/60\n",
      "15664/15664 [==============================] - 18s - loss: 0.0740 - acc: 0.9871 - val_loss: 0.3547 - val_acc: 0.8616\n",
      "Epoch 42/60\n",
      "15664/15664 [==============================] - 20s - loss: 0.0691 - acc: 0.9884 - val_loss: 0.3547 - val_acc: 0.8616\n",
      "Epoch 43/60\n",
      "15664/15664 [==============================] - 20s - loss: 0.0650 - acc: 0.9891 - val_loss: 0.3580 - val_acc: 0.8590\n",
      "Epoch 44/60\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15664/15664 [==============================] - 18s - loss: 0.0610 - acc: 0.9900 - val_loss: 0.3484 - val_acc: 0.8649\n",
      "7968/8392 [===========================>..] - ETA: 0svalLoss: 0.33823418763046964\n"
     ]
    }
   ],
   "source": [
    "n_gram_max = 2\n",
    "embedding_dims = 10\n",
    "maxlen = 1024\n",
    "\n",
    "raw_docs = create_docs(df, n_gram_max=n_gram_max)\n",
    "raw_docs_test = create_docs(df_test, n_gram_max=n_gram_max)\n",
    "\n",
    "seed = 7\n",
    "num_split = 5\n",
    "epochs = 60\n",
    "\n",
    "# for next training\n",
    "predict_prob_features = np.zeros((len(df), 3))\n",
    "predict_prob_features_test = np.zeros((len(df_test), 3))\n",
    "\n",
    "ite = 0\n",
    "sum_loss = 0.\n",
    "min_count = 2\n",
    "\n",
    "kf = KFold(n_splits=num_split, random_state=seed, shuffle=True)\n",
    "for train_index, val_index in kf.split(text):\n",
    "    ite += 1\n",
    "    print('{}/{}: #Trains: {}, #Val: {}'.format(ite, num_split, len(train_index), len(val_index)), end=' ')\n",
    "    \n",
    "    docs_train = [raw_docs[i] for i in train_index]\n",
    "    docs_val = [raw_docs[i] for i in val_index]\n",
    "\n",
    "    # get vocab\n",
    "    tokenizer = Tokenizer(filters='', lower=True)\n",
    "    #     tokenizer.fit_on_texts(docs_train)\n",
    "    tokenizer.fit_on_texts(docs_train + docs_val)\n",
    "\n",
    "    num_words = sum([1 for _, v in tokenizer.word_counts.items() if v >= min_count])\n",
    "    \n",
    "    tokenizer = Tokenizer(num_words=num_words, filters='', lower=True)\n",
    "    tokenizer.fit_on_texts(docs_train + docs_val)    \n",
    "\n",
    "    docs_train = tokenizer.texts_to_sequences(docs_train)\n",
    "    docs_val   = tokenizer.texts_to_sequences(docs_val)\n",
    "    docs_test  = tokenizer.texts_to_sequences(raw_docs_test)\n",
    "\n",
    "    x_train = pad_sequences(sequences=docs_train, maxlen=maxlen)\n",
    "    x_val  = pad_sequences(sequences=docs_val, maxlen=maxlen)\n",
    "    x_test = pad_sequences(sequences=docs_test, maxlen=maxlen)\n",
    "\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    input_dim = max(np.max(x_train), np.max(x_val)) + 1\n",
    "    print('#vocab: {} '.format(num_words), end=' ')\n",
    "    print(x_train.shape, x_val.shape, x_test.shape)\n",
    "\n",
    "    model = create_model(input_dim)\n",
    "    \n",
    "    checkpointer = ModelCheckpoint(filepath='./../fasttext_weights/weights.hdf5', verbose=0, save_best_only=True)\n",
    "\n",
    "    hist = model.fit(x_train, y_train,\n",
    "                     batch_size=16,\n",
    "                     validation_data=(x_val, y_val),\n",
    "                     epochs=epochs,\n",
    "                     callbacks=[EarlyStopping(patience=4, monitor='val_loss'), \n",
    "                                checkpointer])\n",
    "\n",
    "    # load best weights\n",
    "    model.load_weights('./../fasttext_weights/weights.hdf5')\n",
    "    y_pred = model.predict_proba(x_val)\n",
    "    sum_loss += log_loss(y_pred=y_pred, y_true=np.nonzero(y_val)[1])\n",
    "    \n",
    "    # save features\n",
    "    predict_prob_features[val_index] = y_pred\n",
    "    predict_prob_features_test += model.predict_proba(x_test)\n",
    "    print('valLoss: {}'.format(sum_loss/ite))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "name = 'fasttext_bigram_reverse'\n",
    "for a, c in author2class.items():\n",
    "    df['{}_{}'.format(a, name)] = predict_prob_features[:, c]\n",
    "    df_test['{}_{}'.format(a, name)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('./../data/train_feature.csv')\n",
    "df_test.to_csv('./../data/test_feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
