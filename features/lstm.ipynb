{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "93e00783-a024-4e87-a5e1-6709cb8cc981",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "b05ef71268db76a4e2565177bf6a5668a5fc428e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from sklearn.metrics import log_loss\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import KFold\n",
    "np.random.seed(129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "a5cc2c3e-7960-482e-b548-c447b89925ec",
    "_uuid": "d700f739101e37903112e1de293323dcfbb577be",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv('./../data/train_feature.csv')\n",
    "df_test = pd.read_csv('./../data/test_feature.csv')\n",
    "text = df.text.values\n",
    "text_test = df_test.text.values\n",
    "\n",
    "author2class = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "class2author = ['EAP', 'HPL', 'MWS']\n",
    "y = np.array([author2class[a] for a in df.author])\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "37faa360-bb1f-40a1-a07e-b8f808f31a76",
    "_uuid": "450f3d1fc9d5ca4e4a62fbc0e7958d0e7b0ac140",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import re\n",
    "def preprocess(text):\n",
    "    text = text.replace('\"', ' \" ')    \n",
    "    text = re.sub(r\"(')(\\s|$)\", r\" \\1 \", text)\n",
    "    text = re.sub(r\"(^|\\s)(')\", r\" \\2 \", text)\n",
    "\n",
    "    for sign in ';:,': #?\n",
    "        text = re.sub(r'(\\s|^)({})'.format(sign), r' \\2 ', text)\n",
    "        text = re.sub(r'({})($|\\s)'.format(sign), r' \\1 ', text)\n",
    "\n",
    "    text = re.sub(r'(\\.+)(\\s|$)', r' \\1 ', text)\n",
    "\n",
    "    text = re.sub(r\"(')(\\s|$)\", r\" \\1 \", text) # special case: 'hoge'. \n",
    "    \n",
    "    text = re.sub(r\"(\\?)(\\s|$)\", r' \\1 ', text)\n",
    "    text = re.sub(r\"(^|\\s)(\\?+)\", r' \\2 ', text)    \n",
    "    \n",
    "    return text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "f123742f-540f-438d-aba3-ebbca69235be",
    "_uuid": "53f325a090a44f7109f0537022398797704cdc80",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_docs(df):\n",
    "    docs = []\n",
    "    for doc in df.text:\n",
    "        doc = preprocess(doc).lower().split()\n",
    "        docs.append(' '.join(doc).split())\n",
    "    \n",
    "    return docs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "393d1ddb-0a87-42a3-8575-53ff7abff1da",
    "_uuid": "bba1d1a6416876e74ed688f56e4d5bc4990ec12a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "min_count = 2\n",
    "maxlen = 64\n",
    "embedding_dims = 32"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "_cell_guid": "2e3e1e3e-22f4-4727-ba6c-67f7b3e80d2f",
    "_uuid": "e6c16572e6b32923af39dfd29467e32b52561bb1"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start 594056\n",
      "#vocab: 16012\n",
      "563239\n",
      "#vocab: 15691\n",
      "562941\n",
      "#vocab: 15691\n",
      "562941\n"
     ]
    }
   ],
   "source": [
    "docs = raw_docs = create_docs(df)\n",
    "\n",
    "prev_sum_words = np.sum(np.array([len(d) for d in raw_docs]))\n",
    "prev_sum_words\n",
    "print('start {}'.format(prev_sum_words))\n",
    "preprocessed_sum_words = 0\n",
    "\n",
    "while preprocessed_sum_words != prev_sum_words:\n",
    "    prev_sum_words = preprocessed_sum_words\n",
    "    freq = defaultdict(int)\n",
    "    for doc in docs:\n",
    "        for w in doc:\n",
    "            freq[w] += 1\n",
    "    num_vocab = len(freq)\n",
    "\n",
    "    for w, c in freq.copy().items():\n",
    "        if c < min_count:\n",
    "            del freq[w]\n",
    "    print('#vocab: {}'.format(len(freq)))\n",
    "\n",
    "    new_docs = []\n",
    "    for doc in docs:\n",
    "        new_doc = []\n",
    "        for w in doc:\n",
    "            if w in freq:\n",
    "                new_doc.append(w)\n",
    "        new_docs.append(new_doc[:maxlen])\n",
    "    docs = new_docs\n",
    "    preprocessed_sum_words = np.sum(np.array([len(d) for d in docs]))\n",
    "    print(preprocessed_sum_words)\n",
    "\n",
    "word2int = {}\n",
    "int_docs = []\n",
    "for doc in docs:\n",
    "    int_doc = []\n",
    "    for w in doc:\n",
    "        if w not in word2int:\n",
    "            wid = len(word2int) + 1\n",
    "            word2int[w] = wid\n",
    "        else:\n",
    "            wid = word2int[w]\n",
    "        int_doc.append(wid)\n",
    "    int_docs.append(int_doc)\n",
    "\n",
    "docs = pad_sequences(int_docs)\n",
    "input_dim = np.max(docs) + 1\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "docs_test = create_docs(df_test)\n",
    "\n",
    "new_docs = []\n",
    "for doc in docs_test:\n",
    "    new_doc = []\n",
    "    for w in doc:\n",
    "        if w in freq:\n",
    "            new_doc.append(w)\n",
    "    new_docs.append(new_doc[:maxlen])\n",
    "docs_test = new_docs\n",
    "\n",
    "\n",
    "int_docs = []\n",
    "for doc in docs_test:\n",
    "    int_doc = []\n",
    "    for w in doc:\n",
    "        if w in word2int:\n",
    "            wid = word2int[w]\n",
    "            int_doc.append(wid)\n",
    "    int_docs.append(int_doc)\n",
    "\n",
    "x_test = pad_sequences(int_docs)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model():\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dims, mask_zero=True))\n",
    "    model.add(Bidirectional(LSTM(embedding_dims), 'sum'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "0db889db-0b3e-4025-8847-e3eb5f853f37",
    "_uuid": "22e57e010206a3044adf7b82160c7c3ca78030f8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/45\n",
      "15663/15663 [==============================] - 105s - loss: 0.7274 - acc: 0.6838 - val_loss: 0.5143 - val_acc: 0.7952\n",
      "Epoch 2/45\n",
      "15663/15663 [==============================] - 99s - loss: 0.3830 - acc: 0.8512 - val_loss: 0.4321 - val_acc: 0.8276\n",
      "Epoch 3/45\n",
      "15663/15663 [==============================] - 97s - loss: 0.2813 - acc: 0.8931 - val_loss: 0.4211 - val_acc: 0.8378\n",
      "Epoch 4/45\n",
      "15663/15663 [==============================] - 97s - loss: 0.2312 - acc: 0.9120 - val_loss: 0.4309 - val_acc: 0.8366\n",
      "Epoch 5/45\n",
      "15663/15663 [==============================] - 98s - loss: 0.1996 - acc: 0.9268 - val_loss: 0.4117 - val_acc: 0.8468\n",
      "Epoch 6/45\n",
      "15663/15663 [==============================] - 97s - loss: 0.1759 - acc: 0.9364 - val_loss: 0.4905 - val_acc: 0.8320\n",
      "Epoch 7/45\n",
      "15663/15663 [==============================] - 98s - loss: 0.1604 - acc: 0.9407 - val_loss: 0.4500 - val_acc: 0.8432\n",
      "3916/3916 [==============================] - 4s     \n",
      "8384/8392 [============================>.] - ETA: 0svalLoss: 0.4117004111240784\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/45\n",
      "15663/15663 [==============================] - 99s - loss: 0.7323 - acc: 0.6764 - val_loss: 0.5031 - val_acc: 0.8072\n",
      "Epoch 2/45\n",
      "15663/15663 [==============================] - 98s - loss: 0.3794 - acc: 0.8563 - val_loss: 0.4128 - val_acc: 0.8371\n",
      "Epoch 3/45\n",
      "15663/15663 [==============================] - 102s - loss: 0.2813 - acc: 0.8935 - val_loss: 0.3937 - val_acc: 0.8453\n",
      "Epoch 4/45\n",
      "15663/15663 [==============================] - 105s - loss: 0.2309 - acc: 0.9134 - val_loss: 0.4065 - val_acc: 0.8430\n",
      "Epoch 5/45\n",
      "15663/15663 [==============================] - 97s - loss: 0.1991 - acc: 0.9265 - val_loss: 0.4111 - val_acc: 0.8422\n",
      "8392/8392 [==============================] - 9s     \n",
      "valLoss: 0.4027153504348068\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/45\n",
      "15663/15663 [==============================] - 99s - loss: 0.7244 - acc: 0.6854 - val_loss: 0.5378 - val_acc: 0.7909\n",
      "Epoch 2/45\n",
      "15663/15663 [==============================] - 99s - loss: 0.3702 - acc: 0.8573 - val_loss: 0.4468 - val_acc: 0.8235\n",
      "Epoch 3/45\n",
      "15663/15663 [==============================] - 101s - loss: 0.2761 - acc: 0.8959 - val_loss: 0.4494 - val_acc: 0.8225\n",
      "Epoch 4/45\n",
      "15663/15663 [==============================] - 98s - loss: 0.2304 - acc: 0.9130 - val_loss: 0.4433 - val_acc: 0.8350\n",
      "Epoch 5/45\n",
      "15663/15663 [==============================] - 100s - loss: 0.2013 - acc: 0.9239 - val_loss: 0.4323 - val_acc: 0.8424\n",
      "Epoch 6/45\n",
      "15663/15663 [==============================] - 99s - loss: 0.1818 - acc: 0.9323 - val_loss: 0.4798 - val_acc: 0.8276 - ETA: 4s - loss\n",
      "Epoch 7/45\n",
      "15663/15663 [==============================] - 102s - loss: 0.1621 - acc: 0.9387 - val_loss: 0.4551 - val_acc: 0.8401\n",
      "3916/3916 [==============================] - 4s     \n",
      "8384/8392 [============================>.] - ETA: 0svalLoss: 0.41256881329303\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/45\n",
      "15663/15663 [==============================] - 102s - loss: 0.7428 - acc: 0.6718 - val_loss: 0.5632 - val_acc: 0.7722\n",
      "Epoch 2/45\n",
      "15663/15663 [==============================] - 101s - loss: 0.3732 - acc: 0.8578 - val_loss: 0.4429 - val_acc: 0.8241\n",
      "Epoch 3/45\n",
      "15663/15663 [==============================] - 100s - loss: 0.2738 - acc: 0.8949 - val_loss: 0.4266 - val_acc: 0.8309\n",
      "Epoch 4/45\n",
      "15663/15663 [==============================] - 101s - loss: 0.2262 - acc: 0.9145 - val_loss: 0.4083 - val_acc: 0.8419\n",
      "Epoch 5/45\n",
      "15663/15663 [==============================] - 101s - loss: 0.1928 - acc: 0.9279 - val_loss: 0.4631 - val_acc: 0.8368\n",
      "Epoch 6/45\n",
      "15663/15663 [==============================] - 100s - loss: 0.1746 - acc: 0.9337 - val_loss: 0.4514 - val_acc: 0.8363\n",
      "3916/3916 [==============================] - 5s     \n",
      "8392/8392 [==============================] - 10s    \n",
      "valLoss: 0.41149616068232847\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/45\n",
      "15664/15664 [==============================] - 106s - loss: 0.7452 - acc: 0.6663 - val_loss: 0.5174 - val_acc: 0.7987\n",
      "Epoch 2/45\n",
      "15664/15664 [==============================] - 118s - loss: 0.3806 - acc: 0.8530 - val_loss: 0.3981 - val_acc: 0.8434\n",
      "Epoch 3/45\n",
      "15664/15664 [==============================] - 113s - loss: 0.2797 - acc: 0.8929 - val_loss: 0.3740 - val_acc: 0.8531\n",
      "Epoch 4/45\n",
      "15664/15664 [==============================] - 122s - loss: 0.2304 - acc: 0.9120 - val_loss: 0.3984 - val_acc: 0.8496\n",
      "Epoch 5/45\n",
      "15664/15664 [==============================] - 111s - loss: 0.1992 - acc: 0.9251 - val_loss: 0.3974 - val_acc: 0.8450\n",
      "8384/8392 [============================>.] - ETA: 0svalLoss: 0.4040003191039133\n"
     ]
    }
   ],
   "source": [
    "epochs = 45\n",
    "num_split = 5\n",
    "sum_loss = 0.\n",
    "\n",
    "predict_prob_features = np.zeros((len(df), 3))\n",
    "predict_prob_features_test = np.zeros((len(df_test), 3))\n",
    "ite = 0\n",
    "kf = KFold(n_splits=num_split, random_state=8, shuffle=True)\n",
    "for train_index, val_index in kf.split(text):\n",
    "    ite += 1\n",
    "    x_train, x_val = docs[train_index], docs[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    model = create_model()\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath='./../fasttext_weights/lstm.hdf5', verbose=0, save_best_only=True)\n",
    "\n",
    "    hist = model.fit(x_train, y_train,\n",
    "                     batch_size=32,\n",
    "                     validation_data=(x_val, y_val),\n",
    "                     epochs=epochs,\n",
    "                     callbacks=[EarlyStopping(patience=1, monitor='val_loss'), checkpointer])\n",
    "    \n",
    "    model.load_weights('./../fasttext_weights/lstm.hdf5')\n",
    "    y_pred = model.predict_proba(x_val)\n",
    "    sum_loss += log_loss(y_pred=y_pred, y_true=np.nonzero(y_val)[1])\n",
    "    \n",
    "    # save features\n",
    "    predict_prob_features[val_index] = y_pred\n",
    "    predict_prob_features_test += model.predict_proba(x_test)\n",
    "    print('valLoss: {}'.format(sum_loss/ite))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "33752579-4f19-415f-a255-a5da236e1b51",
    "_uuid": "b808ddf96fc3c7c4ab96ad32d48101b095b23c41",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "for a, c in author2class.items():\n",
    "    df['{}_lstm'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_lstm'.format(a)] = predict_prob_features_test[:, c]/num_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "409b8663-bbf5-4757-99c1-40010264de04",
    "_uuid": "eaf5b87353d9a1e7367def64b453555c23d24e7a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('./../data/train_feature.csv')\n",
    "df_test.to_csv('./../data/test_feature.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
