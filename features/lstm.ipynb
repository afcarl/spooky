{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "93e00783-a024-4e87-a5e1-6709cb8cc981",
    "_kg_hide-input": false,
    "_kg_hide-output": true,
    "_uuid": "b05ef71268db76a4e2565177bf6a5668a5fc428e"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from collections import defaultdict\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import preprocess, Tokenizer4keras, create_fastText_model\n",
    "\n",
    "import keras\n",
    "from keras.layers import Dense, Embedding, LSTM, Bidirectional\n",
    "import keras.backend as K\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from keras.models import Sequential\n",
    "from keras.optimizers import RMSprop\n",
    "from sklearn.metrics import log_loss\n",
    "from keras.utils import to_categorical\n",
    "from sklearn.model_selection import KFold\n",
    "np.random.seed(129)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "a5cc2c3e-7960-482e-b548-c447b89925ec",
    "_uuid": "d700f739101e37903112e1de293323dcfbb577be"
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('./../data/train_feature.csv')\n",
    "df_test = pd.read_csv('./../data/test_feature.csv')\n",
    "df_train_texts = df_train.text.values\n",
    "df_test_texts = df_test.text.values\n",
    "\n",
    "author2class = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "class2author = ['EAP', 'HPL', 'MWS']\n",
    "y = np.array([author2class[a] for a in df_train.author])\n",
    "y = to_categorical(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "f123742f-540f-438d-aba3-ebbca69235be",
    "_uuid": "53f325a090a44f7109f0537022398797704cdc80"
   },
   "outputs": [],
   "source": [
    "min_count = 2\n",
    "maxlen = 64\n",
    "embedding_dim = 32\n",
    "reccurent_dim = 32\n",
    "\n",
    "tokenizer = Tokenizer4keras(maxlen=maxlen, min_count=min_count, n_gram_max=1, lower=True, single=False, add_ngram_first=True)\n",
    "x = tokenizer.fit_transform(df_texts=df_train_texts)\n",
    "x_test = tokenizer.transofrm(df_test_texts)\n",
    "input_dim = np.max(x) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_model(input_dim, embedding_dim, reccurent_dim):\n",
    "    model = Sequential()\n",
    "    model.add(Embedding(input_dim=input_dim, output_dim=embedding_dim, mask_zero=True))\n",
    "    model.add(Bidirectional(LSTM(embedding_dim), 'sum'))\n",
    "    model.add(Dense(3, activation='softmax'))\n",
    "\n",
    "    model.compile(loss='categorical_crossentropy',\n",
    "                  optimizer='rmsprop',\n",
    "                  metrics=['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "_cell_guid": "0db889db-0b3e-4025-8847-e3eb5f853f37",
    "_uuid": "22e57e010206a3044adf7b82160c7c3ca78030f8",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/45\n",
      "15663/15663 [==============================] - 124s - loss: 0.7273 - acc: 0.6840 - val_loss: 0.5139 - val_acc: 0.7947\n",
      "Epoch 2/45\n",
      "15663/15663 [==============================] - 123s - loss: 0.3829 - acc: 0.8509 - val_loss: 0.4316 - val_acc: 0.8281\n",
      "Epoch 3/45\n",
      "15663/15663 [==============================] - 131s - loss: 0.2812 - acc: 0.8926 - val_loss: 0.4200 - val_acc: 0.8386\n",
      "Epoch 4/45\n",
      "15663/15663 [==============================] - 134s - loss: 0.2312 - acc: 0.9122 - val_loss: 0.4296 - val_acc: 0.8355\n",
      "Epoch 5/45\n",
      "15663/15663 [==============================] - 128s - loss: 0.1996 - acc: 0.9270 - val_loss: 0.4114 - val_acc: 0.8475\n",
      "Epoch 6/45\n",
      "15663/15663 [==============================] - 126s - loss: 0.1760 - acc: 0.9366 - val_loss: 0.4898 - val_acc: 0.8320\n",
      "Epoch 7/45\n",
      "15663/15663 [==============================] - 137s - loss: 0.1606 - acc: 0.9408 - val_loss: 0.4501 - val_acc: 0.8435\n",
      "3916/3916 [==============================] - 6s     \n",
      "8392/8392 [==============================] - 14s    \n",
      "valLoss: 0.4113873973604394\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/45\n",
      "15663/15663 [==============================] - 132s - loss: 0.7323 - acc: 0.6768 - val_loss: 0.5032 - val_acc: 0.8077\n",
      "Epoch 2/45\n",
      "15663/15663 [==============================] - 124s - loss: 0.3795 - acc: 0.8562 - val_loss: 0.4127 - val_acc: 0.8376\n",
      "Epoch 3/45\n",
      "15663/15663 [==============================] - 123s - loss: 0.2813 - acc: 0.8938 - val_loss: 0.3935 - val_acc: 0.8455\n",
      "Epoch 4/45\n",
      "15663/15663 [==============================] - 124s - loss: 0.2309 - acc: 0.9134 - val_loss: 0.4065 - val_acc: 0.8430\n",
      "Epoch 5/45\n",
      "15663/15663 [==============================] - 125s - loss: 0.1991 - acc: 0.9265 - val_loss: 0.4112 - val_acc: 0.8422\n",
      "8384/8392 [============================>.] - ETA: 0svalLoss: 0.40243492876580655\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/45\n",
      "15663/15663 [==============================] - 133s - loss: 0.7235 - acc: 0.6839 - val_loss: 0.5371 - val_acc: 0.7921\n",
      "Epoch 2/45\n",
      "15663/15663 [==============================] - 124s - loss: 0.3702 - acc: 0.8571 - val_loss: 0.4462 - val_acc: 0.8248\n",
      "Epoch 3/45\n",
      "15663/15663 [==============================] - 127s - loss: 0.2762 - acc: 0.8963 - val_loss: 0.4499 - val_acc: 0.8220\n",
      "Epoch 4/45\n",
      "15663/15663 [==============================] - 129s - loss: 0.2306 - acc: 0.9131 - val_loss: 0.4429 - val_acc: 0.8373\n",
      "Epoch 5/45\n",
      "15663/15663 [==============================] - 125s - loss: 0.2012 - acc: 0.9233 - val_loss: 0.4336 - val_acc: 0.8422\n",
      "Epoch 6/45\n",
      "15663/15663 [==============================] - 126s - loss: 0.1819 - acc: 0.9323 - val_loss: 0.4754 - val_acc: 0.8256\n",
      "Epoch 7/45\n",
      "15663/15663 [==============================] - 125s - loss: 0.1624 - acc: 0.9388 - val_loss: 0.4511 - val_acc: 0.8435\n",
      "8384/8392 [============================>.] - ETA: 0svalLoss: 0.4128262740729764\n",
      "Train on 15663 samples, validate on 3916 samples\n",
      "Epoch 1/45\n",
      "15663/15663 [==============================] - 137s - loss: 0.7430 - acc: 0.6708 - val_loss: 0.5555 - val_acc: 0.7763\n",
      "Epoch 2/45\n",
      "15663/15663 [==============================] - 125s - loss: 0.3749 - acc: 0.8571 - val_loss: 0.4409 - val_acc: 0.8228\n",
      "Epoch 3/45\n",
      "15663/15663 [==============================] - 127s - loss: 0.2743 - acc: 0.8949 - val_loss: 0.4244 - val_acc: 0.8299\n",
      "Epoch 4/45\n",
      "15663/15663 [==============================] - 124s - loss: 0.2262 - acc: 0.9144 - val_loss: 0.4070 - val_acc: 0.8414\n",
      "Epoch 5/45\n",
      "15663/15663 [==============================] - 124s - loss: 0.1925 - acc: 0.9284 - val_loss: 0.4633 - val_acc: 0.8381\n",
      "Epoch 6/45\n",
      "15663/15663 [==============================] - 129s - loss: 0.1746 - acc: 0.9334 - val_loss: 0.4547 - val_acc: 0.8330\n",
      "8392/8392 [==============================] - 13s    \n",
      "valLoss: 0.41138072294795364\n",
      "Train on 15664 samples, validate on 3915 samples\n",
      "Epoch 1/45\n",
      "15664/15664 [==============================] - 137s - loss: 0.7449 - acc: 0.6664 - val_loss: 0.5155 - val_acc: 0.8000\n",
      "Epoch 2/45\n",
      "15664/15664 [==============================] - 159s - loss: 0.3803 - acc: 0.8529 - val_loss: 0.3984 - val_acc: 0.8419\n",
      "Epoch 3/45\n",
      "15664/15664 [==============================] - 158s - loss: 0.2796 - acc: 0.8932 - val_loss: 0.3738 - val_acc: 0.8516\n",
      "Epoch 4/45\n",
      "15664/15664 [==============================] - 166s - loss: 0.2304 - acc: 0.9117 - val_loss: 0.3988 - val_acc: 0.8496\n",
      "Epoch 5/45\n",
      "15664/15664 [==============================] - 154s - loss: 0.1992 - acc: 0.9247 - val_loss: 0.3973 - val_acc: 0.8450\n",
      "8384/8392 [============================>.] - ETA: 0svalLoss: 0.4038599571292874\n"
     ]
    }
   ],
   "source": [
    "epochs = 45\n",
    "num_split = 5\n",
    "sum_loss = 0.\n",
    "\n",
    "predict_prob_features = np.zeros((len(x), 3))\n",
    "predict_prob_features_test = np.zeros((len(x_test), 3))\n",
    "ite = 0\n",
    "kf = KFold(n_splits=num_split, random_state=8, shuffle=True)\n",
    "for train_index, val_index in kf.split(x):\n",
    "    ite += 1\n",
    "    x_train, x_val = x[train_index], x[val_index]\n",
    "    y_train, y_val = y[train_index], y[val_index]\n",
    "\n",
    "    model = create_model(input_dim, embedding_dim, reccurent_dim)\n",
    "\n",
    "    checkpointer = ModelCheckpoint(filepath='./../fasttext_weights/lstm.hdf5', verbose=0, save_best_only=True)\n",
    "\n",
    "    hist = model.fit(x_train, y_train,\n",
    "                     batch_size=32,\n",
    "                     validation_data=(x_val, y_val),\n",
    "                     epochs=epochs,\n",
    "                     callbacks=[EarlyStopping(patience=1, monitor='val_loss'), checkpointer])\n",
    "    \n",
    "    model.load_weights('./../fasttext_weights/lstm.hdf5')\n",
    "    y_pred = model.predict_proba(x_val)\n",
    "    sum_loss += log_loss(y_pred=y_pred, y_true=np.nonzero(y_val)[1])\n",
    "    \n",
    "    # save features\n",
    "    predict_prob_features[val_index] = y_pred\n",
    "    predict_prob_features_test += model.predict_proba(x_test)\n",
    "    print('valLoss: {}'.format(sum_loss/ite))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "_cell_guid": "33752579-4f19-415f-a255-a5da236e1b51",
    "_uuid": "b808ddf96fc3c7c4ab96ad32d48101b095b23c41"
   },
   "outputs": [],
   "source": [
    "for a, c in author2class.items():\n",
    "    df_train['{}_lstm'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_lstm'.format(a)] = predict_prob_features_test[:, c]/num_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "409b8663-bbf5-4757-99c1-40010264de04",
    "_uuid": "eaf5b87353d9a1e7367def64b453555c23d24e7a",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df_train.to_csv('./../data/train_feature.csv', index=False)\n",
    "df_test.to_csv('./../data/test_feature.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
