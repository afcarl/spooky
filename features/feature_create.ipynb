{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import sys\n",
    "sys.path.append('../')\n",
    "from utils import preprocess, create_vector, logistic, vectorizer2NB\n",
    "\n",
    "from collections import defaultdict\n",
    "import string\n",
    "\n",
    "import sklearn\n",
    "from sklearn.metrics import log_loss\n",
    "from sklearn.model_selection import train_test_split, KFold, GridSearchCV\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer, CountVectorizer\n",
    "\n",
    "from gensim.models import word2vec\n",
    "\n",
    "np.random.seed(1234)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "num_split = 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "data_path = '../data/'\n",
    "df = pd.read_csv(data_path + 'train.csv')\n",
    "df_test = pd.read_csv(data_path + 'test.csv')\n",
    "text = df.text.values\n",
    "text_test = df_test.text.values\n",
    "\n",
    "author2class = {'EAP': 0, 'HPL' : 1, 'MWS' : 2}\n",
    "class2author = ['EAP', 'HPL', 'MWS']\n",
    "y = np.array([author2class[a] for a in df.author])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.781053627128\n",
      "cbow100_min1_neg15_ws20_epoch7.vec\n",
      "0.485307596257\n",
      "skip100_min1_neg15_ws20_epoch7.vec\n",
      "0.836930458733\n",
      "cbow100_min1_neg15_ws5_epoch7.vec\n",
      "0.590803953107\n",
      "skip100_min1_neg15_ws5_epoch7.vec\n"
     ]
    }
   ],
   "source": [
    "fnames = ['cbow100_min1_neg15_ws20_epoch7.vec', 'skip100_min1_neg15_ws20_epoch7.vec', 'cbow100_min1_neg15_ws5_epoch7.vec', 'skip100_min1_neg15_ws5_epoch7.vec']\n",
    "for i, fname in enumerate(fnames):\n",
    "    vec = word2vec.KeyedVectors.load_word2vec_format('./../fastText/' + fname)\n",
    "    x, x_test = create_vector(text, text_test, vec, preprocess_single=True)\n",
    "    predict_prob_features, predict_prob_features_test = logistic(x, y, x_test, 7+i)\n",
    "    print(fname)\n",
    "    for a, c in author2class.items():\n",
    "        df['{}_{}_logi'.format(a, fname)] = predict_prob_features[:, c]\n",
    "        df_test['{}_{}_logi'.format(a, fname)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Naive Bayes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': [0.008]} TfidfVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "1/5: #Trains: 15663, #Val: 3916 valLoss: 0.4015830474795028, best_param α= 0.008\n",
      "2/5: #Trains: 15663, #Val: 3916 valLoss: 0.3821852810736139, best_param α= 0.008\n",
      "3/5: #Trains: 15663, #Val: 3916 valLoss: 0.3911772365943706, best_param α= 0.008\n",
      "4/5: #Trains: 15663, #Val: 3916 valLoss: 0.40242707162070324, best_param α= 0.008\n",
      "5/5: #Trains: 15664, #Val: 3915 valLoss: 0.39431323529028456, best_param α= 0.008\n",
      "0.394337174412\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 3), analyzer='word', token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "predict_prob_features, predict_prob_features_test = vectorizer2NB(vectorizer,\n",
    "                                                                  text, \n",
    "                                                                  y,\n",
    "                                                                  text_test,\n",
    "                                                                  7, alphas=[0.008]) # from 0.007, 0.008, 0.009\n",
    "for a, c in author2class.items():\n",
    "    df['{}_word_tfidf_NB'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_word_tfidf_NB'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': [0.013, 0.014]} TfidfVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 5), norm='l2', preprocessor=None, smooth_idf=True,\n",
      "        stop_words=None, strip_accents=None, sublinear_tf=False,\n",
      "        token_pattern='(?u)\\\\b\\\\w+\\\\b', tokenizer=None, use_idf=True,\n",
      "        vocabulary=None)\n",
      "1/5: #Trains: 15663, #Val: 3916 valLoss: 0.3951854930297089, best_param α= 0.013\n",
      "2/5: #Trains: 15663, #Val: 3916 valLoss: 0.37123938784736266, best_param α= 0.013\n",
      "3/5: #Trains: 15663, #Val: 3916 valLoss: 0.40039094882541354, best_param α= 0.014\n",
      "4/5: #Trains: 15663, #Val: 3916 valLoss: 0.3916746370981128, best_param α= 0.013\n",
      "5/5: #Trains: 15664, #Val: 3915 valLoss: 0.36536618930442233, best_param α= 0.014\n",
      "0.384771331221\n"
     ]
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(ngram_range=(1, 5), analyzer='char', token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "predict_prob_features, predict_prob_features_test = vectorizer2NB(vectorizer,\n",
    "                                                                  text, \n",
    "                                                                  y,\n",
    "                                                                  text_test,\n",
    "                                                                  8, alphas=[0.013, 0.014]) # 0.012, 0.013, 0.014\n",
    "for a, c in author2class.items():\n",
    "    df['{}_char_tfidf_NB'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_char_tfidf_NB'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': [1.1]} CountVectorizer(analyzer='word', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 3), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w+\\\\b', tokenizer=None,\n",
      "        vocabulary=None)\n",
      "1/5: #Trains: 15663, #Val: 3916 valLoss: 0.8214301928473606, best_param α= 1.1\n",
      "2/5: #Trains: 15663, #Val: 3916 valLoss: 0.8746266739437928, best_param α= 1.1\n",
      "3/5: #Trains: 15663, #Val: 3916 valLoss: 0.8440447896828697, best_param α= 1.1\n",
      "4/5: #Trains: 15663, #Val: 3916 valLoss: 0.8016879710430628, best_param α= 1.1\n",
      "5/5: #Trains: 15664, #Val: 3915 valLoss: 0.8354528599965615, best_param α= 1.1\n",
      "0.835448497503\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 3), analyzer='word', token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "predict_prob_features, predict_prob_features_test = vectorizer2NB(vectorizer,\n",
    "                                                                  text, \n",
    "                                                                  y,\n",
    "                                                                  text_test,\n",
    "                                                                  9, alphas=[1.1]) # 1.0, 1.1, 1.2, 1.3\n",
    "\n",
    "for a, c in author2class.items():\n",
    "    df['{}_word_count_NB'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_word_count_NB'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': [0.15, 0.2, 0.3, 0.4, 0.5]} CountVectorizer(analyzer='char', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 4), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w+\\\\b', tokenizer=None,\n",
      "        vocabulary=None)\n",
      "1/5: #Trains: 15663, #Val: 3916 valLoss: 2.6198967115985408, best_param α= 0.5\n",
      "2/5: #Trains: 15663, #Val: 3916 valLoss: 2.4562692766909535, best_param α= 0.2\n",
      "3/5: #Trains: 15663, #Val: 3916 valLoss: 2.3908851123573673, best_param α= 0.4\n",
      "4/5: #Trains: 15663, #Val: 3916 valLoss: 2.8082645875690333, best_param α= 0.2\n",
      "5/5: #Trains: 15664, #Val: 3915 valLoss: 2.474757660679112, best_param α= 0.2\n",
      "2.55001466978\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 4), analyzer='char', token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "predict_prob_features, predict_prob_features_test = vectorizer2NB(vectorizer,\n",
    "                                                                  text, \n",
    "                                                                  y,\n",
    "                                                                  text_test,\n",
    "                                                                  10, alphas=[0.15, 0.2, 0.3, 0.4, 0.5])\n",
    "\n",
    "for a, c in author2class.items():\n",
    "    df['{}_char_count_NB'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_char_count_NB'.format(a)] = predict_prob_features_test[:, c]/num_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'alpha': [1.5, 2.0, 2.5]} CountVectorizer(analyzer='char_wb', binary=False, decode_error='strict',\n",
      "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
      "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
      "        ngram_range=(1, 5), preprocessor=None, stop_words=None,\n",
      "        strip_accents=None, token_pattern='(?u)\\\\b\\\\w+\\\\b', tokenizer=None,\n",
      "        vocabulary=None)\n",
      "1/5: #Trains: 15663, #Val: 3916 valLoss: 2.8954841178850548, best_param α= 2.0\n",
      "2/5: #Trains: 15663, #Val: 3916 valLoss: 2.725757021467124, best_param α= 2.5\n",
      "3/5: #Trains: 15663, #Val: 3916 valLoss: 2.6258308016995384, best_param α= 2.0\n",
      "4/5: #Trains: 15663, #Val: 3916 valLoss: 2.919685035156428, best_param α= 2.0\n",
      "5/5: #Trains: 15664, #Val: 3915 valLoss: 2.792691870927538, best_param α= 2.0\n",
      "2.79188976943\n"
     ]
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(ngram_range=(1, 5), analyzer='char_wb', token_pattern='(?u)\\\\b\\\\w+\\\\b')\n",
    "predict_prob_features, predict_prob_features_test = vectorizer2NB(vectorizer,\n",
    "                                                                  text, \n",
    "                                                                  y,\n",
    "                                                                  text_test,\n",
    "                                                                  11, alphas=[1.5, 2., 2.5]) # 2., 2.5\n",
    "for a, c in author2class.items():\n",
    "    df['{}_char_wb_count_NB'.format(a)] = predict_prob_features[:, c]\n",
    "    df_test['{}_char_wb_count_NB'.format(a)] = predict_prob_features_test[:, c]/num_split\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MetaFeatures"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "normal_latters = set(string.ascii_uppercase) | set(string.ascii_lowercase) | set(',.:;\"\\'?! ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df['num_words']      = np.array([len(t.split()) for t in df.text])\n",
    "df_test['num_words'] = np.array([len(t.split()) for t in df_test.text])\n",
    "\n",
    "df['num_chars']      = np.array([len(t) for t in df.text])\n",
    "df_test['num_chars'] = np.array([len(t) for t in df_test.text])\n",
    "\n",
    "df['average_num_chars']      = np.array([np.mean([len(word) for word in t.split()]) for t in df.text])\n",
    "df_test['average_num_chars'] = np.array([np.mean([len(word) for word in t.split()]) for t in df_test.text])\n",
    "\n",
    "df['num_uniq_words']      = np.array([len(set(t.split())) for t in df.text])\n",
    "df_test['num_uniq_words'] = np.array([len(set(t.split())) for t in df_test.text])\n",
    "\n",
    "df['num_uniq_chars']      = np.array([len(set(t)) for t in df.text])\n",
    "df_test['num_uniq_chars'] = np.array([len(set(t)) for t in df_test.text])\n",
    "\n",
    "df['rate_uniq_words']      = np.array([len(set(t.split()))/len(t.split()) for t in df.text])\n",
    "df_test['rate_uniq_words'] = np.array([len(set(t.split()))/len(t.split()) for t in df_test.text])\n",
    "\n",
    "df['rate_uniq_chars']       = np.array([len(set(t))/len(t) for t in df.text])\n",
    "df_test['rate_uniq_chars'] = np.array([len(set(t))/len(t) for t in df_test.text])\n",
    "\n",
    "\n",
    "special = ',' # ',.:;\"\\!'?!'\n",
    "for c in special:\n",
    "    df['num_'+c] = np.array([t.count(c) for t in df.text])\n",
    "    df_test['num_'+c] = np.array([t.count(c) for t in df_test.text])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "df.to_csv('./../data/train_feature.csv')\n",
    "df_test.to_csv('./../data/test_feature.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Go supervised FastText notebook!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
